---
phase: 15-generation-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/distill/models/prior.py
  - src/distill/inference/generation.py
  - src/distill/models/__init__.py
  - src/distill/inference/__init__.py
autonomous: true
requirements:
  - GEN-02
  - GEN-03
  - GEN-04

must_haves:
  truths:
    - "sample_code_sequence() generates a [1, seq_len] tensor of valid code indices from a CodePrior"
    - "Temperature, top-k, and top-p sampling controls produce visibly different code distributions"
    - "generate_audio_from_prior() returns a numpy float32 audio waveform from a LoadedVQModel with prior"
    - "Multi-chunk generation (duration > 1s) uses crossfade_chunks() to stitch waveform segments"
  artifacts:
    - path: "src/distill/models/prior.py"
      provides: "sample_code_sequence() autoregressive sampling function"
      contains: "def sample_code_sequence"
    - path: "src/distill/inference/generation.py"
      provides: "generate_audio_from_prior() end-to-end generation pipeline"
      contains: "def generate_audio_from_prior"
  key_links:
    - from: "src/distill/inference/generation.py"
      to: "src/distill/models/prior.py"
      via: "sample_code_sequence() call"
      pattern: "sample_code_sequence"
    - from: "src/distill/inference/generation.py"
      to: "src/distill/models/vqvae.py"
      via: "codes_to_embeddings() + decode() for code-to-audio"
      pattern: "codes_to_embeddings"
    - from: "src/distill/inference/generation.py"
      to: "src/distill/inference/chunking.py"
      via: "crossfade_chunks() for multi-chunk stitching"
      pattern: "crossfade_chunks"
---

<objective>
Build the core autoregressive sampling engine and end-to-end generation pipeline function that converts prior-sampled code sequences into audio waveforms.

Purpose: This is the foundational backend for all Phase 15 generation -- the UI and CLI both call generate_audio_from_prior(). Without this, no prior-based audio generation is possible.

Output: sample_code_sequence() in prior.py, generate_audio_from_prior() in generation.py, public API exports.
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-generation-pipeline/15-RESEARCH.md
@.planning/phases/14-autoregressive-prior/14-01-SUMMARY.md
@.planning/phases/14-autoregressive-prior/14-03-SUMMARY.md
@src/distill/models/prior.py
@src/distill/models/vqvae.py
@src/distill/inference/generation.py
@src/distill/inference/chunking.py
@src/distill/audio/spectrogram.py
@src/distill/models/persistence.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add sample_code_sequence() to prior.py with temperature/top-k/top-p</name>
  <files>src/distill/models/prior.py</files>
  <action>
Add a `sample_code_sequence()` function to `src/distill/models/prior.py` (after the `CodePrior` class, before `extract_code_sequences()`). This is a standalone module-level function, not a method on the class.

The function signature:

```python
@torch.no_grad()
def sample_code_sequence(
    prior: CodePrior,
    temperature: float = 1.0,
    top_k: int = 0,
    top_p: float = 0.9,
    seed: int | None = None,
    device: torch.device = torch.device("cpu"),
) -> torch.Tensor:
```

Implementation details:
1. Set `prior.eval()` at the start
2. If `seed is not None`, call `torch.manual_seed(seed)`
3. Read `seq_len = prior.seq_len` for the total number of tokens to generate
4. Initialize: `generated = torch.randint(0, prior.codebook_size, (1, 1), device=device)`
5. Loop `seq_len - 1` times, each step:
   a. Forward pass: `logits = prior(generated)[:, -1, :]` to get `[1, codebook_size]`
   b. Temperature scaling: `if temperature != 1.0: logits = logits / temperature`
   c. Top-k filtering: `if top_k > 0:` zero out (set to -inf) logits below the k-th largest. Clamp k to `min(top_k, logits.size(-1))`.
   d. Nucleus (top-p) filtering: `if 0 < top_p < 1.0:` sort logits descending, compute cumulative softmax, mask positions where `(cumulative - current_prob) >= top_p`, scatter back to original indices.
   e. Sample: `probs = softmax(logits)`, `next_token = torch.multinomial(probs, 1)`
   f. Concatenate: `generated = torch.cat([generated, next_token], dim=1)`
6. Return `generated` with shape `[1, seq_len]`

Add the function to the module's `__all__` or ensure it's importable. The function docstring should document parameters and return shape.

IMPORTANT: The autoregressive loop runs `seq_len - 1` iterations because we start with 1 token. For typical configs (e.g., 48 spatial positions * 3 quantizers = 144 tokens), this is fast enough without KV caching per research anti-pattern guidance.
  </action>
  <verify>
Run: `cd H:/dev/Distill-vqvae && python -c "from distill.models.prior import sample_code_sequence, CodePrior; print('import OK')"`
Verify function exists and is importable.
  </verify>
  <done>
sample_code_sequence() exists in prior.py, accepts temperature/top_k/top_p/seed parameters, returns [1, seq_len] code indices tensor. Function is importable from distill.models.prior.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create generate_audio_from_prior() with multi-chunk stitching and public API exports</name>
  <files>
    src/distill/inference/generation.py
    src/distill/models/__init__.py
    src/distill/inference/__init__.py
  </files>
  <action>
Add `generate_audio_from_prior()` function to `src/distill/inference/generation.py` AFTER the existing `GenerationPipeline` class. This is a standalone function, NOT a method on GenerationPipeline (per research: do not modify the v1.0 class).

The function signature:

```python
def generate_audio_from_prior(
    loaded: "LoadedVQModel",
    temperature: float = 1.0,
    top_k: int = 0,
    top_p: float = 0.9,
    duration_s: float = 10.0,
    overlap_samples: int = 2400,
    seed: int | None = None,
    progress_callback: "Callable[[float, str], None] | None" = None,
) -> tuple["np.ndarray", int]:
```

Returns `(audio_waveform, seed_used)` where `audio_waveform` is `np.ndarray` float32 and `seed_used` is the actual seed (for reproducibility tracking).

Implementation:
1. Extract components from `loaded`: `prior = loaded.prior`, `vqvae = loaded.model`, `spectrogram = loaded.spectrogram`, `device = loaded.device`
2. Validate: raise `ValueError("Model has no trained prior")` if `loaded.prior is None`
3. Set both `vqvae.eval()` and `prior.eval()`
4. Compute spatial shape from spectrogram config:
   - `mel_shape = spectrogram.get_mel_shape(48000)` (1 second of audio at 48kHz)
   - `n_mels, time_frames = mel_shape`
   - `H = math.ceil(n_mels / 16)` (after padding + 4x stride-2 downsampling)
   - `W = math.ceil(time_frames / 16)`
   - Verify: for default config (128 mels, 94+1=95 -> pad to 96 -> 96/16=6 time, 128/16=8 mels), spatial_shape = (8, 6)
   - More precisely, compute with padding awareness: `pad_h = (16 - n_mels % 16) % 16`, `padded_h = (n_mels + pad_h) // 16`, same for W. Use this formula for correctness.
5. Compute num_chunks: `num_chunks = max(1, math.ceil(duration_s / 1.0))` (each chunk is ~1 second)
6. Resolve seed: `actual_seed = seed if seed is not None else random.randint(0, 2**31)`
7. Generate chunks in a loop:
   a. Call `progress_callback(i / num_chunks, f"Generating chunk {i+1}/{num_chunks}...")` if provided
   b. Call `sample_code_sequence(prior, temperature, top_k, top_p, seed=actual_seed + i, device=device)`
   c. Unflatten codes: `codes_3d = unflatten_codes(codes_flat, prior.num_quantizers)` -> `[1, spatial_positions, num_quantizers]`
   d. Decode: `quantized = vqvae.codes_to_embeddings(codes_3d.to(device), spatial_shape)`
   e. Mel: `mel = vqvae.decode(quantized, target_shape=mel_shape)`
   f. Waveform: `wav = spectrogram.mel_to_waveform(mel)`
   g. Append: `chunks.append(wav.squeeze().numpy().astype(np.float32))`
8. Call `progress_callback(1.0, "Stitching chunks...")` if provided
9. Stitch: if `len(chunks) == 1`, return `chunks[0]`. Otherwise `from distill.inference.chunking import crossfade_chunks` and return `crossfade_chunks(chunks, overlap_samples=overlap_samples)`.
10. Return `(stitched_audio, actual_seed)`.

Imports needed (lazy, inside function body per project pattern):
- `import math`, `import random`
- `import numpy as np`
- `import torch`
- `from distill.models.prior import sample_code_sequence, unflatten_codes`
- `from distill.inference.chunking import crossfade_chunks`

Also add TYPE_CHECKING import for `LoadedVQModel` and `Callable` at the top of the file.

**Public API exports:**

In `src/distill/models/__init__.py`: Add `sample_code_sequence` to the imports from `distill.models.prior` and to `__all__`.

In `src/distill/inference/__init__.py` (create if needed, or update): Export `generate_audio_from_prior` from `distill.inference.generation`.
  </action>
  <verify>
Run: `cd H:/dev/Distill-vqvae && python -c "from distill.inference.generation import generate_audio_from_prior; print('generation pipeline import OK')"`
Run: `cd H:/dev/Distill-vqvae && python -c "from distill.models import sample_code_sequence; print('models export OK')"`
  </verify>
  <done>
generate_audio_from_prior() exists in inference/generation.py, accepts LoadedVQModel + sampling parameters + duration + progress_callback, returns (np.ndarray, int) audio waveform and seed. Multi-chunk stitching uses crossfade_chunks(). sample_code_sequence exported from distill.models. generate_audio_from_prior exported from distill.inference.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from distill.models.prior import sample_code_sequence"` -- import succeeds
2. `python -c "from distill.inference.generation import generate_audio_from_prior"` -- import succeeds
3. `python -c "from distill.models import sample_code_sequence"` -- public API export works
4. Grep for `crossfade_chunks` in generation.py confirms stitching is wired
5. Grep for `codes_to_embeddings` in generation.py confirms VQ-VAE decode path is wired
</verification>

<success_criteria>
- sample_code_sequence() produces [1, seq_len] code indices with temperature/top-k/top-p controls
- generate_audio_from_prior() accepts LoadedVQModel, returns (np.ndarray, int) audio waveform + seed
- Multi-chunk durations produce crossfade-stitched audio via crossfade_chunks()
- Progress callback receives chunk-level updates
- All new functions importable from public API
</success_criteria>

<output>
After completion, create `.planning/phases/15-generation-pipeline/15-01-SUMMARY.md`
</output>
