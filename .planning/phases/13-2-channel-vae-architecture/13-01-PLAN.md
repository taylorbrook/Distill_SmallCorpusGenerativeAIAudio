---
phase: 13-2-channel-vae-architecture
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/distill/models/vae.py
  - src/distill/models/losses.py
autonomous: true
requirements: [ARCH-01, ARCH-02, ARCH-03, ARCH-04]

must_haves:
  truths:
    - "ConvVAE encoder accepts [B, 2, n_mels, time] input and produces (mu, logvar) each [B, latent_dim]"
    - "ConvVAE decoder produces [B, 2, n_mels, time] output where channel 0 is non-negative (Softplus) and channel 1 is bounded [-1, 1] (Tanh)"
    - "ConvVAE defaults to latent_dim=128 and latent_dim remains configurable"
    - "Round-trip encode-decode of a 2-channel input produces a 2-channel output of matching shape"
  artifacts:
    - path: "src/distill/models/vae.py"
      provides: "5-layer ConvEncoder, ConvDecoder with split activation, ConvVAE"
      contains: "class ConvEncoder"
    - path: "src/distill/models/losses.py"
      provides: "VAE loss function compatible with 2-channel tensors"
      contains: "def vae_loss"
  key_links:
    - from: "src/distill/models/vae.py"
      to: "ConvEncoder.forward"
      via: "2-channel input conv stack"
      pattern: "Conv2d\\(2,"
    - from: "src/distill/models/vae.py"
      to: "ConvDecoder.forward"
      via: "split activation for mag/IF channels"
      pattern: "Softplus|Tanh"
---

<objective>
Rewrite the ConvVAE encoder and decoder for 2-channel (magnitude + instantaneous frequency) spectrograms with 5 convolutional layers, scaled-up channel progression (2->64->128->256->512), split per-channel decoder activations (Softplus for magnitude, Tanh for IF), and configurable latent_dim defaulting to 128.

Purpose: This is the core model architecture change from v1.0 (single-channel mel) to v2.0 (2-channel mag+IF). The encoder must accept 2-channel input and the decoder must produce appropriate per-channel activations.
Output: Rewritten vae.py with ~12M+ parameter model, updated losses.py docstrings.
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-2-channel-vae-architecture/13-CONTEXT.md
@.planning/phases/12-2-channel-data-pipeline/12-01-SUMMARY.md
@.planning/phases/12-2-channel-data-pipeline/12-02-SUMMARY.md
@src/distill/models/vae.py
@src/distill/models/losses.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite ConvEncoder and ConvDecoder for 2-channel 5-layer architecture</name>
  <files>src/distill/models/vae.py</files>
  <action>
Rewrite `src/distill/models/vae.py` with the following architecture changes:

**ConvEncoder:**
- Input shape: `[B, 2, n_mels, time]` (hard-coded 2 channels, not a parameter)
- 5 conv blocks with channel progression: 2 -> 64 -> 128 -> 256 -> 512
  - Each block: Conv2d(in_ch, out_ch, kernel=3, stride=2, padding=1) + BatchNorm2d + ReLU + Dropout2d
- Pad input to multiples of 32 (5 stride-2 layers = 32x spatial reduction), up from 16
- Keep lazy linear init pattern for fc_mu and fc_logvar (compute flatten_dim on first forward pass)
- Store `_padded_shape` for decoder reference
- logvar clamped to [-20, 20] (unchanged)
- Update all docstrings from `[B, 1, ...]` to `[B, 2, ...]`

**ConvDecoder:**
- Output shape: `[B, 2, n_mels, time]`
- 5 deconv blocks with channel progression: 512 -> 256 -> 128 -> 64 -> 2
  - First 4 blocks: ConvTranspose2d(in_ch, out_ch, kernel=3, stride=2, padding=1, output_padding=1) + BatchNorm2d + ReLU + Dropout2d
  - Last (5th) block: ConvTranspose2d(64, 2, kernel=3, stride=2, padding=1, output_padding=1) with NO activation in Sequential
- After the deconv sequential, apply split per-channel activation:
  - Channel 0 (magnitude): Softplus (non-negative, unbounded above)
  - Channel 1 (IF): Tanh (bounded [-1, 1] matching IF normalization range)
  - Implementation: slice the 2-channel output, apply Softplus to `[:, 0:1, :, :]` and Tanh to `[:, 1:2, :, :]`, then concatenate back along dim=1
- Keep lazy linear init pattern for fc (compute flatten_dim from spatial shape)
- Update all docstrings from `[B, 1, ...]` to `[B, 2, ...]`

**ConvVAE:**
- Hard-code `in_channels=2` (no parameter). Remove any `in_channels` parameter.
- Default `latent_dim=128` (already the case in TrainingConfig, now also in model default)
- Update `_DEFAULT_MEL_SHAPE` to still be `(128, 94)` for sample() method
- Update `sample()` to pad to multiples of 32 (not 16) and use 5 stride-2 layers (spatial = padded / 32)
- Update `forward()` to compute decoder spatial as `padded / 32` (not padded / 16)
- Update all docstrings and module docstring to reflect 2-channel, 5-layer, ~12M+ params

**Module docstring update:**
- Update from "4-layer" to "5-layer", from "64-dimensional" to "128-dimensional" defaults
- Update from "~3.1M" to "~12M+" parameters
- Update pad-to-16 references to pad-to-32
- Note this is v2.0 2-channel architecture, clean break from v1.0
  </action>
  <verify>
Run Python to verify:
```
python -c "
import torch
from distill.models.vae import ConvVAE, ConvEncoder, ConvDecoder

# Test default construction
model = ConvVAE()
assert model.latent_dim == 128

# Test forward pass with 2-channel input
x = torch.randn(2, 2, 128, 94)
recon, mu, logvar = model(x)
assert recon.shape == x.shape, f'Shape mismatch: {recon.shape} vs {x.shape}'
assert mu.shape == (2, 128), f'mu shape: {mu.shape}'
assert logvar.shape == (2, 128), f'logvar shape: {logvar.shape}'

# Verify per-channel activations
assert (recon[:, 0, :, :] >= 0).all(), 'Magnitude channel should be non-negative (Softplus)'
assert (recon[:, 1, :, :] >= -1).all() and (recon[:, 1, :, :] <= 1).all(), 'IF channel should be in [-1, 1] (Tanh)'

# Test sample
samples = model.sample(3, torch.device('cpu'))
assert samples.shape == (3, 2, 128, 94), f'Sample shape: {samples.shape}'
assert (samples[:, 0, :, :] >= 0).all(), 'Sampled magnitude should be non-negative'

# Parameter count
n_params = sum(p.numel() for p in model.parameters())
print(f'Parameters: {n_params:,}')
assert n_params > 10_000_000, f'Expected >10M params, got {n_params:,}'

print('All checks passed')
"
```
  </verify>
  <done>
ConvEncoder accepts [B, 2, n_mels, time] with 5-layer 2->64->128->256->512 progression, pads to multiples of 32. ConvDecoder produces [B, 2, n_mels, time] with 512->256->128->64->2 progression and split activation (Softplus for channel 0, Tanh for channel 1). ConvVAE defaults to latent_dim=128. Round-trip produces matching shapes with correct per-channel activation ranges. Model has >10M parameters.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update losses.py docstrings for 2-channel compatibility</name>
  <files>src/distill/models/losses.py</files>
  <action>
Update `src/distill/models/losses.py` docstrings to reflect 2-channel tensors:

1. In `vae_loss` function docstring:
   - Change `recon` description from `[B, 1, n_mels, time]` to `[B, C, n_mels, time]` (where C=2 for v2.0)
   - Change `target` description from `[B, 1, n_mels, time]` to `[B, C, n_mels, time]`
   - Note: The actual MSE computation is unchanged -- `F.mse_loss` works identically on any number of channels. No code change needed, only docstring updates.

2. In module docstring:
   - Change "MSE on log-mel spectrograms" to "MSE on spectrogram tensors (v2.0: 2-channel magnitude + IF)"
   - Keep all other descriptions unchanged

The loss function code itself requires NO modification -- MSE with reduction="mean" correctly averages over all elements including the channel dimension. The KL divergence operates on latent space (mu, logvar) which is channel-agnostic.
  </action>
  <verify>
Run Python to verify losses still work with 2-channel tensors:
```
python -c "
import torch
from distill.models.losses import vae_loss, compute_kl_divergence

# 2-channel tensors
recon = torch.randn(4, 2, 128, 94).abs()  # magnitude-like
target = torch.randn(4, 2, 128, 94).abs()
mu = torch.randn(4, 128)
logvar = torch.randn(4, 128)

total, recon_loss, kl_loss = vae_loss(recon, target, mu, logvar, kl_weight=0.01)
assert total.isfinite(), 'Loss should be finite'
assert recon_loss.isfinite(), 'Recon loss should be finite'
assert kl_loss.isfinite(), 'KL loss should be finite'

kl_div = compute_kl_divergence(mu, logvar)
assert isinstance(kl_div, float), 'KL divergence should be float'

print(f'total={total.item():.4f} recon={recon_loss.item():.4f} kl={kl_loss.item():.4f} kl_div={kl_div:.4f}')
print('Loss functions work with 2-channel tensors')
"
```
  </verify>
  <done>
losses.py docstrings updated to reflect 2-channel tensor shapes. Loss computation verified to work identically with [B, 2, n_mels, time] tensors (MSE averages over all elements including channel dimension; KL operates on latent space which is channel-agnostic).
  </done>
</task>

</tasks>

<verification>
1. `python -c "from distill.models.vae import ConvVAE; m = ConvVAE(); print(m)"` -- model instantiates
2. Forward pass with `[B, 2, 128, 94]` produces matching output shape
3. Magnitude channel (0) is non-negative, IF channel (1) is in [-1, 1]
4. `model.sample(N, device)` produces `[N, 2, 128, 94]` output
5. Parameter count exceeds 10M
6. Loss functions work with 2-channel tensors
</verification>

<success_criteria>
- ConvVAE with 2-channel 5-layer architecture producing correct shapes and activations
- latent_dim defaults to 128, remains configurable
- Split activation: Softplus (mag) + Tanh (IF) on decoder output
- Loss functions compatible with 2-channel spectrograms
- All verification commands pass without errors
</success_criteria>

<output>
After completion, create `.planning/phases/13-2-channel-vae-architecture/13-01-SUMMARY.md`
</output>
