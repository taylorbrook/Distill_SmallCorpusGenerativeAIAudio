---
phase: 13-2-channel-vae-architecture
plan: 02
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - src/distill/training/loop.py
  - src/distill/training/config.py
  - src/distill/models/persistence.py
autonomous: true
requirements: [ARCH-01, ARCH-02, ARCH-03, ARCH-04]

must_haves:
  truths:
    - "Training loop always uses cached 2-channel spectrograms with no v1.0 waveform path"
    - "Model initialization in training and persistence uses 5-layer architecture constants (512 channels, 32x spatial reduction)"
    - "Loading a saved .distill model reconstructs the 2-channel 5-layer ConvVAE correctly"
    - "ComplexSpectrogramConfig.enabled field is removed (always 2-channel)"
  artifacts:
    - path: "src/distill/training/loop.py"
      provides: "Training orchestrator using only v2.0 cached spectrogram path"
      contains: "def train"
    - path: "src/distill/training/config.py"
      provides: "Training config without complex_spectrogram.enabled toggle"
      contains: "class ComplexSpectrogramConfig"
    - path: "src/distill/models/persistence.py"
      provides: "Model save/load with 5-layer architecture init"
      contains: "def load_model"
  key_links:
    - from: "src/distill/training/loop.py"
      to: "src/distill/models/vae.py"
      via: "model init with 5-layer spatial dims"
      pattern: "padded.*//.*32"
    - from: "src/distill/models/persistence.py"
      to: "src/distill/models/vae.py"
      via: "model reconstruction with 5-layer spatial dims"
      pattern: "padded.*//.*32"
---

<objective>
Wire the new 2-channel 5-layer ConvVAE into the training loop and model persistence layer, removing all v1.0 single-channel training code paths and the `complex_spectrogram.enabled` toggle.

Purpose: The training loop and persistence code still reference the old 4-layer architecture constants (256 channels, 16x spatial reduction) and maintain a v1.0 waveform-to-mel training path that is no longer needed. This plan updates all initialization code and removes the obsolete v1.0 toggle.
Output: Clean v2.0-only training pipeline with correct architecture constants.
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-2-channel-vae-architecture/13-CONTEXT.md
@.planning/phases/13-2-channel-vae-architecture/13-01-SUMMARY.md
@src/distill/training/loop.py
@src/distill/training/config.py
@src/distill/models/persistence.py
@src/distill/models/vae.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update model initialization for 5-layer architecture in loop.py and persistence.py</name>
  <files>src/distill/training/loop.py, src/distill/models/persistence.py</files>
  <action>
Update all model initialization code that computes spatial dimensions and flatten_dim to match the new 5-layer, 512-channel architecture.

**In `src/distill/training/loop.py` (train function, model initialization section ~lines 388-398):**
1. Change padding from multiples of 16 to multiples of 32:
   - `pad_h = (32 - n_mels % 32) % 32` (was 16)
   - `pad_w = (32 - time_frames % 32) % 32` (was 16)
2. Change spatial computation from `/16` to `/32`:
   - `spatial = (padded_h // 32, padded_w // 32)` (was // 16)
3. Change flatten_dim from 256 to 512 channels:
   - `flatten_dim = 512 * spatial[0] * spatial[1]` (was 256)
4. Note: `time_frames` computation from `spec_config` may need adjustment. The encoder pads the actual input tensor, so this pre-initialization just needs to produce compatible dimensions. Use the same logic: `time_frames = spec_config.sample_rate // spec_config.hop_length + 1` (unchanged).

**In `src/distill/models/persistence.py` (load_model function ~lines 320-338):**
1. Change padding from 16 to 32:
   - `pad_h = (32 - n_mels % 32) % 32`
   - `pad_w = (32 - time_frames % 32) % 32`
2. Change spatial from `/16` to `/32`:
   - `spatial = (padded_h // 32, padded_w // 32)`
3. Change comment from "4 stride-2 layers" to "5 stride-2 layers"
4. Change flatten_dim from 256 to 512:
   - `flatten_dim = 512 * spatial[0] * spatial[1]`
5. Change default latent_dim fallback from 64 to 128:
   - `latent_dim = saved.get("latent_dim", 128)` (was 64)

**In `src/distill/models/persistence.py` (save_model_from_checkpoint function ~lines 465-479):**
1. Same padding/spatial/flatten_dim changes as load_model
2. Change default latent_dim fallback from 64 to 128:
   - `latent_dim = checkpoint.get("latent_dim", 128)` (was 64)
  </action>
  <verify>
Run Python to verify model initialization produces correct dimensions:
```
python -c "
import torch
from distill.models.vae import ConvVAE

# Simulate loop.py init pattern with 5-layer constants
n_mels = 128
time_frames = 48000 // 512 + 1  # = 94
pad_h = (32 - n_mels % 32) % 32  # 128 % 32 = 0, pad = 0
pad_w = (32 - time_frames % 32) % 32
padded_h = n_mels + pad_h
padded_w = time_frames + pad_w
spatial = (padded_h // 32, padded_w // 32)
flatten_dim = 512 * spatial[0] * spatial[1]

model = ConvVAE(latent_dim=128)
model.decoder._init_linear(spatial)
model.encoder._init_linear(flatten_dim)

# Verify forward pass works after manual init
x = torch.randn(1, 2, 128, 94)
recon, mu, logvar = model(x)
assert recon.shape == x.shape, f'Shape mismatch: {recon.shape}'
print(f'spatial={spatial}, flatten_dim={flatten_dim}')
print(f'Forward pass OK: {recon.shape}')
"
```
  </verify>
  <done>
All model initialization code in loop.py and persistence.py uses 5-layer architecture constants: padding to multiples of 32, spatial dims divided by 32, 512-channel flatten_dim. Default latent_dim fallback is 128. Forward pass through manually-initialized model produces correct shapes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Remove v1.0 training code paths and complex_spectrogram.enabled toggle</name>
  <files>src/distill/training/loop.py, src/distill/training/config.py</files>
  <action>
Remove v1.0 single-channel waveform training code paths per the locked decision: "Remove v1.0 single-channel code paths -- delete old waveform-to-mel training paths, single-channel dataset code, and the `complex_spectrogram` toggle."

**In `src/distill/training/config.py`:**
1. Remove the `enabled` field from `ComplexSpectrogramConfig`. The class should still exist (it holds STFT parameters used by the preprocessing pipeline) but without the `enabled: bool = True` field since it is always 2-channel now.
2. Alternatively, if removing the field would break downstream code that reads `config.complex_spectrogram.enabled`, remove the field and update all references.

**In `src/distill/training/loop.py` (train function):**
1. Remove the `if config.complex_spectrogram.enabled:` branch and the `else:` v1.0 branch entirely.
2. The train function should ALWAYS use the v2.0 cached spectrogram path:
   - Always import and call `preprocess_complex_spectrograms`
   - Always call `create_complex_data_loaders`
   - Always set `use_cached_spectrograms = True`
   - Remove the `use_cached_spectrograms = False` initialization and the conditional
3. Remove the v1.0 waveform path code that creates `AugmentationPipeline` and calls `create_data_loaders` (the function from dataset.py that loads raw waveforms). Keep the augmentation config setup for the preprocessing pipeline.
4. Remove the `# Phase 13 will update ConvVAE to accept in_channels=2` comment (Phase 13 is now).
5. Remove `use_cached_spectrograms` parameter from `train_epoch` and `validate_epoch` calls since it's always True now.
6. In `train_epoch` function signature: remove the `use_cached_spectrograms` parameter. Always treat batches as pre-computed `[B, 2, n_mels, time]` spectrograms. Remove the `if use_cached_spectrograms:` / `else:` waveform-to-mel conversion branch. Simply use `mel = batch` directly (batch is always a cached spectrogram).
7. In `validate_epoch` function signature: same removal of `use_cached_spectrograms` parameter and conditional branch.
8. In the latent space analysis section at end of training: keep the `if config.complex_spectrogram.enabled:` skip logic but rewrite it as an unconditional skip (since it's always 2-channel now). Change to: `logger.info("Skipping latent space analysis (2-channel mode) -- handled in Phase 16")` without any conditional. Remove the `else:` v1.0 analysis block entirely.
9. Remove unused imports that were only needed for v1.0 path: `AudioSpectrogram`, `SpectrogramConfig` are still needed for the spectrogram object passed to preview generation. Keep them. Remove `create_data_loaders` import if present.
10. The `spectrogram` parameter in `train_epoch` and `validate_epoch` can be removed since batches are always pre-computed. However, the `spectrogram` object is still constructed in `train()` for preview generation. Keep the `spectrogram` local variable in `train()` for preview use, but remove it from `train_epoch`/`validate_epoch` signatures.

**IMPORTANT: Do NOT remove `AudioTrainingDataset` or `create_data_loaders` from `dataset.py`** -- they may be imported by other modules (latent analyzer). Only remove v1.0 paths from the training loop itself.

**IMPORTANT: Do NOT remove preview generation code** -- it still uses `spectrogram.mel_to_waveform()` which is the v1.0 AudioSpectrogram. Phase 15 will replace this with ISTFT. For now, preview generation will be temporarily non-functional with 2-channel models (it tries to convert 2-channel mel to waveform via GriffinLim which expects 1 channel). Wrap the preview generation call in a try/except that logs a warning like "Preview generation not yet supported for 2-channel models (requires Phase 15 ISTFT)". This prevents training from crashing.
  </action>
  <verify>
Verify the code changes are syntactically valid and the module loads:
```
python -c "
from distill.training.loop import train, train_epoch, validate_epoch
from distill.training.config import TrainingConfig, ComplexSpectrogramConfig
import inspect

# Verify ComplexSpectrogramConfig has no 'enabled' field
cfg = ComplexSpectrogramConfig()
assert not hasattr(cfg, 'enabled'), 'enabled field should be removed'
print(f'ComplexSpectrogramConfig fields: {list(cfg.__dataclass_fields__.keys())}')

# Verify train_epoch signature has no use_cached_spectrograms
sig = inspect.signature(train_epoch)
params = list(sig.parameters.keys())
assert 'use_cached_spectrograms' not in params, f'use_cached_spectrograms should be removed from train_epoch, got: {params}'
assert 'spectrogram' not in params, f'spectrogram should be removed from train_epoch, got: {params}'
print(f'train_epoch params: {params}')

# Verify validate_epoch signature has no use_cached_spectrograms
sig = inspect.signature(validate_epoch)
params = list(sig.parameters.keys())
assert 'use_cached_spectrograms' not in params, f'use_cached_spectrograms should be removed from validate_epoch'
assert 'spectrogram' not in params, f'spectrogram should be removed from validate_epoch'
print(f'validate_epoch params: {params}')

# Verify TrainingConfig still has complex_spectrogram
tc = TrainingConfig()
assert hasattr(tc, 'complex_spectrogram'), 'complex_spectrogram config should still exist'
print('All v1.0 cleanup checks passed')
"
```
  </verify>
  <done>
v1.0 waveform training path removed from loop.py. `complex_spectrogram.enabled` toggle removed from config.py. Training loop always uses cached 2-channel spectrograms. `train_epoch` and `validate_epoch` no longer have `use_cached_spectrograms` or `spectrogram` parameters. Latent space analysis unconditionally skipped (Phase 16). Preview generation wrapped in try/except for 2-channel graceful degradation.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from distill.training.loop import train"` -- module loads without error
2. `python -c "from distill.training.config import ComplexSpectrogramConfig; c = ComplexSpectrogramConfig(); assert not hasattr(c, 'enabled')"` -- enabled field removed
3. Model init in loop.py uses 32x spatial reduction and 512 channels
4. Model init in persistence.py uses 32x spatial reduction and 512 channels
5. train_epoch and validate_epoch signatures have no use_cached_spectrograms or spectrogram params
6. No remaining references to v1.0 waveform training path in loop.py
</verification>

<success_criteria>
- Training loop is v2.0-only: always cached spectrograms, no waveform-to-mel path
- Model initialization everywhere uses correct 5-layer constants (32x reduction, 512 channels)
- Persistence load/save work with new architecture dimensions
- ComplexSpectrogramConfig.enabled removed
- Preview generation gracefully degrades for 2-channel models
- All modules import and load without errors
</success_criteria>

<output>
After completion, create `.planning/phases/13-2-channel-vae-architecture/13-02-SUMMARY.md`
</output>
