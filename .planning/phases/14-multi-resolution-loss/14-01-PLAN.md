---
phase: 14-multi-resolution-loss
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/distill/training/config.py
  - src/distill/models/losses.py
autonomous: true
requirements: [LOSS-01, LOSS-02, LOSS-03, LOSS-04]

must_haves:
  truths:
    - "A LossConfig dataclass exists with nested sections for STFT, reconstruction, and KL weights"
    - "Multi-resolution STFT loss computes spectral convergence + log magnitude at 3+ window sizes via auraloss"
    - "Per-channel reconstruction uses L1 loss with magnitude weighting on the IF channel"
    - "All loss weights are configurable via LossConfig without code changes"
    - "auraloss is declared as a project dependency"
  artifacts:
    - path: "pyproject.toml"
      provides: "auraloss dependency declaration"
      contains: "auraloss"
    - path: "src/distill/training/config.py"
      provides: "LossConfig dataclass with nested weight sections"
      contains: "class LossConfig"
    - path: "src/distill/models/losses.py"
      provides: "Combined loss function with multi-resolution STFT, per-channel recon, magnitude-weighted IF, and KL"
      exports: ["compute_combined_loss", "vae_loss", "get_kl_weight"]
  key_links:
    - from: "src/distill/models/losses.py"
      to: "auraloss.freq.MultiResolutionSTFTLoss"
      via: "import and instantiation"
      pattern: "MultiResolutionSTFTLoss"
    - from: "src/distill/models/losses.py"
      to: "src/distill/training/config.py"
      via: "LossConfig parameter"
      pattern: "LossConfig"
---

<objective>
Implement the multi-resolution loss function with configurable weights for 2-channel VAE training.

Purpose: Replace the single MSE reconstruction loss with a perceptually grounded combined loss: multi-resolution STFT loss (auraloss) for spectral quality, per-channel L1 reconstruction loss with magnitude weighting on IF, and KL divergence with annealing. This is the core perceptual signal for v2.0 audio quality.

Output: LossConfig dataclass, compute_combined_loss function, auraloss dependency
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-multi-resolution-loss/14-CONTEXT.md

@src/distill/models/losses.py
@src/distill/training/config.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add auraloss dependency and LossConfig dataclass</name>
  <files>pyproject.toml, src/distill/training/config.py</files>
  <action>
1. Add `auraloss>=0.4.0` to pyproject.toml dependencies list (after `scipy`). Then run `uv sync` to install it.

2. In `src/distill/training/config.py`, add a new `LossConfig` dataclass (after `ComplexSpectrogramConfig`, before `TrainingConfig`) with nested weight configuration. Use the grouped config pattern per user decision:

```python
@dataclass
class LossConfig:
    """Configuration for combined multi-resolution loss.

    Grouped sections for each loss component. All weights are relative
    and applied multiplicatively to their respective loss terms.
    """
    # Multi-resolution STFT loss (auraloss)
    stft_weight: float = 1.0          # Weight for STFT loss in total
    stft_fft_sizes: tuple[int, ...] = (512, 1024, 2048)  # Window sizes
    stft_hop_sizes: tuple[int, ...] = (128, 256, 512)     # Hop sizes
    stft_win_lengths: tuple[int, ...] = (512, 1024, 2048) # Window lengths

    # Per-channel reconstruction loss (L1)
    recon_weight: float = 0.1         # Weight for reconstruction loss in total
    magnitude_weight: float = 1.0     # Relative weight for magnitude channel
    if_weight: float = 0.5            # Relative weight for IF channel

    # KL divergence
    kl_weight_max: float = 0.01       # Maximum KL weight (beta)
    kl_warmup_fraction: float = 0.3   # Fraction of epochs for annealing
    kl_free_bits: float = 0.1         # Per-dimension KL floor
```

Note: Default weights favor STFT loss (1.0) over reconstruction (0.1), per user decision that spectral quality takes precedence. KL defaults match existing config values. The `stft_fft_sizes` use 3 standard resolutions covering different temporal/spectral tradeoffs.

3. Add `loss: LossConfig = field(default_factory=LossConfig)` to `TrainingConfig`. Keep the existing `kl_warmup_fraction`, `kl_weight_max`, and `free_bits` fields on TrainingConfig for backward compatibility but document they are superseded by `loss.*` when the combined loss is used.

4. Export `LossConfig` from `src/distill/training/__init__.py`.
  </action>
  <verify>
Run `python -c "from distill.training.config import LossConfig, TrainingConfig; c = TrainingConfig(); print(c.loss); print(c.loss.stft_fft_sizes)"` -- should print the LossConfig with defaults and the FFT sizes tuple.
Run `python -c "import auraloss; print(auraloss.__version__)"` -- should print version >= 0.4.0.
  </verify>
  <done>LossConfig dataclass exists with nested STFT/reconstruction/KL weight sections. auraloss is installed. TrainingConfig has a `loss` field.</done>
</task>

<task type="auto">
  <name>Task 2: Implement compute_combined_loss in losses.py</name>
  <files>src/distill/models/losses.py</files>
  <action>
Add a new `compute_combined_loss` function to `src/distill/models/losses.py` that combines three loss components. Keep existing `vae_loss` and `get_kl_weight` functions unchanged (backward compatibility).

The function signature:

```python
def compute_combined_loss(
    recon: torch.Tensor,          # [B, 2, n_mels, time]
    target: torch.Tensor,         # [B, 2, n_mels, time]
    mu: torch.Tensor,             # [B, latent_dim]
    logvar: torch.Tensor,         # [B, latent_dim]
    loss_config: "LossConfig",
    kl_weight: float,
    _stft_loss_fn: "MultiResolutionSTFTLoss | None" = None,
) -> dict[str, torch.Tensor]:
```

Implementation details:

**1. Per-channel reconstruction loss (L1):**
- Slice `recon` and `target` into magnitude (ch 0) and IF (ch 1) channels
- Compute L1 loss for magnitude: `F.l1_loss(recon_mag, target_mag)`
- Compute magnitude-weighted IF loss: weight IF errors by the target magnitude so that errors in low-energy (inaudible) regions contribute less. Use a soft weighting approach:
  ```python
  mag_weights = target_mag / (target_mag.mean() + 1e-8)  # Normalize so mean weight = 1
  if_error = (recon_if - target_if).abs()
  weighted_if_loss = (if_error * mag_weights).mean()
  ```
- Combined recon: `loss_config.recon_weight * (loss_config.magnitude_weight * mag_loss + loss_config.if_weight * weighted_if_loss)`

**2. Multi-resolution STFT loss (auraloss):**
- Accept a pre-created `_stft_loss_fn` (MultiResolutionSTFTLoss instance) to avoid re-creating it every call. The caller creates it once per training run.
- If `_stft_loss_fn` is None, create one from `loss_config.stft_fft_sizes`, `loss_config.stft_hop_sizes`, `loss_config.stft_win_lengths` (lazy initialization pattern).
- auraloss MultiResolutionSTFTLoss expects `[B, 1, T]` or `[B, T]` waveform-like input. Since we are operating on spectrograms (not waveforms), we need to use it on flattened spectrogram rows. Reshape each channel from `[B, 1, n_mels, time]` to `[B, 1, n_mels * time]` (treat the spectrogram as a 1D signal for STFT loss). Apply to magnitude channel only (IF is a derivative signal, not spectral content).
- Weighted: `loss_config.stft_weight * stft_loss`
- Add epsilon guard: `stft_loss = stft_loss.clamp(min=0.0)` to prevent negative values from numerical issues.

**3. KL divergence:**
- Reuse existing KL computation from `vae_loss`: per-dim KL with free bits, sum over dims, mean over batch.
- Weighted: `kl_weight * kl_loss` (kl_weight comes from annealing schedule, not config)

**4. Total loss:**
- `total = stft_term + recon_term + kl_term`

**5. Return dict:**
```python
return {
    "total_loss": total,
    "stft_loss": stft_loss_raw,        # unweighted STFT loss
    "mag_recon_loss": mag_loss,         # unweighted magnitude L1
    "if_recon_loss": weighted_if_loss,  # unweighted mag-weighted IF L1
    "kl_loss": kl_loss,                # unweighted KL
    "recon_loss": recon_term,           # weighted recon total (for backward compat logging)
}
```

**NaN/stability guards:**
- Add `+ 1e-8` epsilon to any division
- If total_loss is NaN, log warning and return a zero-grad-safe fallback (recon_loss only, no STFT or KL)

Also add a factory function to create the STFT loss module:

```python
def create_stft_loss(loss_config: "LossConfig", device: "torch.device") -> "MultiResolutionSTFTLoss":
    """Create a MultiResolutionSTFTLoss from config, moved to device."""
    from auraloss.freq import MultiResolutionSTFTLoss
    return MultiResolutionSTFTLoss(
        fft_sizes=list(loss_config.stft_fft_sizes),
        hop_sizes=list(loss_config.stft_hop_sizes),
        win_lengths=list(loss_config.stft_win_lengths),
    ).to(device)
```

Update the module docstring to reflect the new combined loss.
  </action>
  <verify>
Run a quick smoke test:
```python
python -c "
import torch
from distill.training.config import LossConfig
from distill.models.losses import compute_combined_loss, create_stft_loss

config = LossConfig()
device = torch.device('cpu')
stft_fn = create_stft_loss(config, device)

B, C, H, W = 2, 2, 128, 94
recon = torch.randn(B, C, H, W)
target = torch.randn(B, C, H, W)
mu = torch.randn(B, 128)
logvar = torch.randn(B, 128)

result = compute_combined_loss(recon, target, mu, logvar, config, kl_weight=0.5, _stft_loss_fn=stft_fn)
print({k: v.item() for k, v in result.items()})
print('All finite:', all(torch.isfinite(v) for v in result.values()))
"
```
Should print all loss components as finite values.
  </verify>
  <done>compute_combined_loss returns a dict with total_loss, stft_loss, mag_recon_loss, if_recon_loss, kl_loss, and recon_loss -- all finite. The STFT loss operates at 3 resolutions. IF loss is magnitude-weighted. All weights come from LossConfig.</done>
</task>

</tasks>

<verification>
1. `python -c "from distill.training.config import LossConfig; print(LossConfig())"` prints defaults
2. `python -c "from distill.models.losses import compute_combined_loss, create_stft_loss"` imports without error
3. Smoke test with random tensors produces finite loss values for all components
4. `import auraloss` succeeds
5. Existing `vae_loss` function is unchanged (backward compatibility)
</verification>

<success_criteria>
- LossConfig dataclass with STFT, reconstruction, and KL weight sections exists in config.py
- compute_combined_loss computes multi-resolution STFT + magnitude-weighted IF recon + KL
- auraloss installed as project dependency
- All loss components return finite values on random input
- Existing vae_loss function preserved for backward compatibility
</success_criteria>

<output>
After completion, create `.planning/phases/14-multi-resolution-loss/14-01-SUMMARY.md`
</output>
