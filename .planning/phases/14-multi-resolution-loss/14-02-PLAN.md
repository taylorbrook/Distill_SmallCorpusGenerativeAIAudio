---
phase: 14-multi-resolution-loss
plan: 02
type: execute
wave: 2
depends_on: ["14-01"]
files_modified:
  - src/distill/training/loop.py
  - src/distill/training/metrics.py
  - src/distill/models/__init__.py
  - src/distill/training/__init__.py
autonomous: true
requirements: [LOSS-01, LOSS-02, LOSS-03, LOSS-04]

must_haves:
  truths:
    - "Training loop uses compute_combined_loss instead of vae_loss for the forward pass"
    - "Loss component weights are logged at training start as a config summary"
    - "Individual loss components (STFT, mag recon, IF recon, KL) are visible in training progress output"
    - "A divergence warning is logged if total loss increases for N consecutive epochs"
    - "Training on a small dataset converges (loss decreases, no NaN)"
  artifacts:
    - path: "src/distill/training/loop.py"
      provides: "Training loop wired to compute_combined_loss with STFT loss module, config summary, divergence detection"
      contains: "compute_combined_loss"
    - path: "src/distill/training/metrics.py"
      provides: "Extended StepMetrics and EpochMetrics with STFT and per-channel loss fields"
      contains: "stft_loss"
    - path: "src/distill/models/__init__.py"
      provides: "Re-export of compute_combined_loss and create_stft_loss"
      contains: "compute_combined_loss"
    - path: "src/distill/training/__init__.py"
      provides: "Re-export of LossConfig"
      contains: "LossConfig"
  key_links:
    - from: "src/distill/training/loop.py"
      to: "src/distill/models/losses.py"
      via: "import compute_combined_loss, create_stft_loss"
      pattern: "from distill.models.losses import compute_combined_loss"
    - from: "src/distill/training/loop.py"
      to: "src/distill/training/config.py"
      via: "config.loss access for LossConfig"
      pattern: "config\\.loss"
    - from: "src/distill/training/loop.py"
      to: "src/distill/training/metrics.py"
      via: "StepMetrics and EpochMetrics with new loss fields"
      pattern: "StepMetrics.*stft_loss"
---

<objective>
Wire the combined multi-resolution loss into the training loop with per-component logging, divergence detection, and config summary.

Purpose: Replace the single MSE-based vae_loss call with compute_combined_loss in train_epoch and validate_epoch, add loss component visibility to training output, and implement stability monitoring (divergence warning). This completes the Phase 14 integration -- training now uses perceptually grounded loss.

Output: Updated training loop, extended metrics, divergence detection
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-multi-resolution-loss/14-CONTEXT.md
@.planning/phases/14-multi-resolution-loss/14-01-SUMMARY.md

@src/distill/training/loop.py
@src/distill/training/metrics.py
@src/distill/models/losses.py
@src/distill/training/config.py
@src/distill/models/__init__.py
@src/distill/training/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend metrics and update __init__.py exports</name>
  <files>src/distill/training/metrics.py, src/distill/models/__init__.py, src/distill/training/__init__.py</files>
  <action>
1. In `src/distill/training/metrics.py`, add optional loss component fields to StepMetrics and EpochMetrics. Use default values so existing code that constructs these without the new fields still works:

**StepMetrics** -- add after `kl_loss`:
```python
stft_loss: float = 0.0           # Multi-resolution STFT loss (unweighted)
mag_recon_loss: float = 0.0      # Magnitude channel L1 loss (unweighted)
if_recon_loss: float = 0.0       # IF channel mag-weighted L1 loss (unweighted)
```

**EpochMetrics** -- add after `val_kl_loss`:
```python
val_stft_loss: float = 0.0       # Validation STFT loss (unweighted)
val_mag_recon_loss: float = 0.0  # Validation magnitude L1 loss (unweighted)
val_if_recon_loss: float = 0.0   # Validation IF mag-weighted L1 loss (unweighted)
```

2. Update `MetricsHistory.to_dict()` to include the new fields in both step_metrics and epoch_metrics serialization dicts. Use `.get()` pattern in `from_dict()` so that old checkpoints without these fields can still be deserialized (default to 0.0).

3. In `src/distill/models/__init__.py`, add `compute_combined_loss` and `create_stft_loss` to the imports from `distill.models.losses` and the `__all__` list.

4. In `src/distill/training/__init__.py`, add `LossConfig` to the imports from `distill.training.config` and the `__all__` list.
  </action>
  <verify>
```
python -c "
from distill.training.metrics import StepMetrics, EpochMetrics, MetricsHistory
s = StepMetrics(epoch=0, step=0, total_steps=10, train_loss=1.0, recon_loss=0.5, kl_loss=0.1, kl_weight=0.5, learning_rate=1e-3, step_time_s=0.1, stft_loss=0.3, mag_recon_loss=0.2, if_recon_loss=0.1)
print(s.stft_loss, s.mag_recon_loss, s.if_recon_loss)
# Test backward compat -- old-style construction without new fields
s2 = StepMetrics(epoch=0, step=0, total_steps=10, train_loss=1.0, recon_loss=0.5, kl_loss=0.1, kl_weight=0.5, learning_rate=1e-3, step_time_s=0.1)
print(s2.stft_loss)  # should be 0.0
from distill.models import compute_combined_loss, create_stft_loss
from distill.training import LossConfig
print('All imports OK')
"
```
  </verify>
  <done>StepMetrics and EpochMetrics have STFT and per-channel loss fields with backward-compatible defaults. Serialization handles old checkpoints. compute_combined_loss, create_stft_loss, and LossConfig are re-exported.</done>
</task>

<task type="auto">
  <name>Task 2: Wire compute_combined_loss into training loop with logging and divergence detection</name>
  <files>src/distill/training/loop.py</files>
  <action>
Modify `src/distill/training/loop.py` to replace `vae_loss` with `compute_combined_loss` in both `train_epoch` and `validate_epoch`, add a config summary at training start, and implement divergence detection.

**1. Update imports in train_epoch and validate_epoch:**
Replace `from distill.models.losses import vae_loss` with:
```python
from distill.models.losses import compute_combined_loss, create_stft_loss
```
Keep `get_kl_weight` import in `train()`.

**2. Create STFT loss module once in `train()`:**
After model creation and before the training loop, add:
```python
from distill.models.losses import create_stft_loss
stft_loss_fn = create_stft_loss(config.loss, device)
```
Pass `stft_loss_fn` to both `train_epoch` and `validate_epoch` as a new parameter.

**3. Add config summary logging at training start:**
After the data loader creation section and before the training loop, print a loss configuration summary:
```python
print(f"[TRAIN] Loss config: STFT weight={config.loss.stft_weight}, "
      f"recon weight={config.loss.recon_weight} "
      f"(mag={config.loss.magnitude_weight}, IF={config.loss.if_weight}), "
      f"KL max={config.loss.kl_weight_max}, "
      f"STFT resolutions={config.loss.stft_fft_sizes}", flush=True)
```

**4. Update train_epoch:**
- Add `stft_loss_fn` parameter and `loss_config: "LossConfig"` parameter
- Replace the `vae_loss(...)` call with:
  ```python
  loss_dict = compute_combined_loss(
      recon, mel, mu, logvar, loss_config,
      kl_weight=kl_weight, _stft_loss_fn=stft_loss_fn,
  )
  total = loss_dict["total_loss"]
  ```
- Update accumulation: track stft_loss, mag_recon_loss, if_recon_loss alongside existing totals
- Update the print statement to show STFT and recon components:
  ```python
  f"loss={total.item():.4f}  stft={loss_dict['stft_loss'].item():.4f}  "
  f"recon={loss_dict['recon_loss'].item():.4f}  kl={loss_dict['kl_loss'].item():.4f}  "
  ```
- Update StepMetrics emission with new fields: `stft_loss=..., mag_recon_loss=..., if_recon_loss=...`
- Update return dict: add `stft_loss`, `mag_recon_loss`, `if_recon_loss` averages
- Keep `recon_loss` in return for backward compatibility (now = weighted recon term)

**5. Update validate_epoch:**
- Add `stft_loss_fn` parameter and `loss_config: "LossConfig"` parameter
- Replace `vae_loss(...)` with `compute_combined_loss(...)` (same pattern as train_epoch)
- Accumulate and average stft_loss, mag_recon_loss, if_recon_loss
- Keep `compute_kl_divergence` for raw KL monitoring
- Return dict: add `val_stft_loss`, `val_mag_recon_loss`, `val_if_recon_loss`

**6. Update train() orchestrator:**
- Pass `stft_loss_fn` and `config.loss` to train_epoch and validate_epoch calls
- Use `config.loss.kl_weight_max` and `config.loss.kl_warmup_fraction` for `get_kl_weight` calls (fall back to `config.kl_weight_max` / `config.kl_warmup_fraction` if loss config not present -- but since LossConfig has defaults this should always work)
- Update EpochMetrics construction with new val_stft_loss, val_mag_recon_loss, val_if_recon_loss fields

**7. Add divergence detection:**
After epoch metrics recording in the main training loop, add divergence detection logic:
```python
# Divergence detection: warn if total loss increases for N consecutive epochs
consecutive_increase_threshold = 5
if len(metrics_history.epoch_metrics) >= consecutive_increase_threshold:
    recent = metrics_history.epoch_metrics[-consecutive_increase_threshold:]
    losses = [m.train_loss for m in recent]
    if all(losses[i] < losses[i+1] for i in range(len(losses)-1)):
        logger.warning(
            "DIVERGENCE WARNING: Total loss has increased for %d consecutive epochs "
            "(%.4f -> %.4f). Consider reducing learning rate or adjusting loss weights.",
            consecutive_increase_threshold, losses[0], losses[-1],
        )
```

**Important:** The `free_bits` parameter for KL is now in `config.loss.kl_free_bits`. Pass it through from the LossConfig. Remove the standalone `free_bits` parameter from `train_epoch` and `validate_epoch` since it now comes from loss_config.
  </action>
  <verify>
```
python -c "
import torch
from distill.training.loop import train_epoch, validate_epoch
# Verify function signatures accept new parameters
import inspect
sig_train = inspect.signature(train_epoch)
sig_val = inspect.signature(validate_epoch)
print('train_epoch params:', list(sig_train.parameters.keys()))
print('validate_epoch params:', list(sig_val.parameters.keys()))
assert 'stft_loss_fn' in sig_train.parameters, 'Missing stft_loss_fn in train_epoch'
assert 'loss_config' in sig_train.parameters, 'Missing loss_config in train_epoch'
assert 'stft_loss_fn' in sig_val.parameters, 'Missing stft_loss_fn in validate_epoch'
assert 'loss_config' in sig_val.parameters, 'Missing loss_config in validate_epoch'
print('All signatures correct')
"
```

Verify the full train() function can be imported and the config summary path works:
```
python -c "
from distill.training.loop import train
from distill.training.config import TrainingConfig
c = TrainingConfig()
print('loss config accessible:', c.loss)
print('stft_weight:', c.loss.stft_weight)
print('PASS')
"
```
  </verify>
  <done>train_epoch and validate_epoch use compute_combined_loss with per-component tracking. Config summary prints at training start. Divergence detection warns after 5 consecutive loss increases. All loss components flow through to StepMetrics and EpochMetrics. KL weight and free_bits come from LossConfig.</done>
</task>

</tasks>

<verification>
1. `python -c "from distill.training.loop import train"` imports without error
2. `python -c "from distill.training.metrics import StepMetrics; s = StepMetrics(epoch=0, step=0, total_steps=1, train_loss=1.0, recon_loss=0.5, kl_loss=0.1, kl_weight=0.5, learning_rate=1e-3, step_time_s=0.1); print(s.stft_loss)"` prints 0.0
3. train_epoch and validate_epoch signatures include stft_loss_fn and loss_config parameters
4. compute_combined_loss and create_stft_loss are importable from distill.models
5. LossConfig is importable from distill.training
6. MetricsHistory.to_dict() / from_dict() round-trips with new fields
</verification>

<success_criteria>
- Training loop calls compute_combined_loss instead of vae_loss
- Loss config summary printed at training start
- Per-component loss values (STFT, mag recon, IF recon, KL) visible in step-level training output
- Extended StepMetrics and EpochMetrics carry all loss components
- Divergence detection warns after consecutive loss increases
- Old checkpoints without new metric fields can still be deserialized
</success_criteria>

<output>
After completion, create `.planning/phases/14-multi-resolution-loss/14-02-SUMMARY.md`
</output>
