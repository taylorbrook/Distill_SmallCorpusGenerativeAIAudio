---
phase: 16-encode-decode-code-visualization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/distill/inference/codes.py
  - src/distill/inference/__init__.py
  - src/distill/ui/components/code_grid.py
autonomous: true
requirements: [CODE-01, CODE-02, CODE-03, CODE-07, CODE-09]

must_haves:
  truths:
    - "encode_audio_file() returns code indices, spatial_shape, mel_shape, and metadata from any audio file"
    - "decode_code_grid() reconstructs audio waveform from code indices"
    - "preview_single_code() produces audio for a single codebook entry"
    - "preview_time_slice() produces audio for all levels at one time position"
    - "play_row_audio() concatenates decoded audio for one level across all time positions"
    - "render_code_grid() produces interactive HTML with per-cell coloring, JS onclick, and playhead"
    - "Level labels follow cascading scheme: Structure/Detail (2L), Structure/Timbre/Detail (3L), Structure/Timbre/Texture/Detail (4L)"
  artifacts:
    - path: "src/distill/inference/codes.py"
      provides: "Encode/decode/preview pipeline functions"
      exports: ["encode_audio_file", "decode_code_grid", "preview_single_code", "preview_time_slice", "play_row_audio"]
    - path: "src/distill/ui/components/code_grid.py"
      provides: "HTML grid renderer with JS bridge, playhead, level labels"
      exports: ["render_code_grid", "DEFAULT_LEVEL_LABELS"]
  key_links:
    - from: "src/distill/inference/codes.py"
      to: "distill.models.vqvae.ConvVQVAE"
      via: "model.forward(), model.codes_to_embeddings(), model.decode()"
      pattern: "loaded\\.model\\.(forward|codes_to_embeddings|decode)"
    - from: "src/distill/inference/codes.py"
      to: "distill.audio.spectrogram.AudioSpectrogram"
      via: "spectrogram.waveform_to_mel(), spectrogram.mel_to_waveform()"
      pattern: "loaded\\.spectrogram\\.(waveform_to_mel|mel_to_waveform)"
    - from: "src/distill/ui/components/code_grid.py"
      to: "codes_tab.py (consumer)"
      via: "JS onclick dispatches to hidden Textbox with elem_id"
      pattern: "code-grid-cell-clicked|dispatchEvent"
---

<objective>
Build the backend encode/decode/preview pipeline and the interactive HTML code grid renderer that Phase 16's Codes tab will consume.

Purpose: Provide all computational functions (encode audio to codes, decode codes to audio, preview individual codebook entries) and the visual grid renderer (HTML with CSS grid, cell coloring, JS click bridge, playhead animation) as reusable modules. The Codes tab (Plan 02) will wire these together.

Output:
- `src/distill/inference/codes.py` -- Pipeline functions for encode, decode, and preview
- `src/distill/ui/components/code_grid.py` -- Interactive HTML grid renderer
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-encode-decode-code-visualization/16-RESEARCH.md

@src/distill/inference/generation.py
@src/distill/inference/__init__.py
@src/distill/models/vqvae.py
@src/distill/models/persistence.py
@src/distill/audio/io.py
@src/distill/audio/spectrogram.py
@src/distill/ui/components/model_card.py

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/distill/models/persistence.py:
```python
@dataclass
class LoadedVQModel:
    model: "ConvVQVAE"
    spectrogram: "AudioSpectrogram"
    metadata: ModelMetadata
    device: "torch.device"
    codebook_health: dict | None
    vqvae_config: dict | None
    prior: "CodePrior | None" = None
    prior_config: dict | None = None
    prior_metadata: dict | None = None
```

From src/distill/models/vqvae.py:
```python
class ConvVQVAE(nn.Module):
    def encode(self, x: torch.Tensor) -> torch.Tensor: ...
    def quantize(self, z: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: ...
    def decode(self, quantized: torch.Tensor, target_shape: tuple[int, int] | None = None) -> torch.Tensor: ...
    def codes_to_embeddings(self, indices: torch.Tensor, spatial_shape: tuple[int, int]) -> torch.Tensor: ...
    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: ...
    # Properties: num_quantizers, codebook_size, codebook_dim
    # Side effect: self._spatial_shape set during forward()
```

From src/distill/audio/io.py:
```python
def load_audio(path: str | Path, target_sample_rate: int = 48000) -> AudioFile:
    # Returns AudioFile with .waveform [channels, samples] and .sample_rate
```

From src/distill/audio/spectrogram.py:
```python
class AudioSpectrogram:
    def waveform_to_mel(self, waveform: torch.Tensor) -> torch.Tensor: ...
        # Input: [B, 1, samples], Output: [B, 1, n_mels, time]
    def mel_to_waveform(self, mel: torch.Tensor) -> torch.Tensor: ...
        # Input: [B, 1, n_mels, time], Output: [B, 1, samples]
```

From src/distill/ui/components/model_card.py (JS bridge pattern):
```python
# Hidden textbox elem_id pattern for JS->Python event communication:
# onclick="
#   var tb = document.querySelector('#model-card-selected-name textarea');
#   var nativeSet = Object.getOwnPropertyDescriptor(
#     window.HTMLTextAreaElement.prototype, 'value').set;
#   nativeSet.call(tb, value);
#   tb.dispatchEvent(new Event('input', {bubbles: true}));
# "
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create inference/codes.py with encode/decode pipeline and public API exports</name>
  <files>
    src/distill/inference/codes.py
    src/distill/inference/__init__.py
  </files>
  <action>
Create `src/distill/inference/codes.py` with the following functions:

1. **`encode_audio_file(audio_path, loaded)`** -- Encode an audio file to VQ-VAE code indices.
   - Accept `audio_path: Path` and `loaded: LoadedVQModel`
   - Load audio via `load_audio(audio_path, target_sample_rate=48000)`
   - Mono mixdown if stereo: `waveform.mean(dim=0, keepdim=True)`
   - Convert to mel: `loaded.spectrogram.waveform_to_mel(waveform.unsqueeze(0).to(loaded.device))`
   - Store `mel_shape = (mel.shape[2], mel.shape[3])` BEFORE forward pass
   - Run `loaded.model(mel)` in `torch.no_grad()` with `model.eval()`
   - Capture `spatial_shape = loaded.model._spatial_shape` IMMEDIATELY after forward (Pitfall 1 from research)
   - Return a dict with keys: `indices` (cpu tensor [1, H*W, num_quantizers]), `spatial_shape`, `mel_shape`, `num_quantizers`, `codebook_size`, `duration_s` (waveform.shape[-1] / 48000)

2. **`decode_code_grid(indices, spatial_shape, mel_shape, loaded)`** -- Decode code indices back to audio.
   - Accept `indices: torch.Tensor`, `spatial_shape: tuple[int,int]`, `mel_shape: tuple[int,int]`, `loaded: LoadedVQModel`
   - In `torch.no_grad()`: `codes_to_embeddings(indices.to(device), spatial_shape)` -> `decode(quantized, target_shape=mel_shape)` -> `mel_to_waveform(mel)`
   - Return 1-D float32 numpy array at 48kHz

3. **`preview_single_code(level, code_index, loaded, spatial_shape, mel_shape)`** -- Audio for one codebook entry.
   - Create zeroed indices tensor `[1, seq_len, num_q]` on loaded.device
   - Set `indices[0, :, level] = code_index` (target code at all positions for this level)
   - Call `decode_code_grid()` and return result

4. **`preview_time_slice(position, full_indices, loaded, spatial_shape, mel_shape)`** -- Audio for all levels at one time position.
   - Extract codes at `position` from `full_indices`: `pos_codes = full_indices[0, position, :]`
   - Broadcast to all positions: create tensor [1, seq_len, num_q] filled with pos_codes
   - Call `decode_code_grid()` and return result

5. **`play_row_audio(level, full_indices, loaded, spatial_shape, mel_shape)`** -- Concatenated audio for one level across all time positions.
   - For each position in range(seq_len): create single-position preview (single code at that level, zeros elsewhere), decode to audio
   - Concatenate all position audios into one continuous array
   - Return 1-D float32 numpy array

Use lazy imports (inside function bodies) for `load_audio` and type hints from `TYPE_CHECKING` for heavy modules. Follow the pattern in `inference/generation.py`.

Then update `src/distill/inference/__init__.py` to export `encode_audio_file`, `decode_code_grid`, `preview_single_code`, `preview_time_slice`, `play_row_audio` from `distill.inference.codes`.
  </action>
  <verify>
    <automated>cd H:/dev/Distill-vqvae && python -c "from distill.inference.codes import encode_audio_file, decode_code_grid, preview_single_code, preview_time_slice, play_row_audio; print('All imports OK')"</automated>
  </verify>
  <done>
    - encode_audio_file returns dict with indices, spatial_shape, mel_shape, num_quantizers, codebook_size, duration_s
    - decode_code_grid returns 1-D float32 numpy array
    - preview_single_code returns audio for single codebook entry
    - preview_time_slice returns audio for full column (all levels at one position)
    - play_row_audio returns concatenated audio for one level
    - All five functions exported from distill.inference
  </done>
</task>

<task type="auto">
  <name>Task 2: Create code_grid.py HTML renderer with level labels, cell coloring, JS bridge, and playhead</name>
  <files>src/distill/ui/components/code_grid.py</files>
  <action>
Create `src/distill/ui/components/code_grid.py` with:

1. **`DEFAULT_LEVEL_LABELS`** dict -- Cascading label scheme per user decision:
   ```python
   DEFAULT_LEVEL_LABELS = {
       2: ["Structure", "Detail"],
       3: ["Structure", "Timbre", "Detail"],
       4: ["Structure", "Timbre", "Texture", "Detail"],
   }
   ```

2. **`get_level_labels(num_quantizers, custom_labels=None)`** -- Returns level labels. Use custom_labels if provided, otherwise fall back to DEFAULT_LEVEL_LABELS, otherwise "Level N".

3. **`render_code_grid(indices, num_quantizers, codebook_size, spatial_shape, level_labels, selected_cell, duration_s)`** -- Returns HTML string.

   Grid structure:
   - CSS grid layout with fixed left column (80px) for row labels, then seq_len data columns (24-28px each)
   - Rows = quantizer levels (coarsest at top = level 0 = "Structure")
   - Columns = spatial positions (H*W flattened, left to right = time progression)
   - First row is column headers: clicking a column header dispatches `"col,{pos}"` to the hidden textbox
   - Add a header row with time markers every ~10 positions (show approximate seconds)

   Cell rendering:
   - Each cell background color derived from `matplotlib.cm.tab20(code_index % 20)` converted to hex via `matplotlib.colors.to_hex()`
   - Display code index number inside each cell (small font, 9-10px)
   - Text color should be white or black depending on background luminance (for readability)
   - Selected cell gets a thick border (3px solid white or contrasting color)

   Row labels:
   - Fixed left column showing level labels ("Structure", "Timbre", "Detail")
   - Each row label also gets a "Play" button that dispatches `"row,{level}"` to the hidden textbox
   - Labels always visible (not scrolled away) -- use `position: sticky; left: 0`

   JS onclick bridge (per CONTEXT.md and model_card.py pattern):
   - Cell click dispatches `"cell,{level},{position}"` to hidden textbox `#code-grid-cell-clicked`
   - Column header click dispatches `"col,{position}"` to same textbox
   - Row play button dispatches `"row,{level}"` to same textbox
   - Use the exact `nativeSet` + `dispatchEvent` pattern from model_card.py

   Playhead:
   - Include a `<div class="playhead-line">` with absolute positioning inside the grid container
   - CSS animation `playhead-sweep` from left:0 to left:100% over `var(--duration)` seconds
   - Default state: `animation-play-state: paused`
   - Add class `.playing` to start: `animation-play-state: running`
   - Include JS functions `startPlayhead(durationSec)` and `stopPlayhead()` that add/remove the `.playing` class and set `--duration`
   - The playhead only covers the data area (skip the sticky label column)

   Horizontal scroll:
   - The grid container should have `overflow-x: auto` for audio longer than one screen
   - Row labels use `position: sticky; left: 0; z-index: 5` to stay visible during scroll

   Empty state:
   - If `indices is None`, return a centered placeholder: "Upload an audio file and click Encode to see codes here."

   Use `html.escape()` for any user-provided text. Import `matplotlib.pyplot` and `matplotlib.colors` at module level (already installed, lightweight for color conversion only).
  </action>
  <verify>
    <automated>cd H:/dev/Distill-vqvae && python -c "from distill.ui.components.code_grid import render_code_grid, DEFAULT_LEVEL_LABELS, get_level_labels; labels = get_level_labels(3); assert labels == ['Structure', 'Timbre', 'Detail']; html = render_code_grid(None, 3, 128, (8, 6), labels, None, 1.0); assert 'Upload' in html; print('Grid renderer OK')"</automated>
  </verify>
  <done>
    - render_code_grid produces valid HTML with CSS grid layout
    - Level labels follow cascading scheme (2L: Structure/Detail, 3L: Structure/Timbre/Detail, 4L: Structure/Timbre/Texture/Detail)
    - Each cell has background color from matplotlib colormap and displays code index
    - JS onclick dispatches cell/col/row events to #code-grid-cell-clicked hidden textbox
    - Playhead div with CSS animation included
    - Row labels are sticky (visible during horizontal scroll)
    - Empty state shows placeholder message when indices is None
  </done>
</task>

</tasks>

<verification>
1. `python -c "from distill.inference.codes import encode_audio_file, decode_code_grid, preview_single_code, preview_time_slice, play_row_audio"` -- All pipeline functions importable
2. `python -c "from distill.ui.components.code_grid import render_code_grid, DEFAULT_LEVEL_LABELS"` -- Grid renderer importable
3. `python -c "import torch; from distill.ui.components.code_grid import render_code_grid, get_level_labels; idx = torch.randint(0, 64, (1, 48, 3)); html = render_code_grid(idx, 3, 64, (8, 6), get_level_labels(3), None, 1.0); assert 'code-cell' in html; assert 'Structure' in html; assert 'playhead' in html; print('Full grid render OK')"` -- Grid renders with real tensor data
</verification>

<success_criteria>
- All 5 pipeline functions exist in inference/codes.py and are importable from distill.inference
- render_code_grid produces interactive HTML with cell coloring, JS bridge, level labels, and playhead
- Level labels correctly cascade based on num_quantizers (2/3/4 levels)
- No new dependencies required (all libraries already installed)
</success_criteria>

<output>
After completion, create `.planning/phases/16-encode-decode-code-visualization/16-01-SUMMARY.md`
</output>
