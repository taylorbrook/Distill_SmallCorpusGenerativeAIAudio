---
phase: 13-vq-vae-training-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/distill/training/metrics.py
  - src/distill/training/loop.py
  - src/distill/training/checkpoint.py
  - src/distill/models/persistence.py
  - src/distill/training/preview.py
  - src/distill/training/runner.py
autonomous: true
requirements:
  - VQVAE-04
  - VQVAE-07
  - PERS-01

must_haves:
  truths:
    - "VQ-VAE training loop runs end-to-end: model trains, validates, emits metrics, and saves checkpoints"
    - "Per-level codebook health (utilization, perplexity, dead codes) is computed and emitted with epoch metrics"
    - "Warning is emitted when any codebook level drops below 30% utilization"
    - "Trained VQ-VAE model saves as v2 .distill file with codebook health snapshot and loss history"
    - "v2 .distill file loads back to a functional ConvVQVAE model with correct weights and metadata"
  artifacts:
    - path: "src/distill/training/metrics.py"
      provides: "VQStepMetrics, VQEpochMetrics, VQMetricsHistory dataclasses and updated MetricsCallback"
      contains: "class VQStepMetrics"
    - path: "src/distill/training/loop.py"
      provides: "train_vqvae_epoch, validate_vqvae_epoch, train_vqvae functions"
      contains: "def train_vqvae"
    - path: "src/distill/models/persistence.py"
      provides: "save_model_v2, load_model_v2, LoadedVQModel"
      contains: "def save_model_v2"
  key_links:
    - from: "src/distill/training/loop.py"
      to: "src/distill/models/vqvae.py"
      via: "ConvVQVAE forward pass in train_vqvae_epoch"
      pattern: "model\\(mel\\)"
    - from: "src/distill/training/loop.py"
      to: "src/distill/models/losses.py"
      via: "vqvae_loss call in train_vqvae_epoch"
      pattern: "vqvae_loss\\("
    - from: "src/distill/training/loop.py"
      to: "src/distill/training/metrics.py"
      via: "VQStepMetrics and VQEpochMetrics emission"
      pattern: "VQStepMetrics\\(|VQEpochMetrics\\("
    - from: "src/distill/training/loop.py"
      to: "src/distill/models/persistence.py"
      via: "save_model_v2 call at end of training"
      pattern: "save_model_v2\\("
---

<objective>
Build the VQ-VAE training loop, VQ-specific metrics infrastructure, codebook health monitoring with low-utilization warnings, and v2 model persistence format.

Purpose: This is the foundational plan -- all UI and CLI integration in later plans depends on these training loop functions and metrics dataclasses existing and working correctly.

Output: Working `train_vqvae()` orchestrator, VQ metrics dataclasses, v2 save/load functions, and VQ-specific preview/checkpoint support.
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-vq-vae-training-pipeline/13-CONTEXT.md
@.planning/phases/13-vq-vae-training-pipeline/13-RESEARCH.md

@src/distill/training/metrics.py
@src/distill/training/loop.py
@src/distill/training/checkpoint.py
@src/distill/training/preview.py
@src/distill/training/runner.py
@src/distill/training/config.py
@src/distill/models/persistence.py
@src/distill/models/vqvae.py
@src/distill/models/losses.py
@src/distill/models/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add VQ metrics dataclasses and VQ-VAE training loop with codebook health</name>
  <files>
    src/distill/training/metrics.py
    src/distill/training/loop.py
    src/distill/training/preview.py
    src/distill/training/checkpoint.py
    src/distill/training/runner.py
  </files>
  <action>
**metrics.py** -- Add VQ-specific metrics dataclasses BELOW the existing v1.0 classes (keep all existing classes intact):

1. `VQStepMetrics` dataclass with fields: `epoch`, `step`, `total_steps`, `train_loss`, `recon_loss`, `commit_loss`, `commitment_weight`, `learning_rate`, `step_time_s`, `codebook_health: dict[str, dict[str, float | int]] | None = None`.

2. `VQEpochMetrics` dataclass with fields: `epoch`, `total_epochs`, `train_loss`, `val_loss`, `val_recon_loss`, `val_commit_loss`, `overfitting_gap`, `learning_rate`, `eta_seconds`, `elapsed_seconds`, `codebook_health: dict[str, dict[str, float | int]] | None = None`, `utilization_warnings: list[str] | None = None`.

3. `VQMetricsHistory` dataclass mirroring `MetricsHistory` but storing `VQStepMetrics` and `VQEpochMetrics`. Provide: `add_step()`, `add_epoch()`, `get_loss_curves()` (returning `train_losses`, `val_losses`, `recon_losses`, `commit_losses`), `get_best_epoch()`, `compute_eta()`, `is_overfitting()`, `to_dict()`, `from_dict()` classmethod. The `to_dict()` / `from_dict()` must correctly serialize/deserialize codebook_health and utilization_warnings.

4. Update the `MetricsCallback` type alias to accept the union of ALL event types (both v1.0 and v1.1): `StepMetrics | EpochMetrics | VQStepMetrics | VQEpochMetrics | PreviewEvent | TrainingCompleteEvent`.

**loop.py** -- Add VQ-VAE training functions BELOW the existing v1.0 functions (keep `train_epoch`, `validate_epoch`, `train` untouched):

1. `train_vqvae_epoch()` -- Mirror `train_epoch()` structure but:
   - Accept `commitment_weight: float = 0.25` instead of `kl_weight` and `free_bits`.
   - Forward pass: `recon, indices, commit_loss = model(mel)`.
   - Loss: `total, recon_loss, weighted_commit = vqvae_loss(recon, mel, commit_loss, commitment_weight)`.
   - NaN detection: same pattern as v1.0 (skip gradient update on NaN).
   - Codebook health: Compute via `model.quantizer.get_codebook_utilization(indices)` every 10 steps (not every step -- O(batch*seq) cost). Store in step metrics.
   - Skip codebook health check at step 0 of epoch 0 (k-means not yet initialized, would give misleading 0% -- see Pitfall 1 in RESEARCH.md).
   - Emit `VQStepMetrics` via callback.
   - Return `{train_loss, recon_loss, commit_loss}`.

2. `validate_vqvae_epoch()` -- Mirror `validate_epoch()` but:
   - No `kl_weight`/`free_bits` params. Accept `commitment_weight`.
   - Forward pass: `recon, indices, commit_loss = model(mel)`.
   - Loss: `vqvae_loss(recon, mel, commit_loss, commitment_weight)`.
   - Compute codebook health on the full validation set by accumulating indices, then calling `get_codebook_utilization()` once at the end.
   - Return `{val_loss, val_recon_loss, val_commit_loss, codebook_health}`.

3. `train_vqvae()` -- Full orchestrator mirroring `train()` but:
   - Accept `VQVAEConfig` instead of `TrainingConfig`. Accept `models_dir`, `dataset_name`, `model_name` like `train()`.
   - Create `ConvVQVAE` using VQVAEConfig fields (`codebook_dim`, `codebook_size`, `num_quantizers`, `decay`, `commitment_weight`, `threshold_ema_dead_code`, `kmeans_init`, `kmeans_iters`, `dropout`).
   - Use `AdamW` optimizer, `CosineAnnealingLR` scheduler (same as v1.0).
   - Create data loaders using `create_data_loaders()` (same as v1.0).
   - Per epoch: call `train_vqvae_epoch()`, check cancellation, call `validate_vqvae_epoch()`, step scheduler.
   - After validation: compute overfitting gap, ETA, create `VQEpochMetrics` with codebook_health from validation.
   - **Low utilization warning** (VQVAE-07): After computing codebook health, check each level. If any level's utilization < 0.30 AND epoch > 0, append warning string like `"Level {N}: utilization {pct:.0%} (below 30% threshold)"` to `utilization_warnings` list. Log warning via `logger.warning()`.
   - Record in `VQMetricsHistory`, emit via callback.
   - Overfitting warning: same pattern as v1.0 (gap > 0.2).
   - Preview generation: Use `generate_vqvae_reconstruction_preview()` (see below) instead of `generate_preview()` (which calls `model.sample()` -- VQ-VAE has no `sample()` method).
   - Checkpoint saving: Use `_save_vqvae_checkpoint_safe()` helper (extend checkpoint with VQ-specific fields -- see below).
   - **End of training**: Skip latent space analysis (v1.0-only). Instead, compute final codebook health snapshot. Call `save_model_v2()` to save to library.
   - Emit `TrainingCompleteEvent` at end.
   - Return `{model, metrics_history, output_dir, best_checkpoint_path, final_codebook_health}`.

**preview.py** -- Add VQ-VAE reconstruction preview function (keep existing functions intact):

1. `generate_vqvae_reconstruction_preview()` -- Similar to existing `generate_reconstruction_preview()` but:
   - Forward pass: `recon, _indices, _commit_loss = model(mel)` (VQ-VAE returns 3 values, not `(recon, mu, logvar)`).
   - Same structure: take a sample batch of mel spectrograms, encode-quantize-decode, save original + reconstruction WAV pairs.
   - Accept `sample_batch` as waveform tensor `[B, 1, samples]`, convert to mel internally using spectrogram converter (the training loop will pass a waveform batch from val_loader).

**checkpoint.py** -- Add VQ-VAE checkpoint support:

1. Add `save_vqvae_checkpoint()` function (or extend existing). The simplest approach: create a new function `save_vqvae_checkpoint()` that saves the same structure as `save_checkpoint()` but replaces `kl_weight` with `commitment_weight`, adds `codebook_health: dict | None`, and uses `VQMetricsHistory.to_dict()` for metrics. Keep `save_checkpoint()` untouched.

2. Add `load_vqvae_checkpoint()` to load VQ-VAE checkpoints with the VQ-specific fields.

**runner.py** -- Add VQ-VAE training mode to `TrainingRunner`:

1. Add `start_vqvae()` method that mirrors `start()` but accepts `VQVAEConfig` and calls `train_vqvae()` instead of `train()`.
2. Add `_run_vqvae_training()` internal method as the thread target.
3. Keep all existing methods unchanged.
  </action>
  <verify>
Run `python -c "from distill.training.metrics import VQStepMetrics, VQEpochMetrics, VQMetricsHistory; print('metrics OK')"` and `python -c "from distill.training.loop import train_vqvae_epoch, validate_vqvae_epoch, train_vqvae; print('loop OK')"` and `python -c "from distill.training.runner import TrainingRunner; print('runner OK')"` -- all must print OK without import errors.
  </verify>
  <done>
VQ-VAE training loop exists and can be invoked. VQ metrics dataclasses serialize/deserialize correctly. Codebook health is computed during validation and emitted with epoch metrics. Low utilization warnings are generated when any level drops below 30%. VQ-specific checkpoint save/load works. TrainingRunner has start_vqvae() method.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement v2 model persistence (save and load)</name>
  <files>
    src/distill/models/persistence.py
    src/distill/models/__init__.py
  </files>
  <action>
**persistence.py** -- Add v2 save/load functions BELOW existing v1.0 functions (keep `save_model`, `load_model`, `delete_model`, `save_model_from_checkpoint` completely unchanged):

1. Add constant `SAVED_MODEL_VERSION_V2 = 2`.

2. Add `LoadedVQModel` dataclass (parallel to `LoadedModel`):
   - Fields: `model: ConvVQVAE`, `spectrogram: AudioSpectrogram`, `metadata: ModelMetadata`, `device: torch.device`, `codebook_health: dict | None`, `vqvae_config: dict | None`.
   - No `analysis` field (VQ-VAE replaces latent analysis with codebook health).

3. `save_model_v2()` function:
   - Accept: `model: ConvVQVAE`, `spectrogram_config: dict`, `vqvae_config: dict`, `training_config: dict`, `metadata: ModelMetadata`, `models_dir: Path`, `codebook_health: dict | None = None`, `loss_curve_history: dict | None = None`.
   - Auto-generate `model_id` and `save_date` if empty (same pattern as v1.0).
   - Build saved dict:
     ```python
     saved = {
         "format": MODEL_FORMAT_MARKER,  # "distill_model"
         "version": SAVED_MODEL_VERSION_V2,  # 2
         "model_type": "vqvae",
         "model_state_dict": model.state_dict(),
         "vqvae_config": vqvae_config,
         "spectrogram_config": spectrogram_config,
         "training_config": training_config,
         "codebook_health_snapshot": codebook_health,
         "loss_curve_history": loss_curve_history,
         "metadata": asdict(metadata),
     }
     ```
   - Use same filename sanitization and duplicate handling as v1.0.
   - Save via `torch.save()`.
   - Create `ModelEntry` and update `ModelLibrary` index (same pattern as v1.0).
   - Return Path to saved file.

4. `load_model_v2()` function:
   - Accept: `model_path: Path`, `device: str = "cpu"`.
   - Load via `torch.load()`.
   - Validate: `format == MODEL_FORMAT_MARKER` and `version >= 2` and `model_type == "vqvae"`.
   - If version == 1 or model_type != "vqvae": raise `ValueError("Not a v2 VQ-VAE model. Use load_model() for v1 models.")`.
   - Reconstruct `SpectrogramConfig` and `AudioSpectrogram` from saved `spectrogram_config`.
   - Reconstruct `ConvVQVAE` from `vqvae_config`: extract `codebook_dim`, `codebook_size`, `num_quantizers`, `decay`, `commitment_weight`, `threshold_ema_dead_code`, `dropout` from the dict. Then `ConvVQVAE(codebook_dim=..., codebook_size=..., ...)`.
   - Initialize ConvVQVAE dimensions: run a dummy forward pass with a zero tensor matching the spectrogram config dimensions to materialize lazy layers, then load state dict. (Alternatively, use the same spatial computation as load_model does for ConvVAE, adapted for VQEncoder/VQDecoder architecture).
   - Load `model_state_dict` via `model.load_state_dict()`.
   - Move to device, set `model.eval()`.
   - Reconstruct `ModelMetadata` from saved metadata dict.
   - Return `LoadedVQModel(model=model, spectrogram=spectrogram, metadata=metadata, device=torch_device, codebook_health=saved.get("codebook_health_snapshot"), vqvae_config=saved.get("vqvae_config"))`.

**models/__init__.py** -- Add v2 exports:
- Import and re-export: `save_model_v2`, `load_model_v2`, `LoadedVQModel`, `SAVED_MODEL_VERSION_V2` from persistence.py.
- Add to `__all__` list.
  </action>
  <verify>
Write a quick smoke test: `python -c "from distill.models.persistence import save_model_v2, load_model_v2, LoadedVQModel, SAVED_MODEL_VERSION_V2; print('v2 persistence OK')"`. Also verify v1.0 still works: `python -c "from distill.models.persistence import save_model, load_model; print('v1 persistence OK')"`.
  </verify>
  <done>
v2 save function creates .distill files with version=2, model_type="vqvae", codebook health snapshot, loss curve history, and VQ-specific config. v2 load function reconstructs a ConvVQVAE from saved state. v1.0 save/load functions remain fully functional and unchanged. New symbols exported from models __init__.py.
  </done>
</task>

</tasks>

<verification>
1. Import all new VQ training functions without errors: `from distill.training.loop import train_vqvae_epoch, validate_vqvae_epoch, train_vqvae`
2. Import all new VQ metrics: `from distill.training.metrics import VQStepMetrics, VQEpochMetrics, VQMetricsHistory`
3. Import v2 persistence: `from distill.models.persistence import save_model_v2, load_model_v2, LoadedVQModel`
4. Import VQ runner method: `from distill.training.runner import TrainingRunner; r = TrainingRunner(); assert hasattr(r, 'start_vqvae')`
5. Verify v1.0 functions still importable: `from distill.training.loop import train; from distill.models.persistence import save_model, load_model`
6. VQMetricsHistory round-trips: `h = VQMetricsHistory(); d = h.to_dict(); h2 = VQMetricsHistory.from_dict(d)`
</verification>

<success_criteria>
- VQ-VAE training loop functions exist and mirror v1.0 structure with VQ-specific changes
- VQ metrics dataclasses serialize/deserialize correctly
- Codebook health computed during validation pass and included in VQEpochMetrics
- Low utilization warnings generated at < 30% threshold (skipping epoch 0)
- v2 model persistence saves and loads ConvVQVAE with codebook health + loss history
- All v1.0 functions remain unchanged and functional
</success_criteria>

<output>
After completion, create `.planning/phases/13-vq-vae-training-pipeline/13-01-SUMMARY.md`
</output>
