---
phase: 13-vq-vae-training-pipeline
plan: 03
type: execute
wave: 2
depends_on:
  - 13-01
files_modified:
  - src/distill/cli/train.py
autonomous: true
requirements:
  - CLI-01

must_haves:
  truths:
    - "User can train a VQ-VAE model from CLI with `distill train DATASET_DIR`"
    - "CLI accepts --codebook-size, --rvq-levels, --commitment-weight flags"
    - "Codebook size auto-determined by default but overridable via --codebook-size"
    - "Per-level codebook health stats displayed at each epoch in CLI output"
    - "End-of-training summary includes final loss, per-level codebook health, model save path, and any warnings"
  artifacts:
    - path: "src/distill/cli/train.py"
      provides: "VQ-VAE CLI training with codebook flags and health display"
      contains: "codebook_size"
  key_links:
    - from: "src/distill/cli/train.py"
      to: "src/distill/training/loop.py"
      via: "train_vqvae() direct call"
      pattern: "train_vqvae\\("
    - from: "src/distill/cli/train.py"
      to: "src/distill/training/config.py"
      via: "get_adaptive_vqvae_config for auto config"
      pattern: "get_adaptive_vqvae_config"
    - from: "src/distill/cli/train.py"
      to: "src/distill/training/metrics.py"
      via: "VQEpochMetrics in cli_callback"
      pattern: "VQEpochMetrics"
---

<objective>
Update the CLI training command to support VQ-VAE training with codebook-specific flags, per-level health display during training, and a comprehensive end-of-training summary.

Purpose: CLI is the power-user complement to the UI -- same information, more control (codebook size override). Users training from the terminal need the same codebook health visibility as the UI.

Output: Updated `distill train` command that trains VQ-VAE models with full codebook monitoring.
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-vq-vae-training-pipeline/13-CONTEXT.md
@.planning/phases/13-vq-vae-training-pipeline/13-RESEARCH.md
@.planning/phases/13-vq-vae-training-pipeline/13-01-SUMMARY.md

@src/distill/cli/train.py
@src/distill/training/loop.py
@src/distill/training/metrics.py
@src/distill/training/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add VQ-VAE flags and training path to CLI</name>
  <files>
    src/distill/cli/train.py
  </files>
  <action>
**cli/train.py** -- Replace the v1.0 training path with VQ-VAE training:

1. **Add new CLI options** to `train_cmd()`:
   - `--codebook-size`: `Annotated[Optional[int], typer.Option("--codebook-size", help="Override auto-determined codebook size (64/128/256)")]  = None`. Default is None (auto from dataset size).
   - `--rvq-levels`: `Annotated[Optional[int], typer.Option("--rvq-levels", help="Number of RVQ quantizer levels (2-4, default: 3)")]  = None`. Default None (uses config default of 3).
   - `--commitment-weight`: `Annotated[Optional[float], typer.Option("--commitment-weight", help="Commitment loss weight (default: 0.25)")]  = None`. Default None (uses config default of 0.25).
   - Keep existing `--model-name` flag (already there).
   - Remove the `--preset` option entirely (v1.0 presets with KL weight are irrelevant). If keeping backward compat is desired, keep it but ignore it with a deprecation warning. Simpler: just remove it.

2. **Replace config building**: Instead of `get_adaptive_config(len(file_paths))` + `TrainingConfig` + KL params, use:
   ```python
   from distill.training.config import VQVAEConfig, get_adaptive_vqvae_config
   vqvae_config = get_adaptive_vqvae_config(len(file_paths))
   ```
   Then apply overrides:
   - `if epochs is not None: vqvae_config.max_epochs = epochs`
   - `if learning_rate is not None: vqvae_config.learning_rate = learning_rate`
   - `if batch_size is not None: vqvae_config.batch_size = batch_size`
   - `if codebook_size is not None: vqvae_config.codebook_size = codebook_size`
   - `if rvq_levels is not None: vqvae_config.num_quantizers = rvq_levels`
   - `if commitment_weight is not None: vqvae_config.commitment_weight = commitment_weight`
   - Remove all KL/preset-related config code.

3. **Print config summary** to console including VQ-specific info:
   ```
   Config: codebook_size={cs} (auto), rvq_levels={rl}, commitment_weight={cw}, epochs={e}, lr={lr}, batch_size={bs}
   ```
   If codebook_size was auto-determined, show "(auto)" suffix. If overridden, show "(override)".

4. **Update Rich progress bar** -- The progress display remains similar but the callback handles VQ events:
   ```python
   def cli_callback(event: object) -> None:
       from distill.training.metrics import VQEpochMetrics, VQStepMetrics
       if isinstance(event, VQEpochMetrics):
           progress.update(
               task_id,
               completed=event.epoch + 1,
               epoch=event.epoch + 1,
               total_epochs=event.total_epochs,
               train_loss=event.train_loss,
               val_loss=event.val_loss,
           )
           # Print per-level codebook health after each epoch
           if event.codebook_health:
               console.print("  [dim]Codebook Health:[/dim]")
               for level_name, stats in sorted(event.codebook_health.items()):
                   util = stats.get("utilization", 0)
                   perp = stats.get("perplexity", 0)
                   dead = stats.get("dead_codes", 0)
                   util_color = "green" if util >= 0.5 else ("yellow" if util >= 0.3 else "red")
                   console.print(
                       f"    {level_name}: "
                       f"[{util_color}]{util:.0%}[/{util_color}] util, "
                       f"{perp:.1f} perplexity, "
                       f"{dead} dead codes"
                   )
           # Print warnings if any
           if event.utilization_warnings:
               for w in event.utilization_warnings:
                   console.print(f"  [bold yellow]Warning:[/bold yellow] {w}")
   ```
   Remove the v1.0 `EpochMetrics` / `PreviewEvent` / `TrainingCompleteEvent` handling (or keep `TrainingCompleteEvent` since it may still be emitted by `train_vqvae()`). Keep `PreviewEvent` handler too (reconstruction previews still generate audio files).

5. **Replace training call**: Instead of `from distill.training.loop import train as run_training` + `run_training(config=training_config, ...)`, use:
   ```python
   from distill.training.loop import train_vqvae
   result = train_vqvae(
       config=vqvae_config,
       file_paths=file_paths,
       output_dir=output_dir,
       device=torch_device,
       callback=cli_callback,
       cancel_event=cancel_event,
       models_dir=models_base,
       dataset_name=dataset_dir.name,
       model_name=model_name or "",
   )
   ```

6. **Update post-training summary** to be a comprehensive VQ-VAE training report (per CONTEXT.md: "Training summary at end should be a complete training report, not just a save path"):
   ```
   Training complete!
     Epochs: 200
     Final train loss: 0.0234
     Final val loss:   0.0312
     Best val loss:    0.0298 (epoch 187)

     Codebook Health (Final):
       Level 0: 85% utilization, 54.2 perplexity, 38 dead codes
       Level 1: 72% utilization, 41.8 perplexity, 71 dead codes
       Level 2: 55% utilization, 29.1 perplexity, 115 dead codes

     Config: codebook_size=128 (auto), rvq_levels=3, commitment_weight=0.25
     Model saved: data/models/my_model.distill
     Output dir: data/models/train_20260221_143022

     Warnings during training:
       Level 2: utilization dropped below 30% at epoch 45
   ```
   Extract final codebook health from `result.get("final_codebook_health")`. Extract warnings from the last VQEpochMetrics in the metrics_history if available.

7. **Update JSON output** (`--json` flag): Include VQ-specific fields in the JSON result: `codebook_size`, `rvq_levels`, `commitment_weight`, `codebook_health` (final snapshot).

8. **Remove the auto-save from checkpoint section**: Since `train_vqvae()` already saves via `save_model_v2()` at the end (Plan 01), the CLI does NOT need to call `save_model_from_checkpoint()` separately. Remove that entire block. The `result` dict will contain the model save path.
  </action>
  <verify>
Run `python -m distill.cli.train --help` and verify the new flags appear: `--codebook-size`, `--rvq-levels`, `--commitment-weight`. Verify `--preset` is gone (or deprecated). Verify `--model-name` still present.
  </verify>
  <done>
CLI `distill train` command trains VQ-VAE models. Codebook size auto-determined by default, overridable with --codebook-size. --rvq-levels and --commitment-weight flags work. Per-level codebook health displayed after each epoch with color-coded utilization. End-of-training summary includes full training report with codebook health, config, model path, and any warnings. JSON output includes VQ-specific fields.
  </done>
</task>

</tasks>

<verification>
1. `python -m distill.cli.train --help` shows VQ-VAE flags
2. No v1.0-specific flags (--preset) visible
3. CLI imports resolve: `python -c "from distill.cli.train import train_cmd; print('OK')"`
4. Help text includes codebook-size, rvq-levels, commitment-weight options
</verification>

<success_criteria>
- CLI accepts --codebook-size (optional override), --rvq-levels (2-4), --commitment-weight flags
- Auto-determined codebook size shown in config summary with "(auto)" marker
- Per-level codebook health printed after each epoch with color-coded utilization
- Low utilization warnings displayed in yellow during training
- End-of-training summary is a complete report (loss, codebook health, config, model path, warnings)
- JSON output includes VQ-specific fields
- SIGINT still triggers graceful cancellation with checkpoint save
</success_criteria>

<output>
After completion, create `.planning/phases/13-vq-vae-training-pipeline/13-03-SUMMARY.md`
</output>
