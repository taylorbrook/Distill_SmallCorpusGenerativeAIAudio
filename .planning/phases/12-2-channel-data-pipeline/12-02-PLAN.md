---
phase: 12-2-channel-data-pipeline
plan: 02
type: execute
wave: 2
depends_on:
  - 12-01
files_modified:
  - src/distill/audio/preprocessing.py
  - src/distill/training/dataset.py
  - src/distill/training/loop.py
autonomous: true
requirements:
  - DATA-02
  - DATA-05

must_haves:
  truths:
    - "Running distill train on a dataset directory auto-triggers 2-channel spectrogram preprocessing if no valid cache exists"
    - "Preprocessed 2-channel spectrograms are cached to disk in .cache/ inside the dataset directory and reloaded without recomputation on subsequent runs"
    - "A JSON manifest in the cache directory records file list, modification times, normalization statistics, augmentation config, and STFT settings"
    - "When audio files change (added/removed/modified) or config changes, the cache is automatically invalidated and rebuilt"
    - "Augmented waveform variants are converted to 2-channel spectrograms and pre-baked into the cache"
    - "Estimated disk usage is shown before preprocessing begins"
  artifacts:
    - path: "src/distill/audio/preprocessing.py"
      provides: "preprocess_complex_spectrograms function with caching, manifest, change detection, augmentation integration"
      contains: "preprocess_complex_spectrograms"
    - path: "src/distill/training/dataset.py"
      provides: "CachedSpectrogramDataset that loads cached 2-channel tensors"
      contains: "class CachedSpectrogramDataset"
    - path: "src/distill/training/loop.py"
      provides: "Training loop wired to use cached 2-channel spectrograms instead of on-the-fly mel conversion"
      contains: "CachedSpectrogramDataset|preprocess_complex"
  key_links:
    - from: "src/distill/audio/preprocessing.py"
      to: "src/distill/audio/spectrogram.py"
      via: "ComplexSpectrogram used to compute 2-channel spectrograms"
      pattern: "ComplexSpectrogram"
    - from: "src/distill/training/dataset.py"
      to: "src/distill/audio/preprocessing.py"
      via: "CachedSpectrogramDataset loads .pt files created by preprocessing"
      pattern: "torch\\.load|cache"
    - from: "src/distill/training/loop.py"
      to: "src/distill/audio/preprocessing.py"
      via: "train() calls preprocessing before creating data loaders"
      pattern: "preprocess_complex"
---

<objective>
Build the preprocessing/caching pipeline that converts audio files into cached 2-channel spectrograms and wire it into the training loop so that `distill train` auto-preprocesses transparently.

Purpose: Preprocessing and caching spectrograms avoids redundant computation across epochs and ensures all augmented variants are pre-baked. This completes the Phase 12 data pipeline by connecting the ComplexSpectrogram computation (Plan 01) to the training infrastructure.

Output: Updated `preprocessing.py` with caching pipeline, new `CachedSpectrogramDataset` in `dataset.py`, and wired training loop in `loop.py`.
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-2-channel-data-pipeline/12-01-SUMMARY.md
@src/distill/audio/preprocessing.py
@src/distill/audio/spectrogram.py
@src/distill/audio/augmentation.py
@src/distill/training/dataset.py
@src/distill/training/loop.py
@src/distill/training/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build 2-channel spectrogram preprocessing and caching pipeline</name>
  <files>src/distill/audio/preprocessing.py</files>
  <action>
Add a new `preprocess_complex_spectrograms` function to the existing `preprocessing.py` module. Keep all existing functions (`preprocess_for_training`, `preprocess_dataset`, `load_cached_dataset`, `clear_cache`) unchanged -- they serve v1.0 code paths.

**`preprocess_complex_spectrograms` function:**

Parameters:
- `files: list[Path]` -- audio file paths
- `dataset_dir: Path` -- dataset root directory (cache will be at `dataset_dir / ".cache"`)
- `complex_spectrogram_config: ComplexSpectrogramConfig` -- from TrainingConfig
- `augmentation_config: AugmentationConfig | None` -- augmentation settings
- `augmentation_expansion: int` -- number of augmented copies per file (from RegularizationConfig)
- `chunk_samples: int` -- chunk size in samples (default 48000)
- `sample_rate: int` -- target sample rate (default 48000)
- `progress_callback: Callable[[str, int, int], None] | None` -- `callback(message, current, total)` for progress
- `confirm_callback: Callable[[str], bool] | None` -- `callback(message) -> bool` for disk usage confirmation

Returns: `tuple[Path, dict]` -- `(cache_dir, normalization_stats)` -- path to cache directory and the normalization statistics dict.

**Implementation flow:**

1. **Cache directory setup:** `cache_dir = dataset_dir / ".cache"`. Create if not exists.

2. **Manifest check / change detection:**
   - Manifest path: `cache_dir / "manifest.json"`
   - If manifest exists, load it. Compare:
     - File list (sorted absolute paths) matches
     - Modification times for each file match (use `os.path.getmtime`)
     - Augmentation config matches (serialize to dict for comparison)
     - Complex spectrogram config matches (if_masking_threshold, n_fft, hop_length, n_mels)
     - chunk_samples and sample_rate match
   - If all match: print "Cache valid, skipping preprocessing", return `(cache_dir, manifest["normalization_stats"])`.
   - If mismatch: print what changed (e.g., "2 files added, augmentation config changed"), clear existing cache .pt files.

3. **Disk usage estimation:**
   - Estimate: `n_files * (1 + augmentation_expansion) * n_chunks_per_file * 2_channels * n_mels * time_frames * 4_bytes_float32`.
   - Estimate `n_chunks_per_file` from average file duration (use `audio.io.get_metadata` on first few files).
   - Print estimated cache size (human-readable: MB/GB).
   - If `confirm_callback` is provided and estimated size > 100MB, call `confirm_callback(f"Estimated cache size: {size}. Continue?")`. If returns False, raise `RuntimeError("Preprocessing cancelled by user")`.

4. **Load and chunk waveforms:**
   - For each file, load via `audio.io.load_audio`, convert to mono, resample if needed.
   - Chunk into fixed-length segments of `chunk_samples` (same logic as existing `AudioTrainingDataset._build_chunk_index` but actually extract chunks).
   - Peak-normalize each chunk.
   - Track per-file progress via `progress_callback("Loading audio", file_idx, total_files)`.

5. **Augmentation:**
   - For each original chunk, create `augmentation_expansion` augmented copies using `AugmentationPipeline.augment()`.
   - Disable PitchShift (set `pitch_shift_probability=0.0`) per existing pattern in loop.py (too slow at 48kHz).
   - Result: list of all chunks (original + augmented).

6. **Compute 2-channel spectrograms:**
   - Create `ComplexSpectrogram` instance from Plan 01 with the config's STFT params and IF masking threshold.
   - For each chunk (batch for efficiency, e.g., batches of 32):
     - Reshape to `[B, 1, samples]`, call `complex_spectrogram.waveform_to_complex_mel()`.
     - Get `[B, 2, n_mels, time]` tensors.
   - Track progress via `progress_callback("Computing spectrograms", batch_idx, total_batches)`.

7. **Compute normalization statistics:**
   - Use `complex_spectrogram.compute_dataset_statistics(all_spectrograms)` to get per-channel mean/std across entire dataset.
   - Apply normalization: `complex_spectrogram.normalize(spec, stats)` to each spectrogram.

8. **Save to cache:**
   - Save each normalized spectrogram tensor as `cache_dir / f"{idx:06d}.pt"` using `torch.save`.
   - Track progress via `progress_callback("Saving cache", idx, total)`.

9. **Write manifest:**
   - Save `cache_dir / "manifest.json"` with:
     ```json
     {
       "version": "2.0",
       "created": "ISO timestamp",
       "file_count": N,
       "files": [{"path": "abs_path", "mtime": float}, ...],
       "chunk_samples": 48000,
       "sample_rate": 48000,
       "augmentation_expansion": 10,
       "augmentation_config": { ... serialized AugmentationConfig ... },
       "complex_spectrogram_config": { "if_masking_threshold": 1e-5, "n_fft": 2048, ... },
       "normalization_stats": {"mag_mean": float, "mag_std": float, "if_mean": float, "if_std": float},
       "total_spectrograms": N,
       "cache_size_bytes": int
     }
     ```
   - Use `json.dump` with indent=2 for human readability.

10. Return `(cache_dir, normalization_stats)`.

**Also add a helper:**

`load_cache_manifest(cache_dir: Path) -> dict | None` -- Load and return the manifest dict, or None if not found/invalid.

**Error handling:**
- Wrap individual file loading in try/except (skip corrupt files with warning, matching existing pattern).
- If no valid files after loading, raise `ValueError("No valid audio files found")`.
  </action>
  <verify>
Run in Python (with a test directory containing 2-3 short audio files):
```python
from pathlib import Path
from distill.audio.preprocessing import preprocess_complex_spectrograms, load_cache_manifest
from distill.training.config import ComplexSpectrogramConfig
import json

dataset_dir = Path("path/to/test/dataset")
cache_dir, stats = preprocess_complex_spectrograms(
    files=list(dataset_dir.glob("*.wav")),
    dataset_dir=dataset_dir,
    complex_spectrogram_config=ComplexSpectrogramConfig(),
    augmentation_config=None,
    augmentation_expansion=0,
)

# Verify cache exists
assert cache_dir.exists()
assert (cache_dir / "manifest.json").exists()
pt_files = list(cache_dir.glob("*.pt"))
assert len(pt_files) > 0

# Verify manifest
manifest = load_cache_manifest(cache_dir)
assert manifest is not None
assert "normalization_stats" in manifest
assert "files" in manifest

# Run again -- should skip (cache valid)
cache_dir2, stats2 = preprocess_complex_spectrograms(
    files=list(dataset_dir.glob("*.wav")),
    dataset_dir=dataset_dir,
    complex_spectrogram_config=ComplexSpectrogramConfig(),
    augmentation_config=None,
    augmentation_expansion=0,
)
assert stats2 == stats  # Same stats returned from manifest
print("Preprocessing and caching verified")
```
  </verify>
  <done>
`preprocess_complex_spectrograms` loads audio files, chunks them, applies augmentation, computes 2-channel spectrograms via ComplexSpectrogram, normalizes per-dataset, caches as .pt files with JSON manifest, and detects changes for cache invalidation. Second run skips preprocessing when cache is valid.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire cached spectrograms into training dataset and loop</name>
  <files>src/distill/training/dataset.py, src/distill/training/loop.py</files>
  <action>
**Part A: Add `CachedSpectrogramDataset` to `dataset.py`:**

Add a new PyTorch Dataset class that loads pre-cached 2-channel spectrogram tensors. Keep existing `AudioTrainingDataset` and `create_data_loaders` unchanged.

```python
class CachedSpectrogramDataset:
    """PyTorch-compatible Dataset that loads cached 2-channel spectrograms.

    Each item is a pre-computed, normalized [2, n_mels, time] tensor loaded
    from a .pt file. No on-the-fly spectrogram computation needed.
    """
```

- Constructor: `__init__(self, cache_dir: Path)` -- scans `cache_dir` for sorted `.pt` files, stores the file list.
- `__len__`: return count of `.pt` files.
- `__getitem__(idx)`: `torch.load(self.pt_files[idx], weights_only=True)` -- returns `[2, n_mels, time]` tensor.
- No augmentation (already pre-baked in cache).
- No chunking (already chunked during preprocessing).

**Add `create_complex_data_loaders` function:**

```python
def create_complex_data_loaders(
    cache_dir: Path,
    config: "TrainingConfig",
    val_fraction: float | None = None,
) -> "tuple[DataLoader, DataLoader]":
```

- Load all `.pt` file paths from `cache_dir` (sorted).
- Split at the spectrogram level (not file level -- files are already mixed across chunks+augmentations in cache). Use the same `_SPLIT_SEED` for reproducibility. Shuffle the indices, take `val_fraction` for validation.
- Create `CachedSpectrogramDataset` for train and val splits (each gets a subset of `.pt` files -- simplest approach: create two subdirectory-like views by passing filtered file lists).
- Actually, simpler: `CachedSpectrogramDataset.__init__` accepts `pt_files: list[Path]` directly instead of scanning a directory. The factory function handles the split.
- Return `(train_loader, val_loader)`.

**Part B: Wire preprocessing into `train()` in `loop.py`:**

Modify the `train()` function to auto-trigger preprocessing when `config.complex_spectrogram.enabled` is True. The key changes are:

1. **After the "Setup" section** (after spectrogram/model creation), add preprocessing:
   ```python
   if config.complex_spectrogram.enabled:
       from distill.audio.preprocessing import preprocess_complex_spectrograms
       from distill.training.dataset import create_complex_data_loaders

       # Determine dataset directory (common parent of file_paths)
       dataset_dir = _get_dataset_dir(file_paths)

       print("[TRAIN] Preprocessing 2-channel spectrograms...", flush=True)
       cache_dir, norm_stats = preprocess_complex_spectrograms(
           files=file_paths,
           dataset_dir=dataset_dir,
           complex_spectrogram_config=config.complex_spectrogram,
           augmentation_config=AugmentationConfig(
               expansion_ratio=config.regularization.augmentation_expansion,
               pitch_shift_probability=0.0,  # Too slow at 48kHz
           ) if config.regularization.augmentation_expansion > 0 else None,
           augmentation_expansion=config.regularization.augmentation_expansion,
           chunk_samples=int(config.chunk_duration_s * 48_000),
           progress_callback=_print_progress,
       )

       train_loader, val_loader = create_complex_data_loaders(
           cache_dir=cache_dir,
           config=config,
       )
   else:
       # Existing v1.0 path (unchanged)
       ...existing augmentation + create_data_loaders code...
   ```

2. **In `train_epoch`:** When using complex spectrograms, the batch from DataLoader is already a `[B, 2, n_mels, time]` tensor (not a waveform). Skip the `spectrogram.waveform_to_mel(batch)` call. Instead, use the batch directly as input to the model.
   - Add a `use_cached_spectrograms: bool = False` parameter to `train_epoch`.
   - When True: `mel = batch.to(device)` (already a spectrogram, no conversion needed).
   - When False: existing path (`mel = spectrogram.waveform_to_mel(batch)`).

3. **Same change in `validate_epoch`:** Add `use_cached_spectrograms` parameter.

4. **Add helper function** `_get_dataset_dir(file_paths: list[Path]) -> Path`:
   - Find the common parent directory of all file paths.
   - Return it as the dataset directory.

5. **Add helper function** `_print_progress(message: str, current: int, total: int) -> None`:
   - Print `f"[PREPROCESS] {message}: {current}/{total}"` with flush=True.

6. **Update model creation:** When `config.complex_spectrogram.enabled`, set `in_channels=2` on ConvVAE (this will be needed in Phase 13, but for now just pass it -- ConvVAE currently defaults to 1. If ConvVAE doesn't accept `in_channels` yet, that's fine -- Phase 13 will add it. For now, just structure the code so the intent is clear with a comment: `# Phase 13 will update ConvVAE to accept in_channels=2`).

**Important design notes:**
- The training loop change is surgical: an if/else branch at the top of `train()` selects between the v1.0 path (waveform loading + on-the-fly mel) and v2.0 path (cached 2-channel spectrograms). The rest of the training loop stays the same.
- `train_epoch` and `validate_epoch` get a simple boolean flag to skip the waveform-to-mel conversion when using cached spectrograms.
- Existing v1.0 code paths are preserved behind the `else` branch.
  </action>
  <verify>
1. Verify `CachedSpectrogramDataset` loads from a cache directory:
```python
from distill.training.dataset import CachedSpectrogramDataset
import torch
# Assuming cache was created by Task 1's preprocessing
dataset = CachedSpectrogramDataset(pt_files=list(cache_dir.glob("*.pt")))
assert len(dataset) > 0
sample = dataset[0]
assert sample.shape[0] == 2  # 2 channels
assert sample.shape[1] == 128  # n_mels
print(f"Dataset size: {len(dataset)}, sample shape: {sample.shape}")
```

2. Verify `create_complex_data_loaders` produces train/val splits:
```python
from distill.training.dataset import create_complex_data_loaders
from distill.training.config import TrainingConfig
config = TrainingConfig()
train_dl, val_dl = create_complex_data_loaders(cache_dir, config)
assert len(train_dl.dataset) > 0
assert len(val_dl.dataset) > 0
batch = next(iter(train_dl))
assert batch.shape[1] == 2  # 2 channels
print(f"Train: {len(train_dl.dataset)}, Val: {len(val_dl.dataset)}")
```

3. Verify training loop imports work without errors:
```python
from distill.training.loop import train_epoch, validate_epoch
# Smoke test: functions accept use_cached_spectrograms parameter
import inspect
sig = inspect.signature(train_epoch)
assert "use_cached_spectrograms" in sig.parameters
```
  </verify>
  <done>
`CachedSpectrogramDataset` loads cached .pt files as `[2, n_mels, time]` tensors. `create_complex_data_loaders` splits cached spectrograms into train/val DataLoaders. Training loop (`train()`) auto-triggers preprocessing when `complex_spectrogram.enabled` is True and uses cached spectrograms directly (skipping on-the-fly mel conversion). Existing v1.0 code path preserved behind else branch.
  </done>
</task>

</tasks>

<verification>
1. `preprocess_complex_spectrograms` creates `.cache/` directory inside dataset dir with `.pt` files and `manifest.json`
2. Running preprocessing a second time with unchanged files/config skips recomputation
3. Changing a file or config triggers cache rebuild
4. `manifest.json` is human-readable with file list, config, normalization stats, timestamps
5. `CachedSpectrogramDataset` correctly loads cached tensors with shape `[2, n_mels, time]`
6. `create_complex_data_loaders` produces train/val DataLoaders from cache
7. Training loop auto-triggers preprocessing when `complex_spectrogram.enabled` is True
8. Training loop skips waveform-to-mel conversion when using cached spectrograms
9. Existing v1.0 training path (1-channel magnitude-only) still works when `complex_spectrogram.enabled` is False
10. Augmented variants are included in cache
</verification>

<success_criteria>
- `distill train <dataset_dir>` auto-preprocesses on first run, shows progress, caches 2-channel spectrograms
- Subsequent runs detect valid cache and skip preprocessing
- The training loop consumes cached 2-channel spectrograms directly from DataLoader
- Cache invalidation works when files or config change
- All v1.0 training functionality preserved when complex spectrogram is disabled
</success_criteria>

<output>
After completion, create `.planning/phases/12-2-channel-data-pipeline/12-02-SUMMARY.md`
</output>
