---
phase: 06-model-persistence-management
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/small_dataset_audio/library/__init__.py
  - src/small_dataset_audio/library/catalog.py
  - src/small_dataset_audio/models/persistence.py
  - src/small_dataset_audio/models/__init__.py
autonomous: true

must_haves:
  truths:
    - "User can save a trained model with metadata to a .sda file"
    - "User can load a .sda model and get back model + spectrogram + analysis + metadata ready for generation"
    - "Loading restores both weights and latent space mappings (sliders work immediately)"
    - "User can browse a model library with search by name/description and filter by tags"
    - "User can delete a model from the library (file + index removed)"
    - "User can convert a training checkpoint to a saved model"
  artifacts:
    - path: "src/small_dataset_audio/library/catalog.py"
      provides: "ModelEntry dataclass, ModelLibrary class with JSON index, search, filter, atomic writes"
      exports: ["ModelEntry", "ModelLibrary"]
    - path: "src/small_dataset_audio/library/__init__.py"
      provides: "Public API for library module"
      exports: ["ModelEntry", "ModelLibrary"]
    - path: "src/small_dataset_audio/models/persistence.py"
      provides: "ModelMetadata, LoadedModel, save_model, load_model, delete_model, save_model_from_checkpoint"
      exports: ["ModelMetadata", "LoadedModel", "save_model", "load_model", "delete_model", "save_model_from_checkpoint", "MODEL_FILE_EXTENSION"]
    - path: "src/small_dataset_audio/models/__init__.py"
      provides: "Updated exports including persistence symbols"
      contains: "from small_dataset_audio.models.persistence import"
  key_links:
    - from: "src/small_dataset_audio/models/persistence.py"
      to: "src/small_dataset_audio/library/catalog.py"
      via: "save_model and delete_model call ModelLibrary.add_entry/remove"
      pattern: "ModelLibrary\\(models_dir\\)"
    - from: "src/small_dataset_audio/models/persistence.py"
      to: "src/small_dataset_audio/controls/serialization.py"
      via: "analysis_to_dict/analysis_from_dict for latent analysis round-trip"
      pattern: "analysis_(to|from)_dict"
    - from: "src/small_dataset_audio/models/persistence.py"
      to: "src/small_dataset_audio/models/vae.py"
      via: "ConvVAE construction and decoder._init_linear before load_state_dict"
      pattern: "decoder\\._init_linear"
    - from: "src/small_dataset_audio/models/persistence.py"
      to: "src/small_dataset_audio/audio/spectrogram.py"
      via: "SpectrogramConfig reconstruction from saved dict"
      pattern: "SpectrogramConfig\\(\\*\\*"
---

<objective>
Build the complete model persistence and library management system for Phase 6.

Purpose: Enable users to save trained models as .sda files with rich metadata, load them for immediate generation (with slider controls restored), browse/search/filter a model library, delete models, and convert training checkpoints to saved models.

Output: Four files -- library/catalog.py (ModelLibrary + JSON index), library/__init__.py (exports), models/persistence.py (save/load/delete), models/__init__.py (updated exports).
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-model-persistence-management/06-RESEARCH.md

@src/small_dataset_audio/models/vae.py
@src/small_dataset_audio/models/__init__.py
@src/small_dataset_audio/training/checkpoint.py
@src/small_dataset_audio/controls/serialization.py
@src/small_dataset_audio/controls/__init__.py
@src/small_dataset_audio/audio/spectrogram.py
@src/small_dataset_audio/inference/generation.py
@src/small_dataset_audio/inference/__init__.py
@src/small_dataset_audio/config/defaults.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create library/catalog.py with ModelEntry and ModelLibrary</name>
  <files>
    src/small_dataset_audio/library/__init__.py
    src/small_dataset_audio/library/catalog.py
  </files>
  <action>
Create the `library/` subpackage directory under `src/small_dataset_audio/`.

**catalog.py** -- Full model library catalog with JSON index management:

1. `ModelEntry` dataclass with fields matching the JSON index structure from the research:
   - model_id: str (UUID)
   - name: str
   - description: str
   - file_path: str (relative filename within models_dir, NOT absolute)
   - file_size_bytes: int
   - dataset_name: str
   - dataset_file_count: int
   - dataset_total_duration_s: float
   - training_date: str (ISO 8601)
   - save_date: str (ISO 8601)
   - training_epochs: int
   - final_train_loss: float
   - final_val_loss: float
   - has_analysis: bool
   - n_active_components: int
   - tags: list[str] (default_factory=list)

2. `ModelLibrary` class managing the JSON index file (`model_library.json`):
   - `__init__(self, models_dir: Path)` -- loads or creates index
   - `_load_index(self)` -- reads JSON, deserializes to dict[str, ModelEntry], handles missing/corrupt file gracefully (logs warning, starts fresh)
   - `_save_index(self)` -- calls `_write_index_atomic`
   - `add_entry(self, entry: ModelEntry)` -- adds to dict + saves
   - `remove(self, model_id: str) -> bool` -- removes from dict + saves, returns False if not found
   - `get(self, model_id: str) -> ModelEntry | None` -- lookup by ID
   - `list_all(self, sort_by: str = "save_date", reverse: bool = True) -> list[ModelEntry]` -- all entries sorted
   - `search(self, query: str = "", tags: list[str] | None = None, sort_by: str = "save_date", reverse: bool = True) -> list[ModelEntry]` -- case-insensitive search on name+description, tag filter (any match), sorted
   - `count(self) -> int` -- number of entries
   - `repair_index(self) -> tuple[int, int]` -- consistency check: (1) remove entries whose .sda file is missing, (2) scan for orphan .sda files not in index and add them by reading metadata via torch.load. Returns (removed_count, added_count). Logs warnings for each inconsistency.

3. `_write_index_atomic(index_path: Path, data: dict)` -- module-level function:
   - Backup existing index to `.json.bak` (best effort)
   - Write to tempfile in same directory (same filesystem for atomic rename)
   - `os.replace(tmp_path, index_path)` for atomic swap
   - Clean up temp file on failure

4. Index JSON structure: `{"version": 1, "models": {model_id: {entry_fields...}}}`

Follow project patterns:
- `from __future__ import annotations`
- `logging.getLogger(__name__)` for module-level logger
- Lazy torch import only inside `repair_index` (the only method that needs it)
- `dataclasses.asdict` for ModelEntry -> dict serialization in JSON
- `datetime.datetime` import from stdlib

**library/__init__.py** -- Public API exports:
```python
from small_dataset_audio.library.catalog import ModelEntry, ModelLibrary

__all__ = ["ModelEntry", "ModelLibrary"]
```
  </action>
  <verify>
Run: `cd "/Users/taylorbrook/Dev/Small DataSet Audio" && uv run python -c "from small_dataset_audio.library import ModelEntry, ModelLibrary; print('ModelEntry fields:', [f.name for f in __import__('dataclasses').fields(ModelEntry)]); print('ModelLibrary methods:', [m for m in dir(ModelLibrary) if not m.startswith('_')])"`

Expected: ModelEntry has all 16 fields listed above. ModelLibrary has add_entry, count, get, list_all, remove, repair_index, search methods.
  </verify>
  <done>ModelEntry dataclass and ModelLibrary class exist with JSON index management, atomic writes, search/filter, and repair_index. All symbols import cleanly.</done>
</task>

<task type="auto">
  <name>Task 2: Create models/persistence.py with save/load/delete functions</name>
  <files>src/small_dataset_audio/models/persistence.py</files>
  <action>
Create `models/persistence.py` with the complete model persistence API.

**Constants:**
- `SAVED_MODEL_VERSION = 1`
- `MODEL_FORMAT_MARKER = "sda_model"`
- `MODEL_FILE_EXTENSION = ".sda"`

**Dataclasses:**

1. `ModelMetadata` -- user-facing metadata stored with a saved model:
   - model_id: str = "" (auto-generated UUID if empty)
   - name: str = "Untitled Model"
   - description: str = ""
   - dataset_name: str = ""
   - dataset_file_count: int = 0
   - dataset_total_duration_s: float = 0.0
   - training_date: str = "" (ISO 8601)
   - save_date: str = "" (ISO 8601, auto-filled)
   - training_epochs: int = 0
   - final_train_loss: float = 0.0
   - final_val_loss: float = 0.0
   - tags: list[str] = field(default_factory=list)
   - has_analysis: bool = False
   - n_active_components: int = 0

2. `LoadedModel` -- complete loaded model ready for generation:
   - model: ConvVAE (use string annotation "ConvVAE" for lazy typing)
   - spectrogram: AudioSpectrogram (string annotation)
   - analysis: AnalysisResult | None (string annotation)
   - metadata: ModelMetadata
   - device: torch.device (string annotation)

**Functions:**

3. `_sanitize_filename(name: str) -> str`:
   - Replace non-alphanumeric chars (except hyphen, underscore) with underscores
   - Collapse multiple underscores
   - Strip leading/trailing underscores
   - Lowercase
   - Truncate to 100 chars
   - Fallback to "untitled" if empty after sanitization

4. `save_model(model, spectrogram_config: dict, training_config: dict, metadata: ModelMetadata, models_dir: Path, analysis=None) -> Path`:
   - Auto-generate model_id (uuid4) if empty
   - Auto-fill save_date (UTC ISO 8601) if empty
   - Set has_analysis and n_active_components from analysis if present
   - Call `analysis_to_dict(analysis)` from controls.serialization if analysis is not None
   - Build saved model dict: format marker, version, model_state_dict, latent_dim, spectrogram_config, latent_analysis (or None), training_config, metadata (as plain dict via dataclasses.asdict)
   - Strip optimizer/scheduler state -- this is a finished model, NOT a checkpoint
   - Sanitize filename from metadata.name, add .sda extension
   - Handle duplicate filenames with counter suffix
   - mkdir parents, torch.save to file
   - Create ModelEntry from metadata + file info (file_size_bytes via os.path.getsize)
   - Instantiate ModelLibrary(models_dir) and call add_entry
   - Return the saved file path
   - Log info on success

5. `load_model(model_path: Path, device: str = "cpu") -> LoadedModel`:
   - torch.load with weights_only=False (per research -- our dicts have numpy arrays)
   - Validate format marker and version
   - Reconstruct SpectrogramConfig from saved dict: `SpectrogramConfig(**saved["spectrogram_config"])`
   - Reconstruct AudioSpectrogram from SpectrogramConfig
   - Reconstruct ConvVAE with correct latent_dim
   - CRITICAL: Initialize both encoder and decoder linear layers BEFORE load_state_dict:
     * Compute spatial shape from spectrogram config: n_mels padded to 16, time_frames (sample_rate // hop_length + 1) padded to 16, each divided by 16
     * Call `model.decoder._init_linear(spatial)`
     * Compute flatten_dim = 256 * spatial[0] * spatial[1]
     * Call `model.encoder._init_linear(flatten_dim)`
   - Call model.load_state_dict(saved["model_state_dict"])
   - Move model to device, set eval mode
   - Move spectrogram to device
   - Reconstruct AnalysisResult via analysis_from_dict if latent_analysis present
   - Reconstruct ModelMetadata from saved metadata dict
   - Return LoadedModel

6. `delete_model(model_id: str, models_dir: Path) -> bool`:
   - Instantiate ModelLibrary(models_dir)
   - Look up entry by model_id
   - If not found, return False
   - Delete the .sda file (models_dir / entry.file_path)
   - Remove entry from catalog, save
   - Return True
   - Log info on success

7. `save_model_from_checkpoint(checkpoint_path: Path, metadata: ModelMetadata, models_dir: Path) -> Path`:
   - torch.load the checkpoint (weights_only=False)
   - Extract spectrogram_config, training_config from checkpoint dict
   - Reconstruct ConvVAE with correct latent_dim (from checkpoint or default 64)
   - Initialize encoder/decoder linear layers from spectrogram config (same spatial computation as load_model)
   - Load model_state_dict into model
   - Extract analysis from checkpoint if it has "latent_analysis" key (some checkpoints from Phase 5 may include it)
   - Populate metadata fields from checkpoint: training_epochs from "epoch", final_train_loss from "train_loss", final_val_loss from "val_loss"
   - Call save_model() with the reconstructed model and extracted data
   - Return the saved file path

Follow project patterns:
- `from __future__ import annotations`
- Lazy torch import inside function bodies (not at module level)
- `logging.getLogger(__name__)` module-level logger
- Type annotations use string literals for torch/model types
- Errors that indicate data corruption raise ValueError with clear messages
  </action>
  <verify>
Run: `cd "/Users/taylorbrook/Dev/Small DataSet Audio" && uv run python -c "
from small_dataset_audio.models.persistence import (
    ModelMetadata, LoadedModel, save_model, load_model, delete_model,
    save_model_from_checkpoint, MODEL_FILE_EXTENSION, SAVED_MODEL_VERSION
)
print('MODEL_FILE_EXTENSION:', MODEL_FILE_EXTENSION)
print('SAVED_MODEL_VERSION:', SAVED_MODEL_VERSION)
print('ModelMetadata fields:', [f.name for f in __import__('dataclasses').fields(ModelMetadata)])
print('LoadedModel fields:', [f.name for f in __import__('dataclasses').fields(LoadedModel)])
print('Functions: save_model, load_model, delete_model, save_model_from_checkpoint')
"`

Expected: All imports succeed, extension is ".sda", version is 1, ModelMetadata has 14 fields, LoadedModel has 5 fields.
  </verify>
  <done>Complete persistence API exists: save_model creates .sda files and updates catalog, load_model reconstructs model+spectrogram+analysis ready for GenerationPipeline, delete_model removes file+index entry, save_model_from_checkpoint converts checkpoints to saved models.</done>
</task>

<task type="auto">
  <name>Task 3: Update models/__init__.py and run integration smoke test</name>
  <files>src/small_dataset_audio/models/__init__.py</files>
  <action>
1. Update `models/__init__.py` to add persistence exports:
   - Add imports: `from small_dataset_audio.models.persistence import (ModelMetadata, LoadedModel, save_model, load_model, delete_model, save_model_from_checkpoint, MODEL_FILE_EXTENSION, SAVED_MODEL_VERSION)`
   - Add all 8 symbols to `__all__`
   - Keep existing vae.py and losses.py exports

2. Run an integration smoke test that exercises the full save->catalog->load->delete cycle:

```python
import tempfile
from pathlib import Path
from small_dataset_audio.models import ConvVAE, save_model, load_model, delete_model, ModelMetadata, MODEL_FILE_EXTENSION
from small_dataset_audio.library import ModelLibrary
from small_dataset_audio.audio.spectrogram import SpectrogramConfig
from dataclasses import asdict

# Create a model and save it
model = ConvVAE(latent_dim=64)
# Force decoder init with default mel shape
model.sample(1, __import__('torch').device('cpu'))

spec_config = asdict(SpectrogramConfig())
train_config = {"epochs": 100, "lr": 0.001}
metadata = ModelMetadata(name="Test Model", dataset_name="test_dataset", dataset_file_count=10)

with tempfile.TemporaryDirectory() as tmpdir:
    models_dir = Path(tmpdir)

    # Save
    saved_path = save_model(model, spec_config, train_config, metadata, models_dir)
    assert saved_path.exists(), f"Saved file missing: {saved_path}"
    assert saved_path.suffix == MODEL_FILE_EXTENSION

    # Catalog
    library = ModelLibrary(models_dir)
    assert library.count() == 1
    entries = library.search("Test")
    assert len(entries) == 1
    assert entries[0].name == "Test Model"
    model_id = entries[0].model_id

    # Load
    loaded = load_model(saved_path, device="cpu")
    assert loaded.metadata.name == "Test Model"
    assert loaded.model is not None
    assert loaded.spectrogram is not None
    # Generate from loaded model to prove it works
    audio = loaded.model.sample(1, loaded.device)
    assert audio.shape[0] == 1

    # Delete
    result = delete_model(model_id, models_dir)
    assert result is True
    library2 = ModelLibrary(models_dir)
    assert library2.count() == 0
    assert not saved_path.exists()

print("ALL INTEGRATION TESTS PASSED")
```

Run this as a Python script to validate the full cycle works.
  </action>
  <verify>
Run: `cd "/Users/taylorbrook/Dev/Small DataSet Audio" && uv run python -c "
from small_dataset_audio.models import (ConvVAE, save_model, load_model, delete_model, ModelMetadata, LoadedModel, MODEL_FILE_EXTENSION, SAVED_MODEL_VERSION, save_model_from_checkpoint)
from small_dataset_audio.library import ModelEntry, ModelLibrary
print('All exports accessible from public API')
print('models exports:', [s for s in dir(__import__('small_dataset_audio.models', fromlist=[''])) if not s.startswith('_')])
print('library exports:', [s for s in dir(__import__('small_dataset_audio.library', fromlist=[''])) if not s.startswith('_')])
"`

And run the integration smoke test script above. Both must pass.
  </verify>
  <done>Public API exports updated. Integration smoke test passes: save creates .sda file + catalog entry, search finds model by name, load reconstructs working model (can generate audio), delete removes file + catalog entry.</done>
</task>

</tasks>

<verification>
1. `uv run python -c "from small_dataset_audio.models import save_model, load_model, delete_model, ModelMetadata, LoadedModel"` -- imports succeed
2. `uv run python -c "from small_dataset_audio.library import ModelEntry, ModelLibrary"` -- imports succeed
3. Integration smoke test (Task 3) passes -- full save/catalog/load/generate/delete cycle works
4. Saved .sda files contain format marker "sda_model" and version 1
5. JSON index uses atomic writes (temp file + os.replace pattern)
6. load_model initializes decoder linear layers before load_state_dict (no RuntimeError)
7. load_model with analysis present returns AnalysisResult (sliders work)
</verification>

<success_criteria>
- save_model creates .sda file with model weights, spectrogram config, latent analysis, training config, and metadata
- load_model reconstructs ConvVAE + AudioSpectrogram + AnalysisResult ready for GenerationPipeline
- ModelLibrary search finds models by name/description substring (case-insensitive)
- ModelLibrary search filters by tags (any-match)
- delete_model removes .sda file AND catalog entry
- save_model_from_checkpoint converts training checkpoints to saved models
- All operations update the JSON index atomically
- ModelLibrary.repair_index handles file/index inconsistencies
</success_criteria>

<output>
After completion, create `.planning/phases/06-model-persistence-management/06-01-SUMMARY.md`
</output>
