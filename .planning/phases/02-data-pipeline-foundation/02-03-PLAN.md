---
phase: 02-data-pipeline-foundation
plan: 03
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/small_dataset_audio/data/__init__.py
  - src/small_dataset_audio/data/dataset.py
  - src/small_dataset_audio/data/summary.py
  - src/small_dataset_audio/audio/thumbnails.py
  - src/small_dataset_audio/audio/__init__.py
autonomous: true

must_haves:
  truths:
    - "User can import audio files by providing a list of file paths"
    - "User can import a batch of files from a folder path"
    - "User can view dataset summary showing file count, total duration, sample rate consistency"
    - "Waveform thumbnails are generated as PNG files for each audio file in the dataset"
    - "Thumbnails are cached and only regenerated when source file is newer"
  artifacts:
    - path: "src/small_dataset_audio/data/dataset.py"
      provides: "Dataset class wrapping a collection of audio files"
      exports: ["Dataset"]
    - path: "src/small_dataset_audio/data/summary.py"
      provides: "Dataset summary computation"
      exports: ["DatasetSummary", "compute_summary"]
    - path: "src/small_dataset_audio/audio/thumbnails.py"
      provides: "Waveform thumbnail generation"
      exports: ["generate_waveform_thumbnail", "generate_dataset_thumbnails"]
  key_links:
    - from: "src/small_dataset_audio/data/dataset.py"
      to: "src/small_dataset_audio/audio/io.py"
      via: "imports get_metadata, load_audio, AudioMetadata"
      pattern: "from.*audio.*import|audio\\.io"
    - from: "src/small_dataset_audio/data/dataset.py"
      to: "src/small_dataset_audio/audio/validation.py"
      via: "imports validate_dataset, collect_audio_files"
      pattern: "from.*audio.*import.*validate|collect_audio_files"
    - from: "src/small_dataset_audio/data/summary.py"
      to: "src/small_dataset_audio/data/dataset.py"
      via: "imports Dataset to compute summary from"
      pattern: "from.*data.*import.*Dataset|from.*dataset import"
    - from: "src/small_dataset_audio/data/summary.py"
      to: "src/small_dataset_audio/audio/thumbnails.py"
      via: "imports generate_dataset_thumbnails for summary creation"
      pattern: "from.*audio.*import.*thumbnail|generate_dataset_thumbnails"
    - from: "src/small_dataset_audio/audio/thumbnails.py"
      to: "matplotlib"
      via: "matplotlib.use('Agg') then pyplot for waveform rendering"
      pattern: "matplotlib\\.use|matplotlib\\.pyplot"
---

<objective>
Dataset management class, dataset summary computation, and waveform thumbnail generation -- the user-facing data pipeline that enables importing files, viewing summaries, and inspecting waveforms.

Purpose: This plan creates the Dataset class (wrapping a collection of audio files with validation on import), the summary module (file count, total duration, sample rate stats), and the thumbnail generator (waveform PNGs cached in .thumbnails/ directory). Together these satisfy the requirements for importing audio files (DATA-01, DATA-02) and viewing dataset summaries (DATA-03).

Output: `data/dataset.py` with Dataset class; `data/summary.py` with summary computation; `audio/thumbnails.py` with waveform PNG generation.
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-data-pipeline-foundation/02-RESEARCH.md
@.planning/phases/02-data-pipeline-foundation/02-01-SUMMARY.md
@src/small_dataset_audio/audio/io.py
@src/small_dataset_audio/audio/validation.py
@src/small_dataset_audio/audio/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement waveform thumbnail generation</name>
  <files>
    src/small_dataset_audio/audio/thumbnails.py
    src/small_dataset_audio/audio/__init__.py
  </files>
  <action>
Create `src/small_dataset_audio/audio/thumbnails.py` with:

**Functions:**
- `generate_waveform_thumbnail(waveform: np.ndarray | torch.Tensor, output_path: Path, width: int = 800, height: int = 120, color: str = "#4A90D9") -> None`:
  Generate a compact waveform thumbnail PNG.
  1. Call `matplotlib.use('Agg')` BEFORE importing pyplot (research pitfall #7 -- avoids TclError on headless systems). Import matplotlib and pyplot lazily inside function body.
  2. If input is torch.Tensor, convert to numpy via `.numpy()` (detach first if needed).
  3. Mix to mono if multi-channel: `waveform.mean(axis=0)`.
  4. Downsample for display efficiency: if `len(mono) > width * 2`, use `np.linspace` indices to sample `width * 2` points.
  5. Create figure with `figsize=(width/dpi, height/dpi)` where `dpi=100`.
  6. Use `ax.fill_between(time, mono, -mono, alpha=0.7, color=color)` for the waveform visualization (symmetric fill looks better than line plot for thumbnails).
  7. Set `ax.set_xlim(0, 1)`, `ax.set_ylim(-1, 1)`, `ax.axis('off')`.
  8. Save with `fig.savefig(output_path, bbox_inches='tight', pad_inches=0, transparent=True)`.
  9. Always call `plt.close(fig)` to prevent memory leaks.

- `generate_dataset_thumbnails(file_metadata_pairs: list[tuple[Path, AudioMetadata]], thumbnail_dir: Path, width: int = 800, height: int = 120, force: bool = False) -> dict[Path, Path]`:
  Generate thumbnails for a list of audio files, with caching.
  1. Create `thumbnail_dir` if it doesn't exist.
  2. For each (audio_path, metadata) pair:
     a. Compute thumbnail path: `thumbnail_dir / f"{audio_path.stem}.png"`
     b. Skip if thumbnail exists AND is newer than source file (mtime comparison), UNLESS `force=True`
     c. Load audio waveform via `load_audio()` from audio.io (lazy import)
     d. Call `generate_waveform_thumbnail()` with the numpy waveform
     e. Wrap in try/except -- if thumbnail generation fails for one file, log warning and continue
  3. Return dict mapping audio_path -> thumbnail_path for all successfully generated thumbnails.

Update `src/small_dataset_audio/audio/__init__.py` to add thumbnail exports:
- Add `from small_dataset_audio.audio.thumbnails import generate_waveform_thumbnail, generate_dataset_thumbnails` to the existing re-exports.

**Critical patterns:**
- matplotlib.use('Agg') MUST be called before importing pyplot -- this is the #1 headless deployment pitfall
- Lazy import matplotlib, numpy, torch inside function bodies (Phase 1 pattern)
- Always plt.close(fig) to prevent memory leaks when generating many thumbnails
- Mtime-based cache invalidation: only regenerate when source is newer than thumbnail
- Per-file try/except in batch generation -- one failed thumbnail must not stop the batch
  </action>
  <verify>
Run:
```
uv run python -c "
import tempfile, os
from pathlib import Path
import numpy as np
from small_dataset_audio.audio.thumbnails import generate_waveform_thumbnail

# Generate a thumbnail from synthetic waveform
with tempfile.TemporaryDirectory() as tmpdir:
    output = Path(tmpdir) / 'test_thumb.png'
    waveform = np.sin(np.linspace(0, 20 * np.pi, 48000)).reshape(1, -1).astype(np.float32)
    generate_waveform_thumbnail(waveform, output)
    assert output.exists()
    size = output.stat().st_size
    assert size > 100  # valid PNG should be more than 100 bytes
    print(f'Thumbnail generated: {output.name} ({size} bytes)')

# Test re-export
from small_dataset_audio.audio import generate_waveform_thumbnail
print('Re-export OK')
print('All thumbnail tests passed')
"
```
Thumbnail PNG generated with valid file size. Re-export works.
  </verify>
  <done>generate_waveform_thumbnail creates compact PNG waveform visualizations. generate_dataset_thumbnails batch-generates with mtime-based caching. matplotlib uses Agg backend for headless compatibility. Memory leaks prevented via plt.close(fig).</done>
</task>

<task type="auto">
  <name>Task 2: Implement Dataset class and summary computation</name>
  <files>
    src/small_dataset_audio/data/dataset.py
    src/small_dataset_audio/data/summary.py
    src/small_dataset_audio/data/__init__.py
  </files>
  <action>
1. Create `src/small_dataset_audio/data/dataset.py` with:

**Dataset class:**
- `Dataset` class (not a PyTorch Dataset -- this is a higher-level wrapper for the data pipeline. Phase 3 will create a PyTorch Dataset that wraps this).
  - `__init__(self, name: str, base_dir: Path)`: Initialize with dataset name and base directory. Store `self.name`, `self.base_dir`, `self.files: list[Path] = []`, `self.metadata: dict[Path, AudioMetadata] = {}`, `self.validation_issues: list[ValidationIssue] = []`, `self.thumbnail_dir: Path = base_dir / ".thumbnails"`.

  - `@classmethod from_directory(cls, directory: Path, name: str | None = None) -> "Dataset"`: Import all supported audio files from a directory (DATA-02). Use `collect_audio_files()` from audio.validation. Validate with `validate_dataset()`. Load metadata for all valid files. Name defaults to directory name if not provided.

  - `@classmethod from_files(cls, files: list[Path], name: str, base_dir: Path | None = None) -> "Dataset"`: Import specific files (DATA-01). Validate files. Load metadata for valid files. base_dir defaults to common parent of all files.

  - `add_files(self, files: list[Path]) -> list[ValidationIssue]`: Add more files to existing dataset. Validate new files, load their metadata, append to self.files. Return issues for the new files only.

  - `@property valid_files(self) -> list[Path]`: Return files that passed validation (no ERROR-severity issues).

  - `@property file_count(self) -> int`: Return len(self.valid_files).

  - `@property total_duration(self) -> float`: Sum duration_seconds from metadata of valid files.

  - `@property sample_rates(self) -> set[int]`: Set of unique sample rates across valid files.

  - `has_errors(self) -> bool`: True if any validation issue has Severity.ERROR.

  - `has_warnings(self) -> bool`: True if any validation issue has Severity.WARNING.

2. Create `src/small_dataset_audio/data/summary.py` with:

**DatasetSummary dataclass:**
- `DatasetSummary` dataclass with fields:
  - `name: str`
  - `file_count: int`
  - `valid_file_count: int`
  - `total_duration_seconds: float`
  - `sample_rates: dict[int, int]` (rate -> count of files at that rate)
  - `dominant_sample_rate: int` (most common rate)
  - `sample_rate_consistent: bool` (True if all files same rate)
  - `formats: dict[str, int]` (format -> count, e.g. {"WAV": 5, "FLAC": 3})
  - `channel_counts: dict[int, int]` (channels -> count, e.g. {1: 4, 2: 6})
  - `min_duration_seconds: float`
  - `max_duration_seconds: float`
  - `avg_duration_seconds: float`
  - `thumbnail_paths: dict[Path, Path]` (audio_path -> thumbnail_path)
  - `validation_issues: list[ValidationIssue]`

**Functions:**
- `compute_summary(dataset: Dataset, generate_thumbnails: bool = True) -> DatasetSummary`:
  Compute all summary statistics from a Dataset instance. If generate_thumbnails is True, call `generate_dataset_thumbnails()` from audio.thumbnails. Compute sample rate distribution, format distribution, channel distribution, duration stats. Return DatasetSummary.

- `format_summary_report(summary: DatasetSummary) -> str`:
  Format summary as human-readable text report. Include:
  - Dataset name and file count
  - Total duration (formatted as HH:MM:SS)
  - Sample rate consistency status with breakdown
  - Format breakdown
  - Validation issue count (errors and warnings)
  - Duration range (min/max/avg)
  Use plain text formatting (no rich -- keep the module dependency-free; rich formatting is a UI concern for Phase 8).

3. Update `src/small_dataset_audio/data/__init__.py` to re-export:
   ```python
   from small_dataset_audio.data.dataset import Dataset
   from small_dataset_audio.data.summary import DatasetSummary, compute_summary, format_summary_report
   ```

**Critical patterns:**
- Dataset class does NOT load waveforms into memory -- only metadata. Waveform loading is deferred to preprocessing/training (anti-pattern: loading entire dataset into memory)
- Validation is run on import (from_directory, from_files) -- user sees issues immediately
- from_directory uses collect_audio_files which skips hidden files and non-audio files
- All imports from audio.io and audio.validation -- do NOT duplicate I/O or validation logic
  </action>
  <verify>
Run:
```
uv run python -c "
from pathlib import Path
from small_dataset_audio.data.dataset import Dataset
from small_dataset_audio.data.summary import compute_summary, format_summary_report, DatasetSummary

# Test empty dataset creation
ds = Dataset(name='test', base_dir=Path('/tmp/test_dataset'))
assert ds.file_count == 0
assert ds.total_duration == 0.0
print(f'Empty dataset: {ds.file_count} files, {ds.total_duration}s')

# Test from_directory on data/datasets (should be empty or nearly)
ds2 = Dataset.from_directory(Path('data/datasets'), name='empty_test')
print(f'From directory: {ds2.file_count} files')

# Test summary computation on empty dataset
summary = compute_summary(ds2, generate_thumbnails=False)
assert isinstance(summary, DatasetSummary)
assert summary.file_count == 0
report = format_summary_report(summary)
assert 'empty_test' in report or '0' in report
print(f'Summary report length: {len(report)} chars')

# Test public API re-exports
from small_dataset_audio.data import Dataset, DatasetSummary, compute_summary
print('Public API re-exports OK')
print('All dataset tests passed')
"
```
Empty dataset creation works. Summary computation handles empty datasets. Report formatting works. Re-exports OK.
  </verify>
  <done>Dataset class supports import from directory (DATA-02) and file list (DATA-01) with validation on import. DatasetSummary provides file count, total duration, sample rate consistency, format breakdown, and thumbnail paths (DATA-03). Only metadata is loaded into memory -- waveforms are deferred. data/__init__.py re-exports full public API.</done>
</task>

</tasks>

<verification>
1. `from small_dataset_audio.data import Dataset, DatasetSummary, compute_summary` succeeds
2. `from small_dataset_audio.audio import generate_waveform_thumbnail` succeeds
3. Dataset.from_directory scans folder recursively for supported audio files
4. Dataset.from_files accepts explicit file list
5. compute_summary produces correct stats (file count, duration, sample rates)
6. generate_waveform_thumbnail creates valid PNG files
7. Thumbnails are cached with mtime-based invalidation
8. Empty datasets handled gracefully (no crashes, zero-value stats)
</verification>

<success_criteria>
- Dataset class imports files from directory (DATA-02) and file list (DATA-01) with validation
- Dataset summary shows file count, total duration, sample rate consistency, format breakdown (DATA-03)
- Waveform thumbnails generated as PNG and cached in .thumbnails/ directory
- Thumbnail cache uses mtime comparison for invalidation
- Only metadata loaded into memory (not full waveforms)
- Empty and error datasets handled gracefully
- data/ and audio/ __init__.py re-export full public APIs
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-pipeline-foundation/02-03-SUMMARY.md`
</output>
