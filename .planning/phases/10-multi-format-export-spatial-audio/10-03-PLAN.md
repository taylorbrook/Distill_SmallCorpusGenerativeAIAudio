---
phase: 10-multi-format-export-spatial-audio
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/small_dataset_audio/inference/blending.py
autonomous: true

must_haves:
  truths:
    - "User can load up to 4 models simultaneously for blending"
    - "User can set individual weight per model (0-100%), auto-normalized to sum to 100%"
    - "User can toggle between latent-space blending and audio-domain blending"
    - "Union of all sliders from loaded models maps to whichever models have that parameter"
    - "Latent-space blending validates that all models share the same latent_dim"
    - "Audio-domain blending works with any models regardless of architecture"
  artifacts:
    - path: "src/small_dataset_audio/inference/blending.py"
      provides: "Multi-model blending engine with latent-space and audio-domain modes"
      contains: "class BlendMode"
    - path: "src/small_dataset_audio/inference/blending.py"
      provides: "ModelSlot dataclass for loaded model with weight"
      contains: "class ModelSlot"
  key_links:
    - from: "src/small_dataset_audio/inference/blending.py"
      to: "src/small_dataset_audio/inference/generation.py"
      via: "BlendEngine uses GenerationPipeline for per-model generation"
      pattern: "GenerationPipeline"
    - from: "src/small_dataset_audio/inference/blending.py"
      to: "src/small_dataset_audio/controls/mapping.py"
      via: "Union slider resolution uses sliders_to_latent for each model"
      pattern: "sliders_to_latent"
---

<objective>
Multi-model blending engine with latent-space and audio-domain modes, weight normalization, and union slider resolution.

Purpose: Allow users to load up to 4 models simultaneously and blend their outputs with configurable ratios, producing novel audio that combines the characteristics of different trained models. This is a core Phase 10 requirement (MOD-04).

Output: New `blending.py` module with `BlendMode` enum, `ModelSlot` dataclass, `BlendEngine` class with `blend_generate()` method, and union slider resolution logic.
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-multi-format-export-spatial-audio/10-CONTEXT.md
@.planning/phases/10-multi-format-export-spatial-audio/10-RESEARCH.md
@src/small_dataset_audio/inference/generation.py
@src/small_dataset_audio/controls/mapping.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create blending.py with ModelSlot, BlendMode, weight normalization, and union slider resolution</name>
  <files>
    src/small_dataset_audio/inference/blending.py
  </files>
  <action>
1. Create `src/small_dataset_audio/inference/blending.py`:
   - Module docstring: "Multi-model blending engine for loading up to 4 models simultaneously and blending their outputs via latent-space or audio-domain averaging."
   - Lazy imports for numpy, torch (project pattern). TYPE_CHECKING imports for type hints.

   - Add `MAX_BLEND_MODELS = 4` constant.

   - Add `BlendMode` enum (str, Enum):
     - `LATENT = "latent"` -- weighted average of latent vectors before decoding
     - `AUDIO = "audio"` -- weighted average of audio waveforms after generation

   - Add `ModelSlot` dataclass:
     - `model: ConvVAE` (TYPE_CHECKING)
     - `spectrogram: AudioSpectrogram` (TYPE_CHECKING)
     - `analysis: AnalysisResult | None`
     - `metadata: ModelMetadata` (TYPE_CHECKING)
     - `device: torch.device` (TYPE_CHECKING)
     - `weight: float = 25.0` -- raw weight 0-100 (not normalized)
     - `active: bool = True` -- whether this slot participates in blending

   - Add `normalize_weights(raw_weights: list[float]) -> list[float]`:
     - Sum all raw weights. If total is 0, distribute equally (1/n each).
     - Otherwise, return `[w / total for w in raw_weights]`.
     - Per discretion recommendation: simple proportional normalization.

   - Add `UnionSliderInfo` dataclass:
     - `index: int` -- global slider index
     - `label: str` -- suggested label (from the first model that has this component)
     - `variance_pct: float` -- max variance from any model for this component
     - `model_indices: list[int]` -- which model slots have this parameter
     - `component_indices: list[int]` -- corresponding PCA component index in each model

   - Add `resolve_union_sliders(slots: list[ModelSlot]) -> list[UnionSliderInfo]`:
     - Collects all active PCA components from all loaded models.
     - Per discretion: zero-fill for models that don't have a parameter. When a slider maps to a component that model A has but model B doesn't, model B treats that slider as zero (neutral/mean position).
     - Implementation: iterate over all active slots, collect each model's slider_info (from `controls.mapping.get_slider_info(analysis)`). Build a union list ordered by maximum variance across models. For each union slider, record which model slots have that component and the corresponding component index.
     - Strategy: Match sliders across models by label similarity if possible, otherwise by component order. Simple approach for v1: merge by index (component 0 from model A aligns with component 0 from model B). This is reasonable since all models use the same PCA analysis structure with components ordered by variance.
     - Return list of UnionSliderInfo, max length = max(n_active_components across all models).

   - Add `blend_latent_space(slots: list[ModelSlot], latent_vectors: list[np.ndarray], weights: list[float]) -> np.ndarray`:
     - Validate all models have same latent_dim. Raise ValueError if not.
     - Compute normalized weights.
     - Return weighted sum: `sum(w * z for w, z in zip(norm_weights, latent_vectors))`.
     - Returns single blended latent vector.

   - Add `blend_audio_domain(audio_outputs: list[np.ndarray], weights: list[float]) -> np.ndarray`:
     - Compute normalized weights.
     - Pad shorter outputs with zeros to match longest.
     - Handle both mono [samples] and stereo [2, samples] (check ndim of first output; all should be same shape).
     - Return weighted sum as float32.
  </action>
  <verify>
    Run: `uv run python -c "from small_dataset_audio.inference.blending import BlendMode, ModelSlot, normalize_weights, MAX_BLEND_MODELS; print('Blending module OK')"` succeeds.
    Run: `uv run python -c "from small_dataset_audio.inference.blending import normalize_weights; assert normalize_weights([60, 40]) == [0.6, 0.4]; assert normalize_weights([0, 0]) == [0.5, 0.5]; print('Weight normalization correct')"` succeeds.
  </verify>
  <done>
    BlendMode enum with LATENT and AUDIO modes. ModelSlot holds a loaded model with weight. normalize_weights auto-normalizes to sum to 1.0. resolve_union_sliders builds unified slider set across loaded models. blend_latent_space and blend_audio_domain implement the two blending strategies.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add BlendEngine class with blend_generate and public API exports</name>
  <files>
    src/small_dataset_audio/inference/blending.py
    src/small_dataset_audio/inference/__init__.py
  </files>
  <action>
1. Add `BlendEngine` class to `src/small_dataset_audio/inference/blending.py`:
   - Constructor: `__init__(self)` with `self.slots: list[ModelSlot] = []` and `self.blend_mode: BlendMode = BlendMode.LATENT`.

   - `add_model(self, model, spectrogram, analysis, metadata, device, weight=25.0) -> int`:
     - Validate len(self.slots) < MAX_BLEND_MODELS. Raise ValueError if full.
     - Create ModelSlot with given params. Append to self.slots. Return slot index.

   - `remove_model(self, index: int) -> None`:
     - Remove slot at index. Move model to CPU to free GPU memory (per pitfall #6).

   - `set_weight(self, index: int, weight: float) -> None`:
     - Set raw weight (0-100) for slot at index.

   - `set_blend_mode(self, mode: BlendMode) -> None`:
     - If mode is LATENT, validate all active models share latent_dim. Raise ValueError if mismatch with message suggesting audio-domain blending instead.
     - Set self.blend_mode.

   - `get_active_slots(self) -> list[ModelSlot]`:
     - Return [s for s in self.slots if s.active and s.weight > 0].

   - `get_union_sliders(self) -> list[UnionSliderInfo]`:
     - Call resolve_union_sliders with active slots. Return result.

   - `blend_generate(self, slider_positions: list[int], config: GenerationConfig) -> GenerationResult`:
     - Lazy imports.
     - Get active slots. If none, raise ValueError.
     - If only 1 active slot, just generate normally from that model (no blending needed).
     - Get normalized weights for active slots.
     - If blend_mode is LATENT:
       - Build per-model latent vectors from slider_positions using union slider resolution.
       - For each model, construct a SliderState with positions mapped to that model's components (zero-fill for missing components, per discretion).
       - Call sliders_to_latent for each model to get per-model latent vectors.
       - Blend via blend_latent_space.
       - Use the first model's pipeline to decode the blended vector (all share same architecture).
       - Set config.latent_vector to the blended vector.
       - Create GenerationPipeline from first active slot, generate, return result.
     - If blend_mode is AUDIO:
       - Generate audio independently from each model (each gets its own pipeline).
       - For each model, build per-model latent vector from slider_positions.
       - Generate audio from each.
       - Blend via blend_audio_domain.
       - Build GenerationResult with blended audio, quality score, and merged metadata.
     - Return GenerationResult.

2. Update `src/small_dataset_audio/inference/__init__.py`:
   - Add import of `BlendMode`, `BlendEngine`, `ModelSlot`, `MAX_BLEND_MODELS` from `inference.blending`.
   - Add all four to `__all__`.
  </action>
  <verify>
    Run: `uv run python -c "from small_dataset_audio.inference.blending import BlendEngine, BlendMode; e = BlendEngine(); e.set_blend_mode(BlendMode.AUDIO); print('BlendEngine OK')"` succeeds.
    Run: `uv run python -c "from small_dataset_audio.inference import BlendEngine, BlendMode, ModelSlot, MAX_BLEND_MODELS; assert MAX_BLEND_MODELS == 4; print('Re-exports OK')"` succeeds.
  </verify>
  <done>
    BlendEngine manages up to 4 model slots with add/remove/set_weight operations. blend_generate handles both latent-space and audio-domain blending with union slider resolution. Latent-space mode validates matching latent_dim. Audio-domain mode works universally. GPU memory managed by moving inactive models to CPU. Public API re-exports updated.
  </done>
</task>

</tasks>

<verification>
1. `uv run python -c "from small_dataset_audio.inference.blending import BlendEngine, BlendMode, ModelSlot, normalize_weights, blend_audio_domain, MAX_BLEND_MODELS; print('All blending symbols available')"` -- passes
2. `uv run python -c "from small_dataset_audio.inference.blending import normalize_weights; w = normalize_weights([75, 25, 0]); assert abs(w[0] - 0.75) < 0.01; assert abs(w[1] - 0.25) < 0.01; assert w[2] == 0.0; print('Normalization correct')"` -- passes
3. `uv run python -c "from small_dataset_audio.inference import BlendEngine, BlendMode; print('Public API re-exports work')"` -- passes
</verification>

<success_criteria>
- BlendEngine can hold up to 4 ModelSlots simultaneously
- Individual weights per model (0-100%), auto-normalized to sum to 100%
- BlendMode enum with LATENT and AUDIO modes, user can toggle
- Latent-space blending validates matching latent_dim before allowing
- Audio-domain blending works regardless of model architecture
- Union slider resolution: single slider set maps to whichever models have that parameter
- Non-shared parameters zero-filled (neutral position) per discretion decision
- Inactive models moved to CPU to manage memory pressure
- Public API re-exports in inference/__init__.py
</success_criteria>

<output>
After completion, create `.planning/phases/10-multi-format-export-spatial-audio/10-03-SUMMARY.md`
</output>
