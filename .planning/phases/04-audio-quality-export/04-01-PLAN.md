---
phase: 04-audio-quality-export
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/small_dataset_audio/audio/filters.py
  - src/small_dataset_audio/inference/chunking.py
autonomous: true

must_haves:
  truths:
    - "Anti-aliasing filter removes frequencies above 20kHz from generated audio"
    - "Multiple chunks can be concatenated via Hann-windowed crossfade without audible clicks"
    - "Multiple chunks can be concatenated via SLERP latent interpolation for smooth evolving sound"
  artifacts:
    - path: "src/small_dataset_audio/audio/filters.py"
      provides: "Butterworth low-pass anti-aliasing filter"
      contains: "apply_anti_alias_filter"
    - path: "src/small_dataset_audio/inference/chunking.py"
      provides: "Chunk generation with crossfade and latent interpolation"
      contains: "generate_chunks_crossfade"
  key_links:
    - from: "src/small_dataset_audio/inference/chunking.py"
      to: "src/small_dataset_audio/models/vae.py"
      via: "ConvVAE.sample() and ConvVAE.decode() for latent vector generation"
      pattern: "model\\.sample|model\\.decode"
    - from: "src/small_dataset_audio/inference/chunking.py"
      to: "src/small_dataset_audio/audio/spectrogram.py"
      via: "AudioSpectrogram.mel_to_waveform() for mel-to-audio conversion"
      pattern: "spectrogram\\.mel_to_waveform"
---

<objective>
Build the anti-aliasing filter and chunk-based audio generation engine with two concatenation modes (crossfade and latent interpolation).

Purpose: The VAE generates fixed-length chunks. For configurable-duration output (up to 60s), multiple chunks must be generated and concatenated seamlessly. Anti-aliasing ensures no artifacts above 20kHz in the final audio. Per user decision, BOTH crossfade and latent interpolation modes must be available.

Output: `audio/filters.py` (anti-aliasing), `inference/chunking.py` (chunk generation + concatenation), scipy added to dependencies.
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-audio-quality-export/04-RESEARCH.md
@.planning/phases/03-core-training-engine/03-01-SUMMARY.md
@src/small_dataset_audio/audio/spectrogram.py
@src/small_dataset_audio/models/vae.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Anti-aliasing filter and scipy dependency</name>
  <files>pyproject.toml, src/small_dataset_audio/audio/filters.py</files>
  <action>
1. Add `scipy>=1.12` to `[project] dependencies` in pyproject.toml. Run `uv lock` afterward to update the lockfile.

2. Create `src/small_dataset_audio/audio/filters.py` with:

- `apply_anti_alias_filter(audio: np.ndarray, sample_rate: int, cutoff_hz: float = 20_000.0, order: int = 8) -> np.ndarray`
  - Lazy import scipy.signal (project pattern: imports inside function body)
  - Uses `scipy.signal.butter(order, normalized_cutoff, btype='low', output='sos')` for filter design
  - Uses `scipy.signal.sosfiltfilt(sos, audio, axis=-1)` for zero-phase filtering
  - Clamp `normalized_cutoff = min(cutoff_hz, nyquist * 0.95) / nyquist` to prevent instability at Nyquist
  - Handle edge case: if `cutoff_hz >= sample_rate / 2`, return audio unchanged (no filtering needed)
  - Handle both 1-D `[samples]` and 2-D `[channels, samples]` input shapes via `axis=-1`
  - Type: `np.ndarray` in, `np.ndarray` out (float32)
  - Docstring with Parameters/Returns following project conventions

3. Use lazy imports for numpy and scipy (following project pattern from `training/preview.py`).

Do NOT hand-roll the filter with numpy. Use scipy.signal.butter + sosfiltfilt per research recommendation.
  </action>
  <verify>
Run: `uv run python -c "from small_dataset_audio.audio.filters import apply_anti_alias_filter; import numpy as np; x = np.random.randn(48000).astype(np.float32); y = apply_anti_alias_filter(x, 48000); print('Shape:', y.shape, 'dtype:', y.dtype, 'OK')"` -- should print shape (48000,), dtype float32, OK.
  </verify>
  <done>scipy is in project dependencies, anti-aliasing filter module exists with Butterworth low-pass implementation, handles 1-D and 2-D arrays, returns float32 numpy arrays.</done>
</task>

<task type="auto">
  <name>Task 2: Chunk generation with crossfade and latent interpolation</name>
  <files>src/small_dataset_audio/inference/chunking.py</files>
  <action>
Create `src/small_dataset_audio/inference/chunking.py` with the following functions:

1. `slerp(v0: torch.Tensor, v1: torch.Tensor, t: float, dot_threshold: float = 0.9995) -> torch.Tensor`
   - Spherical linear interpolation between two latent vectors
   - Falls back to `torch.lerp` when vectors are nearly parallel (dot > threshold)
   - Per research pattern: normalize, compute angle, interpolate on sphere, scale by original magnitudes
   - Do NOT use linear interpolation (lerp) as default -- SLERP is required for latent space traversal

2. `crossfade_chunks(chunks: list[np.ndarray], overlap_samples: int = 2400) -> np.ndarray`
   - Hann-windowed overlap-add crossfade (per research pattern)
   - 2400 samples = 50ms at 48kHz (discretion: crossfade overlap duration)
   - Handle edge cases: 0 chunks -> empty array, 1 chunk -> return unchanged
   - Use `np.hanning(2 * overlap_samples)` for symmetric fade window
   - Return float32 numpy array

3. `generate_chunks_crossfade(model, spectrogram, num_chunks: int, device, seed: int | None, chunk_samples: int = 48_000, overlap_samples: int = 2400) -> np.ndarray`
   - Generate `num_chunks` independent latent vectors via `_sample_latent_vectors`
   - Decode each through VAE decoder -> mel -> waveform (one at a time to limit memory per research open question #4)
   - Set model to eval mode, use torch.no_grad()
   - Crossfade waveforms using `crossfade_chunks`
   - Return concatenated float32 numpy array

4. `generate_chunks_latent_interp(model, spectrogram, num_chunks: int, device, seed: int | None, chunk_samples: int = 48_000, steps_between: int = 10) -> np.ndarray`
   - Generate `num_chunks` anchor latent vectors
   - Interpolate between each pair using SLERP with `steps_between` intermediate points
   - Decode each interpolated latent vector to waveform (one at a time, streaming approach)
   - Concatenate all waveforms sequentially (no crossfade needed -- interpolation ensures smooth transitions)
   - Return float32 numpy array

5. `_sample_latent_vectors(model, num_vectors: int, device, seed: int | None) -> list[torch.Tensor]`
   - Private helper to generate N random latent vectors from the VAE's prior distribution (standard normal)
   - If seed is not None, set `torch.manual_seed(seed)` for reproducibility
   - Each vector shape: `[1, latent_dim]` matching model.latent_dim
   - Return list of tensors on the specified device

Key patterns:
- Lazy imports for torch, numpy (project pattern)
- Model set to eval mode before generation, restored after
- InverseMelScale + GriffinLim run on CPU (existing pattern from spectrogram.py -- mel_to_waveform forces CPU)
- Process chunks incrementally to limit memory (don't hold all mel spectrograms at once)
- mel_shape computed from spectrogram.get_mel_shape(chunk_samples)
- model.decode(z, target_shape=mel_shape) to pass mel shape to decoder

Type annotations reference model as "ConvVAE" and spectrogram as "AudioSpectrogram" using string literals for lazy typing.
  </action>
  <verify>
Run: `uv run python -c "from small_dataset_audio.inference.chunking import slerp, crossfade_chunks; import torch, numpy as np; v0 = torch.randn(64); v1 = torch.randn(64); mid = slerp(v0, v1, 0.5); print('SLERP shape:', mid.shape); chunks = [np.random.randn(48000).astype(np.float32) for _ in range(3)]; out = crossfade_chunks(chunks, 2400); print('Crossfade shape:', out.shape, 'OK')"` -- should print SLERP shape: torch.Size([64]), crossfade result with expected length, OK.
  </verify>
  <done>Chunk generation module exists with SLERP interpolation, Hann crossfade, both crossfade and latent interpolation generation modes, incremental chunk processing to limit memory, and reproducible seed support.</done>
</task>

</tasks>

<verification>
1. `uv run python -c "import scipy; print('scipy', scipy.__version__)"` -- scipy installed
2. `uv run python -c "from small_dataset_audio.audio.filters import apply_anti_alias_filter"` -- filter importable
3. `uv run python -c "from small_dataset_audio.inference.chunking import generate_chunks_crossfade, generate_chunks_latent_interp, slerp, crossfade_chunks"` -- all functions importable
4. Anti-aliasing filter produces output same shape as input
5. Crossfade of 3 chunks produces array shorter than 3x chunk length (overlap removed)
6. SLERP of two vectors produces vector with same shape
</verification>

<success_criteria>
- scipy added to project dependencies and lockfile updated
- Anti-aliasing filter uses 8th-order Butterworth with zero-phase filtering via sosfiltfilt
- Both crossfade and latent interpolation chunk concatenation modes implemented
- SLERP used for latent space interpolation (not lerp)
- Chunks processed incrementally (one at a time) to limit memory usage
- All functions importable without errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-audio-quality-export/04-01-SUMMARY.md`
</output>
