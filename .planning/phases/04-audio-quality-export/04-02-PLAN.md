---
phase: 04-audio-quality-export
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/small_dataset_audio/inference/stereo.py
  - src/small_dataset_audio/inference/quality.py
autonomous: true

must_haves:
  truths:
    - "Mono audio can be widened to stereo using mid-side processing with Haas effect"
    - "Stereo can be generated from two different seeds for L/R channels (dual-seed mode)"
    - "Quality score reports SNR in dB and clipping detection percentage"
    - "Quality score categorized as green/yellow/red traffic light"
  artifacts:
    - path: "src/small_dataset_audio/inference/stereo.py"
      provides: "Mid-side stereo widening and dual-seed stereo generation"
      contains: "apply_mid_side_widening"
    - path: "src/small_dataset_audio/inference/quality.py"
      provides: "SNR calculation, clipping detection, quality score"
      contains: "compute_quality_score"
  key_links:
    - from: "src/small_dataset_audio/inference/stereo.py"
      to: "numpy"
      via: "np.ndarray operations for stereo processing"
      pattern: "np\\.stack|np\\.zeros_like"
    - from: "src/small_dataset_audio/inference/quality.py"
      to: "numpy"
      via: "np.ndarray operations for signal analysis"
      pattern: "np\\.mean|np\\.abs"
---

<objective>
Build stereo processing (mid-side widening + dual-seed generation) and quality metrics (SNR + clipping detection + traffic light score).

Purpose: Per user decisions, stereo is opt-in per generation with two method choices (mid-side widening or dual-seed). Quality feedback with SNR + clipping detection gives users immediate, practical audio quality assessment after each generation. These modules are independent of the chunk generation engine and can be built in parallel.

Output: `inference/stereo.py` (stereo processing), `inference/quality.py` (quality metrics).
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-audio-quality-export/04-RESEARCH.md
@.planning/phases/04-audio-quality-export/04-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Stereo processing with mid-side widening and dual-seed</name>
  <files>src/small_dataset_audio/inference/stereo.py</files>
  <action>
Create `src/small_dataset_audio/inference/stereo.py` with the following functions:

1. `apply_mid_side_widening(mono: np.ndarray, width: float = 0.7, sample_rate: int = 48_000, haas_delay_ms: float = 15.0) -> np.ndarray`
   - Input: 1-D mono array `[samples]`
   - Output: 2-D stereo array `[2, samples]`
   - Implementation (per research pattern):
     a. Create delayed copy for right channel using Haas effect: `delay_samples = int(haas_delay_ms * sample_rate / 1000.0)`
     b. `left = mono.copy()`, `right = np.zeros_like(mono)`, `right[delay_samples:] = mono[:-delay_samples]` (handle delay_samples=0 edge case)
     c. Mid-side decomposition: `mid = (left + right) * 0.5`, `side = (left - right) * 0.5`
     d. Apply width: `new_left = mid + width * side`, `new_right = mid - width * side`
     e. Stack as `[2, samples]`: `np.stack([new_left, new_right], axis=0)`
   - Width parameter: continuous 0.0 to 1.5 (discretion: slider range, 1.0 = natural width)
   - Validate width range: clamp to [0.0, 1.5] with warning if out of range
   - Return float32

2. `create_dual_seed_stereo(left_mono: np.ndarray, right_mono: np.ndarray) -> np.ndarray`
   - Input: two 1-D mono arrays (generated with different seeds by the pipeline)
   - Output: 2-D stereo array `[2, samples]`
   - Pad shorter channel with zeros if lengths differ
   - Stack as `[2, samples]`
   - This is a simple combiner -- the actual dual-seed generation (two different seeds for L/R) is handled by the GenerationPipeline in Plan 03
   - Return float32

3. `peak_normalize(audio: np.ndarray, target_peak: float = 0.891) -> np.ndarray`
   - Normalize peak amplitude to target_peak (default -1 dBFS = 0.891 for headroom, per research anti-pattern)
   - Handle edge case: if max(abs(audio)) == 0, return unchanged
   - Works for both 1-D and 2-D arrays
   - Return float32
   - IMPORTANT: Do NOT normalize to 1.0 -- use 0.891 (-1 dBFS) for professional headroom

Key patterns:
- Lazy import numpy (project pattern)
- All functions operate on numpy arrays, not torch tensors (stereo processing is post-generation, CPU-only)
- Docstrings with Parameters/Returns following project conventions
- Per user decision: stereo method is a user choice per generation, not a global setting
  </action>
  <verify>
Run: `uv run python -c "from small_dataset_audio.inference.stereo import apply_mid_side_widening, create_dual_seed_stereo, peak_normalize; import numpy as np; mono = np.random.randn(48000).astype(np.float32); stereo = apply_mid_side_widening(mono, width=0.7); print('Mid-side shape:', stereo.shape); dual = create_dual_seed_stereo(mono, mono); print('Dual shape:', dual.shape); normed = peak_normalize(mono); print('Peak:', np.abs(normed).max()); print('OK')"` -- should print shape (2, 48000), (2, 48000), peak ~0.891, OK.
  </verify>
  <done>Stereo processing module exists with mid-side widening (Haas effect + width control 0.0-1.5), dual-seed stereo combiner, and peak normalization at -1 dBFS headroom.</done>
</task>

<task type="auto">
  <name>Task 2: Quality metrics with SNR, clipping detection, and score</name>
  <files>src/small_dataset_audio/inference/quality.py</files>
  <action>
Create `src/small_dataset_audio/inference/quality.py` with the following functions:

1. `compute_snr_db(audio: np.ndarray, sample_rate: int = 48_000, silence_threshold: float = 0.01) -> float`
   - Frame-based SNR estimation (per research pattern)
   - Frame size: 10ms (480 samples at 48kHz), computed as `int(0.01 * sample_rate)`
   - Classify frames: RMS > silence_threshold = signal, else = noise
   - Compute average power for signal frames and noise frames
   - Return `10 * np.log10(avg_signal / avg_noise)` in dB
   - Handle edge cases: no noise frames -> return `float('inf')`, no signal frames -> return 0.0
   - Flatten input to 1-D before processing (handles both mono and stereo)

2. `detect_clipping(audio: np.ndarray, threshold: float = 0.999, consecutive_threshold: int = 3) -> dict`
   - Per research pattern: detect samples at or above threshold
   - Return dict with keys:
     - `clipped_samples`: int count
     - `clipped_percentage`: float (0-100)
     - `peak_value`: float (maximum absolute value)
     - `max_consecutive_clipped`: int (longest run of clipped samples)
     - `has_clipping`: bool
   - Flatten input to 1-D before processing
   - Use vectorized numpy operations (no Python for-loop for consecutive detection -- use np.diff on clipped indices)

3. `compute_quality_score(audio: np.ndarray, sample_rate: int = 48_000) -> dict`
   - Orchestrates SNR + clipping detection into a single quality report
   - Return dict with keys:
     - `snr_db`: float (from compute_snr_db)
     - `clipping`: dict (from detect_clipping)
     - `rating`: str -- "green", "yellow", or "red" (discretion: traffic light presentation)
     - `rating_reason`: str -- human-readable explanation of rating
   - Rating thresholds (per research discretion recommendation):
     - **green**: SNR > 30 dB AND no clipping
     - **yellow**: SNR 15-30 dB OR < 0.1% clipped samples
     - **red**: SNR < 15 dB OR > 0.1% clipped samples
   - Per user decision: NO spectral coverage or flatness metrics (keep it focused on SNR + clipping)

Key patterns:
- Lazy import numpy (project pattern)
- All functions operate on numpy arrays
- Docstrings with Parameters/Returns following project conventions
- Per user decision: quality score is practical (SNR + clipping), not academic
  </action>
  <verify>
Run: `uv run python -c "from small_dataset_audio.inference.quality import compute_snr_db, detect_clipping, compute_quality_score; import numpy as np; audio = np.random.randn(48000).astype(np.float32) * 0.5; snr = compute_snr_db(audio); print('SNR:', snr); clip = detect_clipping(audio); print('Clipping:', clip['has_clipping']); score = compute_quality_score(audio); print('Rating:', score['rating']); print('OK')"` -- should print SNR value, clipping False, rating green or yellow, OK.
  </verify>
  <done>Quality metrics module exists with frame-based SNR calculation, clipping detection with consecutive sample analysis, and traffic light quality score (green/yellow/red) combining both metrics.</done>
</task>

</tasks>

<verification>
1. `uv run python -c "from small_dataset_audio.inference.stereo import apply_mid_side_widening, create_dual_seed_stereo, peak_normalize"` -- all stereo functions importable
2. `uv run python -c "from small_dataset_audio.inference.quality import compute_snr_db, detect_clipping, compute_quality_score"` -- all quality functions importable
3. Mid-side widening produces [2, samples] from [samples] input
4. Peak normalize produces max absolute value of ~0.891
5. SNR returns finite positive value for signal+noise audio
6. Clipping detection returns False for audio with peak < 0.999
7. Quality score returns dict with rating key
</verification>

<success_criteria>
- Mid-side stereo widening with Haas effect delay and continuous width control (0.0-1.5)
- Dual-seed stereo combiner for two independently generated channels
- Peak normalization at -1 dBFS (0.891) for professional headroom
- Frame-based SNR calculation in dB
- Clipping detection with percentage, peak value, and consecutive sample count
- Traffic light quality score (green/yellow/red) combining SNR and clipping thresholds
- All functions use numpy arrays (no torch dependency in these modules)
</success_criteria>

<output>
After completion, create `.planning/phases/04-audio-quality-export/04-02-SUMMARY.md`
</output>
