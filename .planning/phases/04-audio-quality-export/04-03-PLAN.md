---
phase: 04-audio-quality-export
plan: 03
type: execute
wave: 2
depends_on: ["04-01", "04-02"]
files_modified:
  - src/small_dataset_audio/inference/generation.py
  - src/small_dataset_audio/inference/export.py
  - src/small_dataset_audio/inference/__init__.py
  - src/small_dataset_audio/audio/__init__.py
autonomous: true

must_haves:
  truths:
    - "User can generate audio from a trained model with configurable duration (up to 60s)"
    - "User can choose between crossfade and latent interpolation concatenation modes"
    - "User can choose mono, mid-side stereo, or dual-seed stereo output"
    - "User can export generated audio as WAV at configurable sample rate and bit depth"
    - "Exported WAV has sidecar JSON with full generation metadata"
    - "Generated audio has anti-aliasing applied (no artifacts above 20kHz)"
    - "Quality score is computed and included in generation result"
  artifacts:
    - path: "src/small_dataset_audio/inference/generation.py"
      provides: "GenerationPipeline orchestrator and GenerationConfig dataclass"
      contains: "class GenerationPipeline"
    - path: "src/small_dataset_audio/inference/export.py"
      provides: "WAV export with configurable format and sidecar JSON"
      contains: "export_wav"
    - path: "src/small_dataset_audio/inference/__init__.py"
      provides: "Public API re-exports for all inference modules"
      contains: "GenerationPipeline"
  key_links:
    - from: "src/small_dataset_audio/inference/generation.py"
      to: "src/small_dataset_audio/inference/chunking.py"
      via: "generate_chunks_crossfade/generate_chunks_latent_interp for chunk-based synthesis"
      pattern: "generate_chunks_crossfade|generate_chunks_latent_interp"
    - from: "src/small_dataset_audio/inference/generation.py"
      to: "src/small_dataset_audio/inference/stereo.py"
      via: "apply_mid_side_widening/create_dual_seed_stereo for stereo processing"
      pattern: "apply_mid_side_widening|create_dual_seed_stereo"
    - from: "src/small_dataset_audio/inference/generation.py"
      to: "src/small_dataset_audio/audio/filters.py"
      via: "apply_anti_alias_filter before export"
      pattern: "apply_anti_alias_filter"
    - from: "src/small_dataset_audio/inference/generation.py"
      to: "src/small_dataset_audio/inference/quality.py"
      via: "compute_quality_score for generation quality assessment"
      pattern: "compute_quality_score"
    - from: "src/small_dataset_audio/inference/export.py"
      to: "soundfile"
      via: "sf.write with configurable subtype for WAV export"
      pattern: "sf\\.write"
---

<objective>
Build the GenerationPipeline orchestrator that ties together chunk generation, stereo processing, anti-aliasing, quality metrics, and WAV export into a single coherent generation API. Also create the export module with sidecar JSON.

Purpose: This is the integration plan that combines all Phase 4 components into a usable generation pipeline. The GenerationPipeline is the primary interface for generating audio from trained models, handling all configuration (duration, concatenation mode, stereo mode, sample rate, bit depth) and producing a GenerationResult with audio data, quality metrics, and export capabilities.

Output: `inference/generation.py` (pipeline + config), `inference/export.py` (WAV + JSON), updated `inference/__init__.py` and `audio/__init__.py` with public API exports.
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-audio-quality-export/04-RESEARCH.md
@.planning/phases/04-audio-quality-export/04-CONTEXT.md
@.planning/phases/04-audio-quality-export/04-01-SUMMARY.md
@.planning/phases/04-audio-quality-export/04-02-SUMMARY.md
@src/small_dataset_audio/audio/spectrogram.py
@src/small_dataset_audio/models/vae.py
@src/small_dataset_audio/training/checkpoint.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: GenerationPipeline orchestrator, export module, and sidecar JSON</name>
  <files>src/small_dataset_audio/inference/generation.py, src/small_dataset_audio/inference/export.py</files>
  <action>
**Part A: Create `src/small_dataset_audio/inference/export.py`**

1. `BIT_DEPTH_MAP` -- module-level dict mapping user-facing strings to soundfile subtypes:
   ```
   {"16-bit": "PCM_16", "24-bit": "PCM_24", "32-bit float": "FLOAT"}
   ```

2. `SAMPLE_RATE_OPTIONS` -- tuple of valid sample rates: `(44_100, 48_000, 96_000)`

3. `export_wav(audio: np.ndarray, path: Path, sample_rate: int = 48_000, bit_depth: str = "24-bit") -> Path`
   - Lazy import soundfile
   - Map bit_depth to subtype via BIT_DEPTH_MAP (raise ValueError for invalid)
   - If audio is 2-D `[channels, samples]`, transpose to `[samples, channels]` for soundfile
   - `sf.write(str(path), audio_data, sample_rate, subtype=subtype)`
   - Return the path for chaining

4. `write_sidecar_json(wav_path: Path, model_name: str, generation_config: dict, seed: int | None, quality_metrics: dict, duration_s: float) -> Path`
   - Per research pattern and user decision: sidecar JSON alongside WAV, no embedded tags
   - Write JSON FIRST (before WAV in the pipeline) per research pitfall #6
   - Sidecar path: `wav_path.with_suffix(".json")`
   - Metadata structure:
     ```json
     {
       "version": 1,
       "timestamp": "ISO 8601 UTC",
       "model_name": "...",
       "seed": 42,
       "generation": { ...config dict... },
       "quality": { ...metrics dict... },
       "audio": {
         "file": "filename.wav",
         "format": "WAV",
         "sample_rate": 48000,
         "bit_depth": "24-bit",
         "channels": 1,
         "duration_s": 5.0
       }
     }
     ```
   - Use `datetime.now(timezone.utc).isoformat()` for timestamp
   - Return the sidecar path

**Part B: Create `src/small_dataset_audio/inference/generation.py`**

1. `@dataclass GenerationConfig`
   - `duration_s: float = 1.0` -- desired output duration in seconds (freeform per user decision)
   - `max_duration_s: float = 60.0` -- architecture limit (60s for v1 per user decision)
   - `seed: int | None = None` -- random seed for reproducibility
   - `chunk_duration_s: float = 1.0` -- configurable chunk duration per user decision
   - `concat_mode: str = "crossfade"` -- "crossfade" or "latent_interpolation" (BOTH available per user decision)
   - `stereo_mode: str = "mono"` -- "mono", "mid_side", or "dual_seed" (per user decision)
   - `stereo_width: float = 0.7` -- width for mid-side mode (0.0-1.5 per discretion)
   - `sample_rate: int = 48_000` -- output sample rate
   - `bit_depth: str = "24-bit"` -- output bit depth
   - `steps_between: int = 10` -- interpolation steps for latent_interpolation mode
   - `overlap_samples: int = 2400` -- crossfade overlap (50ms at 48kHz per discretion)
   - Validation method `validate()` that checks:
     - `duration_s > 0 and duration_s <= max_duration_s`
     - `concat_mode in ("crossfade", "latent_interpolation")`
     - `stereo_mode in ("mono", "mid_side", "dual_seed")`
     - `sample_rate in SAMPLE_RATE_OPTIONS`
     - `bit_depth in BIT_DEPTH_MAP`
     - Raises ValueError with descriptive message on invalid config

2. `@dataclass GenerationResult`
   - `audio: np.ndarray` -- generated audio `[samples]` or `[2, samples]`
   - `sample_rate: int` -- actual sample rate of audio data
   - `quality: dict` -- quality score from compute_quality_score
   - `config: GenerationConfig` -- config used for this generation
   - `seed_used: int` -- actual seed used (may differ from config if config.seed was None)
   - `duration_s: float` -- actual duration of generated audio
   - `channels: int` -- 1 for mono, 2 for stereo

3. `class GenerationPipeline`
   - `__init__(self, model: "ConvVAE", spectrogram: "AudioSpectrogram", device: "torch.device") -> None`
     - Store model, spectrogram, device
     - Store `model_name: str = "unknown"` (can be set by caller)

   - `generate(self, config: GenerationConfig) -> GenerationResult`
     - Full pipeline orchestration:
       a. Validate config
       b. Compute num_chunks = `math.ceil(config.duration_s / config.chunk_duration_s)`
       c. Compute chunk_samples from `config.chunk_duration_s * config.sample_rate` (use model's training sample rate, 48kHz)
       d. Set seed: if config.seed is None, generate random seed; store as seed_used
       e. Generate audio based on concat_mode:
          - "crossfade" -> `generate_chunks_crossfade(model, spectrogram, num_chunks, device, seed, chunk_samples, overlap_samples)`
          - "latent_interpolation" -> `generate_chunks_latent_interp(model, spectrogram, num_chunks, device, seed, chunk_samples, steps_between)`
       f. Apply anti-aliasing filter: `apply_anti_alias_filter(audio, 48_000)` (always at generation sample rate)
       g. Apply stereo processing based on stereo_mode:
          - "mono" -> no change
          - "mid_side" -> `apply_mid_side_widening(audio, config.stereo_width, 48_000)`
          - "dual_seed" -> Generate a second audio with seed+1, combine with `create_dual_seed_stereo(audio, audio_right)`
       h. Apply peak normalization: `peak_normalize(audio, target_peak=0.891)` (after stereo to catch clipping per research pitfall #3)
       i. Resample if config.sample_rate != 48_000:
          - Use `torchaudio.transforms.Resample(48_000, config.sample_rate)` (cached per existing project pattern)
          - Convert to tensor, resample, convert back to numpy
       j. Compute quality score: `compute_quality_score(audio, config.sample_rate)`
       k. Trim to exact requested duration: `audio = audio[:int(config.duration_s * config.sample_rate)]` (or `audio[:, :N]` for stereo)
       l. Return GenerationResult

   - `export(self, result: GenerationResult, output_dir: Path, filename: str | None = None) -> tuple[Path, Path]`
     - Auto-generate filename if None: `gen_{timestamp}_seed{seed}.wav` (per discretion: auto-naming)
     - Ensure output_dir exists (mkdir -p)
     - Write sidecar JSON first (per research pitfall #6)
     - Export WAV with result's sample rate and config bit depth
     - Return tuple of (wav_path, json_path)

Key patterns:
- Lazy imports for torch, numpy, torchaudio (project pattern)
- Model set to eval mode in generate (chunking functions handle this)
- Resampler caching: module-level dict for `torchaudio.transforms.Resample` instances (project pattern from Phase 2)
- Per user decision: default output folder is `data/generated/{model_name}/` (discretion)
- All audio processing at 48kHz internally, resample only at the very end before export
  </action>
  <verify>
Run: `uv run python -c "from small_dataset_audio.inference.generation import GenerationConfig, GenerationResult, GenerationPipeline; from small_dataset_audio.inference.export import export_wav, write_sidecar_json, BIT_DEPTH_MAP; config = GenerationConfig(); config.validate(); print('Config valid'); print('Bit depths:', list(BIT_DEPTH_MAP.keys())); print('OK')"` -- should print Config valid, bit depths, OK.
  </verify>
  <done>GenerationPipeline orchestrates chunks->concat->stereo->anti-alias->normalize->resample->quality->export pipeline. GenerationConfig supports all user-facing options (duration, concat mode, stereo mode, sample rate, bit depth). Export module writes WAV with configurable format and sidecar JSON with full generation metadata.</done>
</task>

<task type="auto">
  <name>Task 2: Public API exports for inference and audio modules</name>
  <files>src/small_dataset_audio/inference/__init__.py, src/small_dataset_audio/audio/__init__.py</files>
  <action>
1. **Update `src/small_dataset_audio/inference/__init__.py`** to re-export all public symbols:

   From `inference.generation`:
   - `GenerationConfig`, `GenerationResult`, `GenerationPipeline`

   From `inference.export`:
   - `export_wav`, `write_sidecar_json`, `BIT_DEPTH_MAP`, `SAMPLE_RATE_OPTIONS`

   From `inference.chunking`:
   - `slerp`, `crossfade_chunks`, `generate_chunks_crossfade`, `generate_chunks_latent_interp`

   From `inference.stereo`:
   - `apply_mid_side_widening`, `create_dual_seed_stereo`, `peak_normalize`

   From `inference.quality`:
   - `compute_snr_db`, `detect_clipping`, `compute_quality_score`

   Include `__all__` list with all exported names.

   Follow the existing pattern from `audio/__init__.py` and `models/__init__.py`: module-level docstring, explicit imports, `__all__` list.

2. **Update `src/small_dataset_audio/audio/__init__.py`** to add the new `filters` module export:

   Add import of `apply_anti_alias_filter` from `small_dataset_audio.audio.filters`.
   Add to `__all__` list.

   Preserve all existing imports and exports -- only ADD the new filter import.

Follow the public API re-export pattern established in Phase 3: `from small_dataset_audio.inference import GenerationPipeline, GenerationConfig` should work cleanly.
  </action>
  <verify>
Run: `uv run python -c "from small_dataset_audio.inference import GenerationPipeline, GenerationConfig, GenerationResult, export_wav, write_sidecar_json, compute_quality_score, apply_mid_side_widening, slerp, crossfade_chunks; from small_dataset_audio.audio import apply_anti_alias_filter; print('All imports OK')"` -- should print All imports OK.
  </verify>
  <done>Public API exports enable clean imports like `from small_dataset_audio.inference import GenerationPipeline` and `from small_dataset_audio.audio import apply_anti_alias_filter`. All Phase 4 modules accessible through package-level imports.</done>
</task>

</tasks>

<verification>
1. `uv run python -c "from small_dataset_audio.inference import GenerationPipeline, GenerationConfig"` -- pipeline importable
2. `uv run python -c "from small_dataset_audio.inference import export_wav, write_sidecar_json"` -- export importable
3. `uv run python -c "from small_dataset_audio.audio import apply_anti_alias_filter"` -- filter importable via audio package
4. GenerationConfig validates correctly (rejects invalid concat_mode, stereo_mode, sample_rate, bit_depth)
5. GenerationConfig defaults match user decisions: 48kHz, 24-bit, mono, crossfade, 1.0s duration
6. BIT_DEPTH_MAP contains all three options: 16-bit, 24-bit, 32-bit float
7. SAMPLE_RATE_OPTIONS contains 44100, 48000, 96000
</verification>

<success_criteria>
- GenerationPipeline.generate() orchestrates full pipeline: chunks -> concat -> anti-alias -> stereo -> normalize -> resample -> quality
- GenerationConfig supports all user-facing options with validation
- Export produces WAV at configurable sample rate and bit depth via soundfile
- Sidecar JSON written alongside every exported WAV with full generation metadata
- Dual-seed stereo generates two separate audio streams and combines them
- Sample rate conversion uses torchaudio.transforms.Resample (not hand-rolled)
- All Phase 4 public symbols accessible via package-level imports
- Peak normalization at -1 dBFS applied after stereo processing
</success_criteria>

<output>
After completion, create `.planning/phases/04-audio-quality-export/04-03-SUMMARY.md`
</output>
