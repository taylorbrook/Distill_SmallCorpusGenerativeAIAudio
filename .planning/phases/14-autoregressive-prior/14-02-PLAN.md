---
phase: 14-autoregressive-prior
plan: 02
type: execute
wave: 2
depends_on:
  - 14-01
files_modified:
  - src/distill/training/prior_loop.py
  - src/distill/training/metrics.py
autonomous: true
requirements:
  - GEN-01
  - GEN-06

must_haves:
  truths:
    - "train_prior() orchestrates full prior training given a VQ-VAE model path, dataset path, and PriorConfig"
    - "Each epoch computes validation perplexity (exp of cross-entropy loss) and reports it via callback"
    - "Memorization detection warns when validation perplexity drops below adaptive threshold (relaxed sensitivity per user decision)"
    - "Best checkpoint (lowest validation perplexity) is tracked and available for rollback"
    - "Training works end-to-end: load VQ-VAE -> extract codes -> split train/val -> train prior -> return trained prior + metadata"
  artifacts:
    - path: "src/distill/training/prior_loop.py"
      provides: "train_prior, train_prior_epoch, validate_prior_epoch, check_memorization"
      min_lines: 200
    - path: "src/distill/training/metrics.py"
      provides: "PriorStepMetrics, PriorEpochMetrics dataclasses"
      contains: "PriorEpochMetrics"
  key_links:
    - from: "src/distill/training/prior_loop.py"
      to: "src/distill/models/prior.py"
      via: "imports CodePrior, extract_code_sequences, flatten_codes"
      pattern: "from distill\\.models\\.prior import"
    - from: "src/distill/training/prior_loop.py"
      to: "src/distill/models/persistence.py"
      via: "loads VQ-VAE model via load_model_v2"
      pattern: "load_model_v2"
    - from: "src/distill/training/prior_loop.py"
      to: "torch.nn.functional.cross_entropy"
      via: "next-token prediction loss on flattened code sequences"
      pattern: "F\\.cross_entropy"
---

<objective>
Build the prior training loop with memorization detection and best-checkpoint tracking.

Purpose: This is the core training engine for the autoregressive prior. It orchestrates the full pipeline: loading a frozen VQ-VAE, extracting code sequences from the training dataset, splitting into train/val, training the CodePrior with cross-entropy loss, monitoring validation perplexity, detecting memorization, and tracking the best model state.

Output: `src/distill/training/prior_loop.py` (training functions) and extended `src/distill/training/metrics.py` (prior-specific metric dataclasses)
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-autoregressive-prior/14-RESEARCH.md
@.planning/phases/14-autoregressive-prior/14-01-SUMMARY.md
@src/distill/models/prior.py
@src/distill/training/prior_config.py
@src/distill/training/loop.py
@src/distill/training/metrics.py
@src/distill/models/persistence.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add PriorStepMetrics and PriorEpochMetrics to metrics.py</name>
  <files>src/distill/training/metrics.py</files>
  <action>
Extend `src/distill/training/metrics.py` by appending prior-specific metric dataclasses after the existing VQ-VAE metrics section. Follow the exact pattern established by VQStepMetrics/VQEpochMetrics.

**PriorStepMetrics dataclass:**
- `epoch: int` -- current epoch (0-indexed)
- `step: int` -- current step within epoch
- `total_steps: int` -- total steps in epoch
- `train_loss: float` -- cross-entropy loss for this step
- `learning_rate: float` -- current LR
- `step_time_s: float` -- wall-clock step time

**PriorEpochMetrics dataclass:**
- `epoch: int` -- current epoch (0-indexed)
- `total_epochs: int` -- total planned epochs
- `train_loss: float` -- average training cross-entropy loss
- `val_loss: float` -- average validation cross-entropy loss
- `val_perplexity: float` -- exp(val_loss)
- `best_perplexity: float` -- best validation perplexity seen so far
- `is_memorizing: bool` -- whether memorization was detected
- `memorization_message: str` -- warning message if memorizing, empty string otherwise
- `epoch_time_s: float` -- wall-clock epoch time
- `learning_rate: float` -- current LR

**PriorTrainingCompleteEvent dataclass:**
- `final_train_loss: float`
- `final_val_loss: float`
- `final_perplexity: float`
- `best_perplexity: float`
- `epochs_trained: int`
- `was_memorizing: bool`

Update the `MetricsCallback` type alias to include `PriorStepMetrics | PriorEpochMetrics | PriorTrainingCompleteEvent` in its Union type.

Do NOT modify any existing dataclasses. Append new classes after the VQ-VAE section with a clear `# v1.1 Prior metrics` section comment.
  </action>
  <verify>
Run: `cd H:/dev/Distill-vqvae && python -c "from distill.training.metrics import PriorStepMetrics, PriorEpochMetrics, PriorTrainingCompleteEvent; p = PriorEpochMetrics(epoch=0, total_epochs=50, train_loss=4.5, val_loss=4.2, val_perplexity=66.7, best_perplexity=66.7, is_memorizing=False, memorization_message='', epoch_time_s=1.2, learning_rate=1e-3); print(f'PriorEpochMetrics: perplexity={p.val_perplexity}'); print('All prior metrics checks passed')"`
  </verify>
  <done>PriorStepMetrics, PriorEpochMetrics, and PriorTrainingCompleteEvent dataclasses exist in metrics.py. MetricsCallback union type accepts prior metric events.</done>
</task>

<task type="auto">
  <name>Task 2: Create prior training loop with memorization detection and best-checkpoint tracking</name>
  <files>src/distill/training/prior_loop.py</files>
  <action>
Create `src/distill/training/prior_loop.py` with the following functions. Follow the structure and patterns of `training/loop.py` (train_vqvae pattern).

**check_memorization(val_perplexity, codebook_size, dataset_file_count) -> tuple[bool, str]:**
- Relaxed sensitivity per user decision (only warn when very likely)
- Adaptive thresholds: `<= 20 files -> threshold 2.0`, `<= 100 files -> threshold 3.0`, `> 100 files -> threshold 5.0`
- Returns `(is_memorizing, message)` tuple
- If `val_perplexity < threshold`: return True with descriptive warning message
- Otherwise: return False with empty string

**train_prior_epoch(prior_model, train_codes, optimizer, device, codebook_size, batch_size, callback) -> float:**
- Trains one epoch on shuffled train_codes tensor `[N_train, flat_seq_len]`
- Shuffles indices each epoch (torch.randperm)
- For each mini-batch: `input = batch[:, :-1]`, `target = batch[:, 1:]` (standard next-token setup)
- Forward pass: `logits = prior_model(input)` -> `[B, T-1, V]`
- Loss: `F.cross_entropy(logits.reshape(-1, codebook_size), target.reshape(-1))`
- Backward, clip gradients (`torch.nn.utils.clip_grad_norm_`), optimizer step
- Emit PriorStepMetrics via callback if provided
- NaN detection: skip gradient update on NaN loss (project pattern from loop.py)
- Returns average training loss for the epoch

**validate_prior_epoch(prior_model, val_codes, device, codebook_size, batch_size) -> float:**
- Validates on val_codes with torch.no_grad()
- Same next-token prediction loss but with `reduction="sum"` divided by total tokens for accurate average
- Returns average validation cross-entropy loss

**train_prior(model_path, dataset_dir, prior_config, callback, cancel_event) -> dict:**
This is the top-level orchestrator. Steps:

1. Load VQ-VAE model: `load_model_v2(model_path, device)` -> get model, spectrogram, metadata, vqvae_config
2. Set VQ-VAE to eval, freeze all parameters (`param.requires_grad_(False)`)
3. Create data loaders from dataset_dir using existing `create_data_loaders` (with a shim TrainingConfig for compatibility, same pattern as train_vqvae)
4. Extract code sequences: call `extract_code_sequences(vqvae_model, train_loader, spectrogram, device)` for train set, same for val set
5. Flatten codes: `flatten_codes(all_train_indices)` -> `[N_train, flat_seq_len]`, same for val
6. Determine seq_len = flat_seq_len, num_quantizers from VQ-VAE model config
7. Create CodePrior model with `codebook_size` from vqvae_config, `seq_len` from flattened length, `num_quantizers` from vqvae_config, and hidden_size/num_layers/num_heads/dropout from prior_config
8. Create AdamW optimizer with prior_config.learning_rate and weight_decay
9. Create CosineAnnealingLR scheduler (T_max = prior_config.max_epochs)
10. Training loop:
    - For each epoch up to max_epochs (or until cancel_event is set):
      - `train_loss = train_prior_epoch(...)`
      - `val_loss = validate_prior_epoch(...)`
      - `val_perplexity = math.exp(val_loss)`
      - Track best: if val_perplexity < best_val_perplexity, deep copy state_dict
      - Check memorization: `check_memorization(val_perplexity, codebook_size, dataset_file_count)`
      - Emit PriorEpochMetrics via callback
      - Step scheduler
11. After training completes: load best checkpoint state_dict into prior model
12. Emit PriorTrainingCompleteEvent
13. Return dict with: `prior_model` (with best weights loaded), `prior_config` dict (for persistence), `prior_metadata` dict (epochs_trained, final_perplexity, best_perplexity, training_date), `codebook_size`, `seq_len`, `num_quantizers`

Use `from __future__ import annotations`, lazy imports for torch/heavy deps inside function bodies (match loop.py pattern), module-level logger. TYPE_CHECKING block for type hints.

Important: The VQ-VAE model is NOT needed during prior training itself -- only for code extraction. After extracting codes, the prior trains purely on the code tensors. This ensures no gradients leak into the VQ-VAE (RESEARCH.md pitfall 5).
  </action>
  <verify>
Run: `cd H:/dev/Distill-vqvae && python -c "from distill.training.prior_loop import train_prior, train_prior_epoch, validate_prior_epoch, check_memorization; ok, msg = check_memorization(1.5, 64, 10); assert ok == True; ok2, msg2 = check_memorization(10.0, 256, 200); assert ok2 == False; print('check_memorization works correctly'); print('All prior loop imports verified')"`
  </verify>
  <done>train_prior orchestrates the full pipeline: load VQ-VAE -> extract codes -> train prior with cross-entropy -> track best checkpoint -> detect memorization. check_memorization uses adaptive thresholds. All training functions are importable.</done>
</task>

</tasks>

<verification>
1. `python -c "from distill.training.prior_loop import train_prior, check_memorization"` -- importable
2. `python -c "from distill.training.metrics import PriorEpochMetrics, PriorStepMetrics"` -- importable
3. check_memorization returns True for val_perplexity=1.5 with 10 files (threshold 2.0)
4. check_memorization returns False for val_perplexity=10.0 with 200 files (threshold 5.0)
5. PriorEpochMetrics contains val_perplexity, best_perplexity, is_memorizing fields
6. train_prior function signature accepts model_path, dataset_dir, prior_config, callback, cancel_event
</verification>

<success_criteria>
- Prior training loop follows the established project pattern (parallel to train_vqvae)
- Cross-entropy loss computed on next-token prediction (input[:, :-1] -> target[:, 1:])
- Validation perplexity = exp(avg_cross_entropy_loss) computed each epoch
- Best checkpoint tracked via deepcopy of state_dict when val_perplexity improves
- Memorization detection uses relaxed adaptive thresholds (2.0 / 3.0 / 5.0 by dataset tier)
- VQ-VAE is completely frozen and only used for code extraction (not during prior training)
- Prior metrics dataclasses follow the same pattern as VQ metrics
</success_criteria>

<output>
After completion, create `.planning/phases/14-autoregressive-prior/14-02-SUMMARY.md`
</output>
