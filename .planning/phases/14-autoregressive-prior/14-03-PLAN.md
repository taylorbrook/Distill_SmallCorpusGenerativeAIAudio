---
phase: 14-autoregressive-prior
plan: 03
type: execute
wave: 3
depends_on:
  - 14-02
files_modified:
  - src/distill/models/persistence.py
  - src/distill/cli/train_prior.py
  - src/distill/models/__init__.py
  - src/distill/training/__init__.py
  - src/distill/cli/__init__.py
autonomous: true
requirements:
  - GEN-05
  - PERS-02
  - CLI-02

must_haves:
  truths:
    - "Prior model state is saved into the .sda file with has_prior flag, prior_state_dict, prior_config, and prior_metadata"
    - "Loading a model with has_prior=True reconstructs the CodePrior and makes it available on LoadedVQModel.prior"
    - "User can run distill train-prior MODEL_PATH to train a prior from the CLI"
    - "CLI flags --epochs, --hidden-size, --layers, --heads mirror the PriorConfig parameters"
    - "CLI shows per-epoch validation perplexity and memorization warnings during training"
  artifacts:
    - path: "src/distill/models/persistence.py"
      provides: "save_prior_to_model, updated load_model_v2 with prior support, LoadedVQModel.prior field"
      contains: "has_prior"
    - path: "src/distill/cli/train_prior.py"
      provides: "distill train-prior CLI command with Rich progress and perplexity display"
      min_lines: 80
    - path: "src/distill/models/__init__.py"
      provides: "CodePrior and prior persistence exports"
      contains: "CodePrior"
    - path: "src/distill/training/__init__.py"
      provides: "Prior training loop and config exports"
      contains: "train_prior"
  key_links:
    - from: "src/distill/models/persistence.py"
      to: "src/distill/models/prior.py"
      via: "reconstructs CodePrior from prior_config when loading"
      pattern: "CodePrior"
    - from: "src/distill/cli/train_prior.py"
      to: "src/distill/training/prior_loop.py"
      via: "calls train_prior() and emits PriorEpochMetrics via callback"
      pattern: "train_prior"
    - from: "src/distill/cli/train_prior.py"
      to: "src/distill/models/persistence.py"
      via: "calls save_prior_to_model() after training completes"
      pattern: "save_prior_to_model"
---

<objective>
Bundle prior state into .sda model files and create the CLI command for prior training.

Purpose: Completes the prior training pipeline by enabling persistence (saving trained prior into existing model file, loading it back) and providing a CLI interface. After this plan, users can train a prior from the command line by pointing at a saved VQ-VAE model.

Output: Extended `persistence.py` with prior save/load, `cli/train_prior.py` CLI command, updated `__init__.py` exports
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-autoregressive-prior/14-RESEARCH.md
@.planning/phases/14-autoregressive-prior/14-01-SUMMARY.md
@.planning/phases/14-autoregressive-prior/14-02-SUMMARY.md
@src/distill/models/persistence.py
@src/distill/models/prior.py
@src/distill/training/prior_loop.py
@src/distill/training/prior_config.py
@src/distill/cli/train.py
@src/distill/cli/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend persistence layer to save and load prior in .sda files</name>
  <files>src/distill/models/persistence.py</files>
  <action>
Extend `src/distill/models/persistence.py` with prior bundling support:

**1. Update LoadedVQModel dataclass:**
Add a `prior` field: `prior: "CodePrior | None" = None` -- the loaded prior model (None if no prior trained). This is the key change that enables "loading a model with a prior auto-loads both."

**2. Create save_prior_to_model(model_path, prior_model, prior_config, prior_metadata) -> Path:**
- Loads the existing .sda file via `torch.load(model_path, ...)`
- Validates it is a v2 VQ-VAE model (format marker + version check)
- Adds/updates the following keys in the saved dict:
  - `"has_prior": True`
  - `"prior_state_dict": prior_model.state_dict()`
  - `"prior_config": prior_config` (dict with hidden_size, num_layers, num_heads, seq_len, num_quantizers, dropout)
  - `"prior_metadata": prior_metadata` (dict with epochs_trained, final_perplexity, best_perplexity, training_date)
- Uses atomic write pattern: write to a temp file in the same directory (`model_path.with_suffix('.tmp')`), then `os.replace(temp_path, model_path)` -- per RESEARCH.md pitfall 6
- Returns the model_path
- Log message: "Bundled prior into {model_path}"

**3. Update load_model_v2:**
After loading VQ-VAE weights (existing code), add prior loading:
- Check `saved.get("has_prior", False)`
- If True:
  - Import CodePrior from distill.models.prior (lazy import inside function)
  - Extract `prior_config = saved["prior_config"]`
  - Extract `codebook_size` from `vq_cfg["codebook_size"]`
  - Create `prior = CodePrior(codebook_size=codebook_size, seq_len=prior_config["seq_len"], num_quantizers=prior_config["num_quantizers"], hidden_size=prior_config["hidden_size"], num_layers=prior_config["num_layers"], num_heads=prior_config["num_heads"], dropout=prior_config.get("dropout", 0.1))`
  - Load state dict: `prior.load_state_dict(saved["prior_state_dict"])`
  - Move to device, set eval mode
  - Set `prior` in the returned LoadedVQModel
- If False: `prior = None`
- Pass `prior=prior` to LoadedVQModel constructor

Also add `prior_config` and `prior_metadata` fields to LoadedVQModel:
- `prior_config: dict | None = None`
- `prior_metadata: dict | None = None`
Populate these from the saved dict when loading.

Do NOT modify save_model_v2 (it stays as-is for initial VQ-VAE saves). The save_prior_to_model function updates the file after prior training.
  </action>
  <verify>
Run: `cd H:/dev/Distill-vqvae && python -c "from distill.models.persistence import save_prior_to_model, LoadedVQModel; print('save_prior_to_model importable'); import dataclasses; fields = [f.name for f in dataclasses.fields(LoadedVQModel)]; assert 'prior' in fields, f'prior not in fields: {fields}'; assert 'prior_config' in fields; assert 'prior_metadata' in fields; print(f'LoadedVQModel fields: {fields}'); print('All persistence checks passed')"`
  </verify>
  <done>save_prior_to_model atomically updates .sda files with prior state. load_model_v2 detects has_prior and reconstructs CodePrior. LoadedVQModel has prior, prior_config, and prior_metadata fields.</done>
</task>

<task type="auto">
  <name>Task 2: Create CLI train-prior command with Rich progress display</name>
  <files>src/distill/cli/train_prior.py, src/distill/cli/__init__.py</files>
  <action>
**Create `src/distill/cli/train_prior.py`:**

Follow the exact structure and patterns of `cli/train.py`. Create a Typer app with:

**Command signature: `train_prior_cmd(model_path, dataset_dir, epochs, hidden_size, layers, heads, ...)`**

Arguments:
- `model_path: Path` -- required, path to a trained .sda VQ-VAE model file (exists=True, file_okay=True, dir_okay=False)
- `dataset_dir: Path` -- required, path to audio dataset directory (exists=True, dir_okay=True)

Options (matching user decision "CLI flags mirror UI knobs"):
- `--epochs / -e`: Override max_epochs (Optional[int])
- `--hidden-size`: Override hidden_size (Optional[int])
- `--layers`: Override num_layers (Optional[int])
- `--heads`: Override num_heads (Optional[int])
- `--lr`: Override learning_rate (Optional[float])
- `--device`: Compute device (default "auto")
- `--json`: JSON output mode (bool, default False)

**Implementation steps:**
1. Bootstrap config + device (use `from distill.cli import bootstrap` pattern)
2. Count audio files to determine adaptive config: `file_count = len(collect_audio_files(dataset_dir))`
3. Get adaptive config: `prior_config = get_adaptive_prior_config(file_count)`
4. Apply CLI overrides: if epochs is not None, set prior_config.max_epochs = epochs, etc.
5. Display config summary with Rich: model path, dataset, file count, epochs, hidden_size, layers, heads, learning rate. Show "(auto)" or "(override)" suffix on each param (same pattern as train.py)
6. Set up cancel event for SIGINT graceful shutdown (same pattern as train.py)
7. Define callback for PriorEpochMetrics:
   - Display epoch progress: `Epoch {n}/{total} | train_loss: {:.4f} | val_loss: {:.4f} | perplexity: {:.1f} | best: {:.1f}`
   - If is_memorizing: display warning in yellow/red with the memorization_message
   - Accumulate memorization warnings for end-of-training summary
8. Call `train_prior(model_path, dataset_dir, prior_config, callback, cancel_event)`
9. After training returns, call `save_prior_to_model(model_path, result["prior_model"], result["prior_config"], result["prior_metadata"])`
10. Display end-of-training summary:
    - Final loss, validation perplexity, best perplexity
    - Epochs trained
    - Prior config summary
    - Model path (same .sda file, now with prior)
    - Accumulated memorization warnings (if any)
11. If `--json`: output JSON with all training results

**Update `src/distill/cli/__init__.py`:**
- Import the train_prior app and register it as a subcommand `train-prior` on the main CLI app
- Follow the same pattern used for `train` command registration

Use `from __future__ import annotations`, lazy imports for torch and training modules inside command body (fast `--help` response), Rich console for stderr output.
  </action>
  <verify>
Run: `cd H:/dev/Distill-vqvae && python -c "from distill.cli.train_prior import app; print('train_prior CLI app importable')" && python -m distill.cli --help 2>&1 | head -20`
  </verify>
  <done>CLI `distill train-prior MODEL_PATH DATASET_DIR` command exists with --epochs, --hidden-size, --layers, --heads flags. Shows per-epoch perplexity and memorization warnings. Saves prior into the model file after training.</done>
</task>

<task type="auto">
  <name>Task 3: Update __init__.py exports for prior model, training, and persistence</name>
  <files>src/distill/models/__init__.py, src/distill/training/__init__.py</files>
  <action>
**Update `src/distill/models/__init__.py`:**
Add a new `# v1.1 Prior exports` section after the existing VQ-VAE section:

```python
# v1.1 Prior exports -- added in Phase 14
from distill.models.prior import CodePrior, extract_code_sequences, flatten_codes, unflatten_codes
from distill.models.persistence import save_prior_to_model
```

Add to `__all__`:
```python
# prior.py (v1.1)
"CodePrior",
"flatten_codes",
"unflatten_codes",
"extract_code_sequences",
# persistence.py (v1.1 prior)
"save_prior_to_model",
```

**Update `src/distill/training/__init__.py`:**
Add a new `# v1.1 Prior training exports -- added in Phase 14` section:

```python
from distill.training.prior_config import PriorConfig, get_adaptive_prior_config
from distill.training.prior_loop import train_prior, train_prior_epoch, validate_prior_epoch, check_memorization
from distill.training.metrics import PriorStepMetrics, PriorEpochMetrics, PriorTrainingCompleteEvent
```

Add to `__all__`:
```python
# prior_config.py (v1.1)
"PriorConfig",
"get_adaptive_prior_config",
# prior_loop.py (v1.1)
"train_prior",
"train_prior_epoch",
"validate_prior_epoch",
"check_memorization",
# metrics.py (v1.1 prior)
"PriorStepMetrics",
"PriorEpochMetrics",
"PriorTrainingCompleteEvent",
```

Preserve all existing exports. Follow the section comment pattern established in Phase 12/13.
  </action>
  <verify>
Run: `cd H:/dev/Distill-vqvae && python -c "from distill.models import CodePrior, flatten_codes, unflatten_codes, extract_code_sequences, save_prior_to_model; print('All model exports verified')" && python -c "from distill.training import PriorConfig, get_adaptive_prior_config, train_prior, check_memorization, PriorEpochMetrics, PriorStepMetrics, PriorTrainingCompleteEvent; print('All training exports verified')"`
  </verify>
  <done>All Phase 14 symbols are exported from distill.models and distill.training public APIs. Import paths match project conventions.</done>
</task>

</tasks>

<verification>
1. `python -c "from distill.models import CodePrior, save_prior_to_model"` -- model exports work
2. `python -c "from distill.training import train_prior, PriorConfig, PriorEpochMetrics"` -- training exports work
3. `python -c "from distill.models.persistence import LoadedVQModel; import dataclasses; assert 'prior' in [f.name for f in dataclasses.fields(LoadedVQModel)]"` -- LoadedVQModel has prior field
4. `python -m distill.cli --help` -- shows `train-prior` subcommand
5. `python -c "from distill.models.persistence import save_prior_to_model"` -- persistence function importable
</verification>

<success_criteria>
- save_prior_to_model atomically writes prior state into existing .sda files
- load_model_v2 reconstructs CodePrior when has_prior=True and populates LoadedVQModel.prior
- CLI train-prior command accepts model path and dataset dir with all override flags
- CLI shows per-epoch perplexity progress and memorization warnings
- CLI saves prior into the model file after training completes
- All new symbols exported through distill.models and distill.training public APIs
</success_criteria>

<output>
After completion, create `.planning/phases/14-autoregressive-prior/14-03-SUMMARY.md`
</output>
