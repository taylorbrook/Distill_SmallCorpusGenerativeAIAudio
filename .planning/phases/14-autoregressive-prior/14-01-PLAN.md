---
phase: 14-autoregressive-prior
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/distill/models/prior.py
  - src/distill/training/prior_config.py
autonomous: true
requirements:
  - GEN-01

must_haves:
  truths:
    - "CodePrior model accepts [B, T] integer code indices and produces [B, T, codebook_size] logits"
    - "flatten_codes converts [B, seq_len, num_quantizers] to [B, seq_len * num_quantizers] in position-major order"
    - "PriorConfig dataclass exposes hidden_size, num_layers, num_heads, dropout, max_epochs, learning_rate"
    - "get_adaptive_prior_config(file_count) scales model size and regularization by dataset tier (<=20 / <=100 / >100)"
    - "extract_code_sequences() encodes an entire dataset through a frozen VQ-VAE and returns all code indices as a tensor"
  artifacts:
    - path: "src/distill/models/prior.py"
      provides: "CodePrior transformer model, flatten_codes, unflatten_codes, extract_code_sequences"
      min_lines: 120
    - path: "src/distill/training/prior_config.py"
      provides: "PriorConfig dataclass, get_adaptive_prior_config"
      min_lines: 60
  key_links:
    - from: "src/distill/models/prior.py"
      to: "torch.nn.TransformerEncoder"
      via: "causal self-attention stack with explicit mask"
      pattern: "nn\\.TransformerEncoder"
    - from: "src/distill/models/prior.py"
      to: "src/distill/models/vqvae.py"
      via: "extract_code_sequences uses ConvVQVAE forward pass"
      pattern: "model\\(mel\\)"
---

<objective>
Build the CodePrior autoregressive transformer model and its adaptive configuration, plus the code extraction pipeline that encodes a dataset through a frozen VQ-VAE.

Purpose: Provides the foundational model architecture and data preparation for prior training. The CodePrior is a GPT-style decoder-only transformer that predicts the next discrete code token in a flattened VQ-VAE code sequence. The code extraction pipeline converts audio datasets into code sequences for training.

Output: `src/distill/models/prior.py` (CodePrior model + utilities) and `src/distill/training/prior_config.py` (PriorConfig + adaptive sizing)
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-autoregressive-prior/14-RESEARCH.md
@src/distill/models/vqvae.py
@src/distill/training/config.py
@src/distill/training/dataset.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CodePrior transformer model with code flattening utilities</name>
  <files>src/distill/models/prior.py</files>
  <action>
Create `src/distill/models/prior.py` with the following components:

**flatten_codes(indices: torch.Tensor) -> torch.Tensor:**
- Input: `[B, seq_len, num_quantizers]` integer indices from VQ-VAE
- Output: `[B, seq_len * num_quantizers]` flattened in position-major order (interleaved: pos0_q0, pos0_q1, pos0_q2, pos1_q0, ...)
- Simply `indices.reshape(B, S * Q)` since the input is already `[B, S, Q]`

**unflatten_codes(flat: torch.Tensor, num_quantizers: int) -> torch.Tensor:**
- Input: `[B, seq_len * num_quantizers]` flat sequence
- Output: `[B, seq_len, num_quantizers]`
- `flat.reshape(B, L // num_quantizers, num_quantizers)`

**CodePrior(nn.Module):**
- Constructor args: `codebook_size`, `seq_len`, `num_quantizers`, `hidden_size=256`, `num_layers=4`, `num_heads=4`, `dropout=0.1`
- Three embedding layers summed: `token_emb = nn.Embedding(codebook_size, hidden_size)`, `pos_emb = nn.Embedding(seq_len, hidden_size)`, `level_emb = nn.Embedding(num_quantizers, hidden_size)` -- per RESEARCH.md pitfall 3, level embedding disambiguates quantizer levels
- `nn.Dropout(dropout)` after embedding sum
- Use `nn.TransformerEncoder` (NOT TransformerDecoder) with `nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dim_feedforward=hidden_size*4, dropout=dropout, batch_first=True, norm_first=True)` and `num_layers` layers -- per RESEARCH.md pitfall 4, TransformerEncoder + causal mask is the correct GPT-style pattern
- Register buffer `causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1)` -- use explicit mask tensor, NOT `is_causal=True` flag per RESEARCH.md open question 1
- `ln_f = nn.LayerNorm(hidden_size)` final layer norm
- `head = nn.Linear(hidden_size, codebook_size, bias=False)` output projection
- Store `codebook_size`, `seq_len`, `num_quantizers` as attributes for external access

- `forward(x: torch.Tensor) -> torch.Tensor`:
  - x is `[B, T]` long tensor of code indices
  - Compute positions: `torch.arange(T, device=x.device).unsqueeze(0)`
  - Compute levels: `torch.arange(T, device=x.device) % self.num_quantizers` then unsqueeze(0) -- each token's level within its position
  - Sum: `h = token_emb(x) + pos_emb(positions) + level_emb(levels)`
  - Apply dropout
  - Slice causal mask: `mask = self.causal_mask[:T, :T]`
  - Pass through transformer: `h = self.transformer(h, mask=mask)`
  - Final layer norm + linear head
  - Return `[B, T, codebook_size]` logits

**extract_code_sequences(model, dataloader, spectrogram, device) -> torch.Tensor:**
- Takes a ConvVQVAE model (type annotation as string for lazy import), DataLoader, AudioSpectrogram, and device
- Sets model to eval, iterates dataloader with torch.no_grad()
- For each batch: `batch.to(device)` -> `mel = spectrogram.waveform_to_mel(batch)` -> `_recon, indices, _commit_loss = model(mel)` -> append `indices.cpu()`
- Return `torch.cat(all_indices, dim=0)` -- shape `[N, seq_len, num_quantizers]`

Follow project patterns: `from __future__ import annotations`, module-level logger, comprehensive docstrings. Import torch at module level (same as vqvae.py pattern -- models import torch directly).
  </action>
  <verify>
Run: `cd H:/dev/Distill-vqvae && python -c "from distill.models.prior import CodePrior, flatten_codes, unflatten_codes; import torch; p = CodePrior(256, 144, 3); x = torch.randint(0, 256, (2, 143)); logits = p(x); print(f'logits shape: {logits.shape}'); assert logits.shape == (2, 143, 256); idx = torch.randint(0, 256, (2, 48, 3)); flat = flatten_codes(idx); assert flat.shape == (2, 144); unflat = unflatten_codes(flat, 3); assert torch.equal(idx, unflat); print('All checks passed')"`
  </verify>
  <done>CodePrior accepts [B,T] code indices and returns [B,T,codebook_size] logits. flatten_codes/unflatten_codes round-trip correctly. extract_code_sequences function exists and is importable.</done>
</task>

<task type="auto">
  <name>Task 2: Create PriorConfig dataclass with dataset-adaptive scaling</name>
  <files>src/distill/training/prior_config.py</files>
  <action>
Create `src/distill/training/prior_config.py` with:

**PriorConfig dataclass:**
- `hidden_size: int = 256` -- transformer hidden dimension
- `num_layers: int = 4` -- transformer layers
- `num_heads: int = 4` -- attention heads
- `dropout: float = 0.1` -- dropout rate
- `max_epochs: int = 100` -- training epochs
- `learning_rate: float = 1e-3` -- AdamW learning rate
- `weight_decay: float = 0.01` -- AdamW weight decay
- `gradient_clip_norm: float = 1.0` -- gradient clipping
- `batch_size: int = 32` -- mini-batch for code sequences
- `val_fraction: float = 0.2` -- validation split fraction
- `device: str = "auto"` -- compute device

**get_adaptive_prior_config(file_count: int) -> PriorConfig:**
Per user decision: "Defaults auto-scale based on dataset size (smaller datasets get smaller priors to prevent overfitting)". Follow the same 3-tier pattern as get_adaptive_vqvae_config():

| Files   | hidden_size | num_layers | num_heads | dropout | max_epochs | learning_rate |
|---------|-------------|------------|-----------|---------|------------|---------------|
| <= 20   | 128         | 2          | 4         | 0.3     | 50         | 3e-4          |
| 21-100  | 256         | 4          | 4         | 0.2     | 100        | 1e-3          |
| > 100   | 512         | 6          | 8         | 0.1     | 150        | 1e-3          |

val_fraction follows same adaptive pattern as VQVAEConfig: 0.5 for <10, 0.3 for <50, 0.2 for <200, 0.1 for >=200.
batch_size: `min(32, max(1, file_count * 5 // 4))` (same heuristic as VQVAEConfig).

Follow project patterns: `from __future__ import annotations`, pure Python (no torch dependency), match VQVAEConfig style in training/config.py.
  </action>
  <verify>
Run: `cd H:/dev/Distill-vqvae && python -c "from distill.training.prior_config import PriorConfig, get_adaptive_prior_config; c1 = get_adaptive_prior_config(10); assert c1.hidden_size == 128 and c1.num_layers == 2 and c1.dropout == 0.3 and c1.max_epochs == 50; c2 = get_adaptive_prior_config(50); assert c2.hidden_size == 256 and c2.num_layers == 4; c3 = get_adaptive_prior_config(200); assert c3.hidden_size == 512 and c3.num_layers == 6 and c3.num_heads == 8; print('All adaptive config checks passed')"`
  </verify>
  <done>PriorConfig dataclass exists with all user-facing parameters. get_adaptive_prior_config returns correctly scaled configurations for all three dataset tiers.</done>
</task>

</tasks>

<verification>
1. `python -c "from distill.models.prior import CodePrior, flatten_codes, unflatten_codes, extract_code_sequences"` -- all symbols importable
2. `python -c "from distill.training.prior_config import PriorConfig, get_adaptive_prior_config"` -- config importable
3. CodePrior forward pass produces correct output shape for arbitrary sequence lengths up to seq_len
4. flatten_codes and unflatten_codes are exact inverses
5. PriorConfig defaults match the research-recommended values
6. Adaptive config scales correctly across all three tiers
</verification>

<success_criteria>
- CodePrior model runs a forward pass: `[B, T]` -> `[B, T, codebook_size]`
- Level embedding disambiguates quantizer positions within the flattened sequence
- Causal mask prevents attending to future tokens
- PriorConfig exposes all user-facing parameters (epochs, hidden_size, num_layers, num_heads)
- Adaptive config follows the 3-tier dataset sizing pattern established by VQVAEConfig
- Code extraction function signature matches the frozen-VQ-VAE encode pattern
</success_criteria>

<output>
After completion, create `.planning/phases/14-autoregressive-prior/14-01-SUMMARY.md`
</output>
