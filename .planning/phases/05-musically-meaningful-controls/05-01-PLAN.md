---
phase: 05-musically-meaningful-controls
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/small_dataset_audio/controls/__init__.py
  - src/small_dataset_audio/controls/features.py
  - src/small_dataset_audio/controls/analyzer.py
autonomous: true

must_haves:
  truths:
    - "PCA decomposition of encoded training data produces orthogonal components ranked by variance"
    - "Each PCA component is correlated with computed audio features to suggest a human-readable label"
    - "Safe and warning ranges are computed from the distribution of encoded training data"
    - "Analysis degrades gracefully when only 1-2 meaningful PCA components exist"
  artifacts:
    - path: "src/small_dataset_audio/controls/features.py"
      provides: "Audio feature extraction (spectral centroid, RMS, ZCR, rolloff, flatness)"
      exports: ["compute_audio_features", "FEATURE_NAMES"]
    - path: "src/small_dataset_audio/controls/analyzer.py"
      provides: "LatentSpaceAnalyzer with PCA fitting, feature correlation, safe range computation"
      exports: ["LatentSpaceAnalyzer", "AnalysisResult"]
    - path: "src/small_dataset_audio/controls/__init__.py"
      provides: "Public API re-exports for controls module"
      contains: "AnalysisResult"
  key_links:
    - from: "src/small_dataset_audio/controls/analyzer.py"
      to: "src/small_dataset_audio/models/vae.py"
      via: "model.encode() to collect mu vectors from training data"
      pattern: "model\\.encode"
    - from: "src/small_dataset_audio/controls/analyzer.py"
      to: "src/small_dataset_audio/controls/features.py"
      via: "compute_audio_features for PCA-component-to-feature correlation"
      pattern: "compute_audio_features"
    - from: "src/small_dataset_audio/controls/analyzer.py"
      to: "sklearn.decomposition.PCA"
      via: "PCA fitting on collected mu vectors"
      pattern: "PCA"
---

<objective>
Build the latent space analysis engine: audio feature extraction and LatentSpaceAnalyzer that runs PCA on encoded training data, correlates PCA components with audio features, and computes safe/warning ranges for slider controls.

Purpose: This is the core innovation of the project -- mapping opaque latent dimensions to musically meaningful parameters via PCA discovery and audio feature correlation, enabling users to control generation with intuitive sliders instead of raw latent vectors.

Output: `controls/features.py` (audio feature computation), `controls/analyzer.py` (LatentSpaceAnalyzer + AnalysisResult), `controls/__init__.py` (public API)
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-musically-meaningful-controls/05-CONTEXT.md
@.planning/phases/05-musically-meaningful-controls/05-RESEARCH.md
@src/small_dataset_audio/models/vae.py
@src/small_dataset_audio/audio/spectrogram.py
@src/small_dataset_audio/inference/chunking.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add scikit-learn dependency and create audio feature extraction module</name>
  <files>
    pyproject.toml
    src/small_dataset_audio/controls/__init__.py
    src/small_dataset_audio/controls/features.py
  </files>
  <action>
1. Add `"scikit-learn>=1.8"` to the `dependencies` list in `pyproject.toml`. Run `uv sync` to install.

2. Create `src/small_dataset_audio/controls/__init__.py` as a stub with a docstring and empty `__all__` (will be populated in Plan 02 after all symbols exist).

3. Create `src/small_dataset_audio/controls/features.py` with:

- Module-level constant `FEATURE_NAMES = ["spectral_centroid", "rms_energy", "zero_crossing_rate", "spectral_rolloff", "spectral_flatness"]` -- these are the 5 features from research. Use acoustic terms per user decision.

- Function `compute_audio_features(waveform: np.ndarray, sample_rate: int = 48_000) -> dict[str, float]` that computes all 5 features from a 1-D float32 numpy waveform:
  - **spectral_centroid**: Use numpy FFT. Compute magnitude spectrum via `np.abs(np.fft.rfft(waveform))`, frequency bins via `np.fft.rfftfreq(len(waveform), 1.0/sample_rate)`, then weighted mean `sum(freqs * magnitudes) / sum(magnitudes)`. Handle zero-energy case (return 0.0).
  - **rms_energy**: `np.sqrt(np.mean(waveform**2))`
  - **zero_crossing_rate**: `np.mean(np.abs(np.diff(np.signbit(waveform).astype(int))))`
  - **spectral_rolloff**: Frequency below which 85% of spectral energy is concentrated. Compute cumulative sum of magnitude spectrum, find index where it exceeds 0.85 * total, return corresponding frequency.
  - **spectral_flatness**: Geometric mean / arithmetic mean of power spectrum. Use `scipy.stats.gmean` on power spectrum (magnitude**2). Handle zero-energy (return 0.0). Add epsilon (1e-10) to avoid log(0).

- Use numpy and scipy ONLY (no librosa -- per research recommendation to avoid numba dependency). Lazy import numpy and scipy inside function body (project pattern).

- Returns dict keyed by feature name string, values are floats. All features are scalar summaries of the entire waveform (not frame-level).

- Function `compute_features_batch(waveforms: list[np.ndarray], sample_rate: int = 48_000) -> list[dict[str, float]]` that applies `compute_audio_features` to each waveform in the list with per-item try/except (project error isolation pattern). Failed items return dict of zeros.
  </action>
  <verify>
Run `uv run python -c "from small_dataset_audio.controls.features import compute_audio_features, FEATURE_NAMES; import numpy as np; w = np.random.randn(48000).astype(np.float32); f = compute_audio_features(w); print(f); assert set(f.keys()) == set(FEATURE_NAMES); assert all(isinstance(v, float) for v in f.values())"` -- should print 5 feature values without error.

Run `uv run python -c "from sklearn.decomposition import PCA; print('scikit-learn OK')"` -- confirms dependency installed.
  </verify>
  <done>
`compute_audio_features` returns a dict of 5 named float features for any 1-D waveform. scikit-learn is importable. `controls/__init__.py` exists as stub.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create LatentSpaceAnalyzer with PCA fitting, feature correlation, and safe range computation</name>
  <files>
    src/small_dataset_audio/controls/analyzer.py
    src/small_dataset_audio/controls/__init__.py
  </files>
  <action>
Create `src/small_dataset_audio/controls/analyzer.py` with:

**AnalysisResult dataclass** (frozen=True for immutability):
- `pca_components: np.ndarray` -- shape `[n_components, latent_dim]`, the PCA directions
- `pca_mean: np.ndarray` -- shape `[latent_dim]`, mean of encoded data (latent space center)
- `explained_variance_ratio: np.ndarray` -- shape `[n_components]`, fraction of variance per component
- `n_active_components: int` -- number of components above variance threshold
- `component_labels: list[str]` -- current labels (start as "Axis 1", "Axis 2", ...)
- `suggested_labels: list[str]` -- auto-suggested labels from feature correlation (e.g., "spectral_centroid" or "Axis 3" if no significant correlation)
- `safe_min: np.ndarray` -- shape `[n_components]`, 2nd percentile of projected training data
- `safe_max: np.ndarray` -- shape `[n_components]`, 98th percentile
- `warning_min: np.ndarray` -- shape `[n_components]`, 0.5th percentile (soft warning zone boundary)
- `warning_max: np.ndarray` -- shape `[n_components]`, 99.5th percentile
- `step_size: np.ndarray` -- shape `[n_components]`, safe range divided by (n_steps - 1)
- `n_steps: int` -- number of discrete slider positions (default 21, giving -10 to +10 integer range)
- `feature_correlations: dict[str, list[float]]` -- maps feature name to list of Pearson correlations (one per component)
- `latent_dim: int` -- original latent space dimensionality (64 for this model)

Use `@dataclass(frozen=True)` but make component_labels a tuple (frozen dataclass can't have mutable default). Actually, use regular `@dataclass` since labels need to be mutable (user can rename). Just document that pca_components and pca_mean should not be modified.

**LatentSpaceAnalyzer class:**

`__init__(self, variance_threshold: float = 0.02, n_steps: int = 21, n_sweep_points: int = 20)`:
- Store config parameters. variance_threshold = 2% per research discretion recommendation. n_steps = 21 (gives integer range -10 to +10). n_sweep_points = 20 for feature correlation sweep.

`analyze(self, model: ConvVAE, dataloader: DataLoader, spectrogram: AudioSpectrogram, device: torch.device, n_random_samples: int = 200) -> AnalysisResult`:
- This is the main entry point. User triggers this explicitly (locked decision: user-triggered analysis).

- **Step 1: Collect mu vectors.** Iterate through dataloader in eval mode with torch.no_grad(). For each batch, run `model.encode(mel_batch)` to get `(mu, logvar)`. Collect all `mu` vectors on CPU as numpy. The mel conversion happens in the training loop already, so the dataloader returns raw waveforms -- convert via `spectrogram.waveform_to_mel()` on the device. Use batch_size from the dataloader (should be reasonable, 32 or so).

- Also generate `n_random_samples` random prior samples: `torch.randn(n_random_samples, model.latent_dim)` and add to the collection. This combines training data encodings + random prior samples per locked decision.

- Stack all mu vectors into a single numpy array `[N, latent_dim]`.

- **Step 2: Fit PCA.** Use `sklearn.decomposition.PCA(n_components=None)` to fit on the collected mu vectors. This gives ALL components (up to latent_dim=64). Determine `n_active_components` by counting how many `explained_variance_ratio_` entries exceed `self.variance_threshold`. Minimum 1 component (graceful degradation per locked decision). If fewer than 3 components are active, log a warning suggesting more training data.

- **Step 3: Compute safe ranges.** Project all mu vectors onto the PCA components: `projected = pca.transform(mu_vectors)`. For each active component, compute:
  - `safe_min[i]` = np.percentile(projected[:, i], 2)
  - `safe_max[i]` = np.percentile(projected[:, i], 98)
  - `warning_min[i]` = np.percentile(projected[:, i], 0.5)
  - `warning_max[i]` = np.percentile(projected[:, i], 99.5)
  - `step_size[i]` = (safe_max[i] - safe_min[i]) / (self.n_steps - 1)

- **Step 4: Feature correlation.** For each active PCA component, sweep `n_sweep_points` linearly from `safe_min[i]` to `safe_max[i]`. At each point, construct a latent vector: `z = pca_mean + value * pca_component_i` (all other components at 0). Decode z to mel via `model.decode()`, convert mel to waveform via `spectrogram.mel_to_waveform()`, compute audio features via `compute_audio_features()`. Collect feature values across sweep points. For each feature, compute Pearson correlation between sweep position values and feature values using `scipy.stats.pearsonr`. Store correlation coefficients and p-values. Only suggest a label if the HIGHEST absolute correlation has p < 0.05.

  - Ensure model is in eval mode during sweep. Ensure decoder is initialized (check model.decoder.fc, init if needed using the existing pattern from chunking.py).
  - Use the existing mel shape from spectrogram: `spectrogram.get_mel_shape(spectrogram.config.sample_rate)` for 1 second at 48kHz.

- **Step 5: Build labels.** Default labels: `["Axis 1", "Axis 2", ...]`. Suggested labels: for each component, if a feature has |correlation| > 0.5 AND p < 0.05, suggest that feature name. If multiple features meet the threshold, pick the one with highest |correlation|. Otherwise, suggested label = default label (same as "Axis N"). Per locked decision: acoustic terms for labels.

- **Step 6: Construct and return AnalysisResult** with all computed data. Only include `n_active_components` worth of data in the arrays (trim to n_active_components rows/elements).

**Important implementation details:**
- Lazy imports for torch, numpy, scipy, sklearn inside methods (project pattern).
- Use `@torch.no_grad()` context for all model inference.
- model eval/train mode management: save was_training, set eval, restore after (project pattern from chunking.py).
- Log progress: "Collecting encodings...", "Fitting PCA ({n} components explain {pct}% variance)...", "Computing feature correlations...", "Analysis complete: {n} active components".
- Use `logging.getLogger(__name__)` (project pattern).

Update `controls/__init__.py` to export `LatentSpaceAnalyzer`, `AnalysisResult`, `compute_audio_features`, `FEATURE_NAMES` with proper `__all__`.
  </action>
  <verify>
Run `uv run python -c "
from small_dataset_audio.controls import LatentSpaceAnalyzer, AnalysisResult
import inspect
# Check AnalysisResult has required fields
fields = [f.name for f in __import__('dataclasses').fields(AnalysisResult)]
required = ['pca_components', 'pca_mean', 'explained_variance_ratio', 'n_active_components', 'component_labels', 'suggested_labels', 'safe_min', 'safe_max', 'warning_min', 'warning_max', 'step_size', 'n_steps', 'feature_correlations', 'latent_dim']
missing = [r for r in required if r not in fields]
assert not missing, f'Missing fields: {missing}'
print('AnalysisResult fields OK:', len(fields))
# Check LatentSpaceAnalyzer has analyze method
assert hasattr(LatentSpaceAnalyzer, 'analyze')
sig = inspect.signature(LatentSpaceAnalyzer.analyze)
assert 'model' in sig.parameters
assert 'dataloader' in sig.parameters
print('LatentSpaceAnalyzer.analyze signature OK')
print('All checks passed')
"` -- should print field count and "All checks passed".
  </verify>
  <done>
`LatentSpaceAnalyzer.analyze()` accepts a trained model, dataloader, and spectrogram, runs PCA on encoded training data + random samples, correlates components with 5 audio features, computes safe/warning ranges, and returns a complete `AnalysisResult` with adaptive component count and suggested labels. Controls module exports all public symbols.
  </done>
</task>

</tasks>

<verification>
1. `from small_dataset_audio.controls import LatentSpaceAnalyzer, AnalysisResult, compute_audio_features, FEATURE_NAMES` imports without error
2. `compute_audio_features(np.random.randn(48000).astype(np.float32))` returns dict with 5 float values
3. `AnalysisResult` dataclass has all 14 required fields
4. `LatentSpaceAnalyzer(variance_threshold=0.02).analyze(model, dataloader, spectrogram, device)` has correct signature
5. scikit-learn is importable: `from sklearn.decomposition import PCA`
</verification>

<success_criteria>
- Audio features module computes 5 acoustic features from raw waveforms using numpy/scipy only (no librosa)
- LatentSpaceAnalyzer performs full PCA pipeline: encode -> PCA -> correlate -> ranges -> labels
- Adaptive component count based on 2% variance threshold with graceful degradation
- Feature correlation uses Pearson r with p < 0.05 significance threshold for label suggestions
- Safe ranges from 2nd-98th percentiles, warning zones from 0.5th-99.5th percentiles
- 21 discrete slider steps (integer range -10 to +10)
- All analysis runs on user trigger, not automatically
</success_criteria>

<output>
After completion, create `.planning/phases/05-musically-meaningful-controls/05-01-SUMMARY.md`
</output>
