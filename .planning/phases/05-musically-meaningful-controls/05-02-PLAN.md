---
phase: 05-musically-meaningful-controls
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/small_dataset_audio/controls/mapping.py
  - src/small_dataset_audio/controls/serialization.py
  - src/small_dataset_audio/controls/__init__.py
  - src/small_dataset_audio/inference/generation.py
  - src/small_dataset_audio/inference/__init__.py
autonomous: true

must_haves:
  truths:
    - "User can convert discrete slider positions to a latent vector for generation"
    - "User can randomize all sliders to random positions within safe bounds"
    - "User can reset all sliders to center (latent space mean)"
    - "Analysis results persist in checkpoints and restore instantly on model load"
    - "GenerationPipeline accepts a latent vector from slider mapping instead of random sampling"
    - "User can set a random seed for reproducible generation (GEN-09)"
  artifacts:
    - path: "src/small_dataset_audio/controls/mapping.py"
      provides: "Slider-to-latent vector conversion, randomize all, reset to center"
      exports: ["sliders_to_latent", "randomize_sliders", "center_sliders", "SliderState"]
    - path: "src/small_dataset_audio/controls/serialization.py"
      provides: "Save/load AnalysisResult to/from checkpoint dict"
      exports: ["analysis_to_dict", "analysis_from_dict"]
    - path: "src/small_dataset_audio/inference/generation.py"
      provides: "GenerationPipeline.generate() accepting optional latent_vector parameter"
      contains: "latent_vector"
  key_links:
    - from: "src/small_dataset_audio/controls/mapping.py"
      to: "src/small_dataset_audio/controls/analyzer.py"
      via: "Uses AnalysisResult.pca_components and pca_mean for vector reconstruction"
      pattern: "AnalysisResult"
    - from: "src/small_dataset_audio/controls/serialization.py"
      to: "src/small_dataset_audio/controls/analyzer.py"
      via: "Serializes/deserializes AnalysisResult fields"
      pattern: "AnalysisResult"
    - from: "src/small_dataset_audio/inference/generation.py"
      to: "src/small_dataset_audio/controls/mapping.py"
      via: "GenerationPipeline uses slider-provided latent vector instead of random sampling"
      pattern: "latent_vector"
---

<objective>
Wire slider controls to the generation pipeline: convert slider positions to latent vectors, persist analysis results in checkpoints, and enable GenerationPipeline to accept user-controlled latent vectors.

Purpose: Completes the bridge between the PCA analysis (Plan 01) and actual audio generation -- users can now control what the model generates via stepped sliders mapped to discovered latent space dimensions.

Output: `controls/mapping.py` (slider-to-latent conversion), `controls/serialization.py` (checkpoint persistence), updated `inference/generation.py` (latent vector input)
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-musically-meaningful-controls/05-CONTEXT.md
@.planning/phases/05-musically-meaningful-controls/05-RESEARCH.md
@.planning/phases/05-musically-meaningful-controls/05-01-SUMMARY.md
@src/small_dataset_audio/controls/analyzer.py
@src/small_dataset_audio/controls/features.py
@src/small_dataset_audio/inference/generation.py
@src/small_dataset_audio/inference/chunking.py
@src/small_dataset_audio/training/checkpoint.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create slider-to-latent mapping and analysis serialization</name>
  <files>
    src/small_dataset_audio/controls/mapping.py
    src/small_dataset_audio/controls/serialization.py
  </files>
  <action>
**Create `src/small_dataset_audio/controls/mapping.py`:**

**SliderState dataclass:**
- `positions: list[int]` -- current integer step indices for each slider, range `[-(n_steps//2), n_steps//2]` (e.g., -10 to +10 for n_steps=21)
- `n_components: int` -- number of active PCA components (matches AnalysisResult.n_active_components)

**`sliders_to_latent(slider_state: SliderState, analysis: AnalysisResult) -> np.ndarray`:**
- Convert integer step positions to continuous values: `value_i = position_i * analysis.step_size[i]`
- Then reconstruct latent vector: `z = analysis.pca_mean + sum(value_i * analysis.pca_components[i] for i in range(n_components))`
- Returns numpy array of shape `[latent_dim]` (64-dimensional).
- Per locked decision: integer step indices are the ground truth, NOT floats. Continuous value is derived from step * step_size.
- Per locked decision: fully independent sliders -- each maps to exactly one PCA component, no coupling.

**`randomize_sliders(analysis: AnalysisResult, seed: int | None = None) -> SliderState`:**
- Set all sliders to random integer positions within safe bounds (between step indices corresponding to safe_min and safe_max).
- Per locked decision: "randomize all" sets all sliders to random positions within safe bounds. No per-slider randomize.
- Compute step range: for component i, min_step = ceil(safe_min[i] / step_size[i]), max_step = floor(safe_max[i] / step_size[i]). Clamp to [-(n_steps//2), n_steps//2].
- Use numpy random with optional seed for reproducibility.
- Returns SliderState with random positions.

**`center_sliders(analysis: AnalysisResult) -> SliderState`:**
- Per locked decision: "reset to center" returns all sliders to 0 (latent space mean).
- Returns SliderState with all positions = 0.

**`get_slider_info(analysis: AnalysisResult) -> list[dict]`:**
- Returns a list of dicts, one per active component, containing:
  - `index: int` -- component index
  - `label: str` -- current label from component_labels
  - `suggested_label: str` -- from suggested_labels
  - `min_step: int` -- -(n_steps//2)
  - `max_step: int` -- n_steps//2
  - `safe_min_step: int` -- step corresponding to safe_min
  - `safe_max_step: int` -- step corresponding to safe_max
  - `warning_min_step: int` -- step corresponding to warning_min
  - `warning_max_step: int` -- step corresponding to warning_max
  - `variance_explained_pct: float` -- explained_variance_ratio * 100 (per discretion: expose variance percentages)
- This provides all information the UI (Phase 8) needs to render sliders with warning zones and labels.
- Per discretion: include variance-explained percentages per slider.

**`is_in_warning_zone(position: int, slider_info: dict) -> bool`:**
- Returns True if position is between warning boundary and safe boundary (either end).
- UI can use this to show visual indicator per locked decision (soft warning zone).

**Create `src/small_dataset_audio/controls/serialization.py`:**

**`analysis_to_dict(analysis: AnalysisResult) -> dict`:**
- Convert AnalysisResult to a plain dict suitable for inclusion in a torch.save checkpoint.
- Per research: save numpy arrays directly (NOT sklearn PCA objects -- serialization version mismatch pitfall).
- Convert numpy arrays to lists for JSON-safe nested dicts, OR keep as numpy arrays for torch.save (torch.save handles numpy natively). Keep as numpy arrays since checkpoint uses torch.save.
- Keys match AnalysisResult field names exactly.
- Include a `version: int = 1` field for future compatibility.

**`analysis_from_dict(d: dict) -> AnalysisResult`:**
- Reconstruct AnalysisResult from the dict saved in a checkpoint.
- Convert any lists back to numpy arrays.
- Handle missing keys gracefully (for forward compatibility if new fields added).
- Raise ValueError if version is unsupported.

Both modules use lazy imports for numpy (project pattern). Both use `logging.getLogger(__name__)`.
  </action>
  <verify>
Run `uv run python -c "
from small_dataset_audio.controls.mapping import sliders_to_latent, randomize_sliders, center_sliders, get_slider_info, SliderState
from small_dataset_audio.controls.serialization import analysis_to_dict, analysis_from_dict
from small_dataset_audio.controls.analyzer import AnalysisResult
import numpy as np

# Create a minimal AnalysisResult for testing
n_comp, latent_dim = 3, 64
ar = AnalysisResult(
    pca_components=np.random.randn(n_comp, latent_dim).astype(np.float32),
    pca_mean=np.random.randn(latent_dim).astype(np.float32),
    explained_variance_ratio=np.array([0.3, 0.2, 0.1], dtype=np.float32),
    n_active_components=n_comp,
    component_labels=['Axis 1', 'Axis 2', 'Axis 3'],
    suggested_labels=['spectral_centroid', 'rms_energy', 'Axis 3'],
    safe_min=np.array([-2.0, -1.5, -1.0], dtype=np.float32),
    safe_max=np.array([2.0, 1.5, 1.0], dtype=np.float32),
    warning_min=np.array([-3.0, -2.5, -2.0], dtype=np.float32),
    warning_max=np.array([3.0, 2.5, 2.0], dtype=np.float32),
    step_size=np.array([0.2, 0.15, 0.1], dtype=np.float32),
    n_steps=21,
    feature_correlations={'spectral_centroid': [0.8, 0.1, 0.05]},
    latent_dim=latent_dim,
)

# Test center
s = center_sliders(ar)
assert all(p == 0 for p in s.positions), 'Center not zero'
z = sliders_to_latent(s, ar)
assert z.shape == (64,), f'Wrong shape: {z.shape}'
assert np.allclose(z, ar.pca_mean, atol=1e-5), 'Center should equal mean'

# Test randomize
sr = randomize_sliders(ar, seed=42)
assert len(sr.positions) == 3
zr = sliders_to_latent(sr, ar)
assert zr.shape == (64,)

# Test serialization roundtrip
d = analysis_to_dict(ar)
ar2 = analysis_from_dict(d)
assert ar2.n_active_components == 3
assert np.allclose(ar2.pca_mean, ar.pca_mean)

# Test slider info
info = get_slider_info(ar)
assert len(info) == 3
assert 'variance_explained_pct' in info[0]

print('All mapping + serialization tests passed')
"` -- should print "All mapping + serialization tests passed".
  </verify>
  <done>
`sliders_to_latent` converts integer slider positions to a 64-dim latent vector via PCA reconstruction. `randomize_sliders` and `center_sliders` produce SliderStates. `analysis_to_dict` / `analysis_from_dict` enable checkpoint persistence. `get_slider_info` provides all UI-ready metadata including warning zones and variance percentages.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate slider controls into GenerationPipeline and update public API</name>
  <files>
    src/small_dataset_audio/inference/generation.py
    src/small_dataset_audio/inference/__init__.py
    src/small_dataset_audio/controls/__init__.py
  </files>
  <action>
**Modify `src/small_dataset_audio/inference/generation.py`:**

1. Add `latent_vector: np.ndarray | None = None` field to `GenerationConfig` dataclass, with default `None`. When provided, this is the user-controlled latent vector from slider mapping (replaces random sampling). When `None`, behavior is unchanged (random latent vectors as before). Add docstring: "User-controlled latent vector from slider mapping. When set, all chunks use this vector (or interpolate around it). Overrides random sampling."

2. In `GenerationPipeline.generate()`, modify the chunk generation logic:
   - After seed setup (step 3), add a new step 3.5: if `config.latent_vector is not None`, convert it to a torch tensor on self.device.
   - For crossfade mode: instead of calling `generate_chunks_crossfade` (which samples random z vectors internally), generate chunks using the provided latent vector. Create a new helper function `_generate_chunks_from_vector(model, spectrogram, latent_vector, num_chunks, device, chunk_samples, overlap_samples)` that:
     - Uses the provided latent_vector as a base
     - For multi-chunk generation, adds small random perturbations (scaled by 0.1) to each chunk's latent vector using the seed for reproducibility, so longer audio has variation but stays in the same region of latent space
     - Decodes each chunk to mel, converts to waveform, crossfades together
     - Follows the same pattern as `generate_chunks_crossfade` in chunking.py (eval mode, decoder init check, etc.)
   - For latent_interpolation mode: similarly, use the provided vector as the center and create anchor points around it with small perturbations
   - When latent_vector is None, keep existing behavior unchanged (call existing chunk functions)

3. Add `latent_vector` to the sidecar JSON metadata in `GenerationPipeline.export()`: include `config.latent_vector.tolist()` if not None, else null. This goes into the config_dict via `asdict` which will handle the numpy array -- actually, `dataclasses.asdict` won't serialize numpy arrays to JSON cleanly. Instead, in the export method, after `config_dict = asdict(result.config)`, convert the latent_vector: `if config_dict.get('latent_vector') is not None: config_dict['latent_vector'] = config_dict['latent_vector'].tolist()`.

4. Update `GenerationConfig.validate()` to validate latent_vector: if provided, must be a numpy array of shape `(latent_dim,)` -- but we don't know latent_dim at config level. Just check it's a numpy array with 1 dimension. The pipeline will handle dimension mismatch.

**Put the helper function `_generate_chunks_from_vector` in `inference/generation.py`** (not chunking.py) since it's specific to slider-controlled generation. It follows the same pattern as the chunking functions:
- Lazy import torch, numpy
- Eval mode management
- Decoder init check
- mel shape from spectrogram.get_mel_shape(chunk_samples)
- Uses crossfade_chunks from chunking module for multi-chunk concatenation

**Update `src/small_dataset_audio/inference/__init__.py`:**
- No new exports needed (GenerationConfig and GenerationPipeline already exported).

**Update `src/small_dataset_audio/controls/__init__.py`:**
- Export all public symbols from all controls submodules:
  - From features: `compute_audio_features`, `compute_features_batch`, `FEATURE_NAMES`
  - From analyzer: `LatentSpaceAnalyzer`, `AnalysisResult`
  - From mapping: `sliders_to_latent`, `randomize_sliders`, `center_sliders`, `get_slider_info`, `is_in_warning_zone`, `SliderState`
  - From serialization: `analysis_to_dict`, `analysis_from_dict`
  - Set `__all__` with all exported names.
  </action>
  <verify>
Run `uv run python -c "
import numpy as np

# Test GenerationConfig with latent_vector
from small_dataset_audio.inference import GenerationConfig, GenerationPipeline
config = GenerationConfig(latent_vector=np.zeros(64, dtype=np.float32))
config.validate()  # should not raise
print('GenerationConfig with latent_vector: OK')

# Test GenerationConfig without latent_vector (backward compat)
config2 = GenerationConfig()
config2.validate()  # should not raise
assert config2.latent_vector is None
print('GenerationConfig backward compat: OK')

# Test controls public API completeness
from small_dataset_audio.controls import (
    LatentSpaceAnalyzer, AnalysisResult,
    compute_audio_features, compute_features_batch, FEATURE_NAMES,
    sliders_to_latent, randomize_sliders, center_sliders, get_slider_info, is_in_warning_zone, SliderState,
    analysis_to_dict, analysis_from_dict,
)
print('Controls public API: OK (all 12 symbols imported)')
print('All integration tests passed')
"` -- should print all OK messages.
  </verify>
  <done>
GenerationPipeline accepts an optional latent_vector in GenerationConfig, using it for slider-controlled generation instead of random sampling. Multi-chunk generation adds small perturbations around the provided vector for variation. Controls module has complete public API with all 12 symbols exported. Full backward compatibility maintained (latent_vector=None behaves identically to Phase 4).
  </done>
</task>

</tasks>

<verification>
1. Full slider-to-generation flow works conceptually: `center_sliders(analysis) -> sliders_to_latent(state, analysis) -> GenerationConfig(latent_vector=z) -> pipeline.generate(config)`
2. Serialization roundtrip: `analysis_from_dict(analysis_to_dict(result))` produces equivalent AnalysisResult
3. Backward compatibility: GenerationConfig() with no latent_vector behaves exactly as Phase 4
4. All 12 controls symbols importable from `small_dataset_audio.controls`
5. No deferred features implemented (no per-slider randomize, no per-slider reset, no coupled parameters)
</verification>

<success_criteria>
- Slider positions (integers) convert to 64-dim latent vectors via PCA component reconstruction
- Randomize sets all sliders to random safe positions; center sets all to 0 (mean)
- Analysis results serialize to/from checkpoint-compatible dicts with version field
- GenerationPipeline generates audio from slider-controlled latent vectors with small per-chunk perturbations
- Controls module exports complete public API (12 symbols)
- No regression: existing generation without latent_vector works unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/05-musically-meaningful-controls/05-02-SUMMARY.md`
</output>
