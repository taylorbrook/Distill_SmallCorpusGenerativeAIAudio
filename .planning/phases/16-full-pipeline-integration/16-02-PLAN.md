---
phase: 16-full-pipeline-integration
plan: 02
type: execute
wave: 2
depends_on: [16-01]
files_modified:
  - src/distill/training/preview.py
  - src/distill/training/loop.py
  - src/distill/controls/analyzer.py
autonomous: true
requirements: [INTEG-03, INTEG-04, INTEG-05]
must_haves:
  truths:
    - "Training preview audio files are generated via ISTFT during training"
    - "PCA-based latent space analysis produces feature correlations with 2-channel models"
    - "Slider labels combine technical and musical descriptors"
    - "IF coherence / spectral quality metrics appear in console log output during training"
  artifacts:
    - path: "src/distill/training/preview.py"
      provides: "ISTFT-based training preview generation"
      contains: "complex_mel_to_waveform"
    - path: "src/distill/controls/analyzer.py"
      provides: "PCA analysis with 2-channel ISTFT waveform generation for feature sweep"
      contains: "complex_mel_to_waveform"
    - path: "src/distill/training/loop.py"
      provides: "ComplexSpectrogram and norm_stats passed to preview generation"
      contains: "ComplexSpectrogram"
  key_links:
    - from: "src/distill/training/preview.py"
      to: "ComplexSpectrogram.complex_mel_to_waveform"
      via: "spectrogram.complex_mel_to_waveform(mel_recon.cpu(), stats=normalization_stats)"
      pattern: "complex_mel_to_waveform"
    - from: "src/distill/training/loop.py"
      to: "src/distill/training/preview.py"
      via: "generate_preview receives complex_spectrogram and norm_stats"
      pattern: "generate_preview"
    - from: "src/distill/controls/analyzer.py"
      to: "ComplexSpectrogram.complex_mel_to_waveform"
      via: "Feature sweep decodes to waveform via ISTFT for audio feature extraction"
      pattern: "complex_mel_to_waveform"
---

<objective>
Wire ISTFT reconstruction into training previews and PCA analysis so that training monitoring and latent space slider controls work with v2.0 2-channel models.

Purpose: Without training previews, users cannot monitor model quality during training. Without PCA analysis, slider controls have no feature correlation labels. Both are core v1.0 workflows that must work identically in v2.0.

Output: Training produces preview WAV files at configured intervals. PCA analysis produces feature-correlated slider labels using ISTFT waveforms.
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-full-pipeline-integration/16-01-SUMMARY.md
@.planning/phases/15-istft-reconstruction/15-01-SUMMARY.md
@.planning/phases/12-2-channel-data-pipeline/12-01-SUMMARY.md

<interfaces>
<!-- Key types and contracts from Plan 01 output and existing codebase. -->

From src/distill/audio/spectrogram.py (ComplexSpectrogram -- available after Plan 01 wiring):
```python
class ComplexSpectrogram:
    def complex_mel_to_waveform(
        self,
        spectrogram: "torch.Tensor",  # [B, 2, n_mels, time]
        stats: dict[str, float] | None = None,
        sample_rate: int = 48_000,
    ) -> "torch.Tensor":  # [B, 1, samples]
    def to(self, device) -> "ComplexSpectrogram": ...
```

From src/distill/training/preview.py (current signature -- to be updated):
```python
def generate_preview(model, spectrogram, output_dir, epoch, device, num_samples=1, sample_rate=48_000) -> list[Path]: ...
def generate_reconstruction_preview(model, spectrogram, sample_batch, output_dir, epoch, device, sample_rate=48_000) -> list[Path]: ...
```

From src/distill/controls/analyzer.py:
```python
class LatentSpaceAnalyzer:
    def analyze(self, model, dataloader, spectrogram, device, n_random_samples=200) -> AnalysisResult: ...
```

From src/distill/controls/features.py:
```python
FEATURE_NAMES: list[str]  # ['spectral_centroid', 'spectral_bandwidth', ...]
def compute_audio_features(audio: np.ndarray, sample_rate: int) -> dict[str, float]: ...
```

norm_stats dict format:
```python
{"mag_mean": float, "mag_std": float, "if_mean": float, "if_std": float}
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire ISTFT into training preview generation with metrics logging</name>
  <files>
    src/distill/training/preview.py
    src/distill/training/loop.py
  </files>
  <action>
**Goal:** Replace NotImplementedError stubs in preview.py with ComplexSpectrogram.complex_mel_to_waveform calls. Pass ComplexSpectrogram and norm_stats from the training loop. Add IF coherence / spectral quality metrics to console log output (per CONTEXT.md decision: "Add IF coherence / spectral quality metrics to console log output alongside loss values").

**preview.py -- Update generate_preview:**

1. Add parameters `complex_spectrogram=None` and `normalization_stats: dict | None = None` to `generate_preview` signature. Keep the existing `spectrogram` parameter for backward compat (it's still used in signature but the actual mel-to-waveform now uses complex_spectrogram).

2. Replace the TODO stub (lines 82-86) with:
   ```python
   # Convert 2-channel spectrogram to waveform via ISTFT
   waveforms = complex_spectrogram.complex_mel_to_waveform(
       mel_recon.cpu(), stats=normalization_stats,
   )
   ```
   The `raise NotImplementedError(...)` and the commented-out old code should be removed entirely.

3. The existing loop at line 89-99 (`for i in range(waveforms.shape[0])`) should work as-is, but update the audio extraction:
   - Current: `audio = waveforms[i, 0]  # [samples] -- mono`
   - This is correct since `complex_mel_to_waveform` returns `[B, 1, samples]`, so `waveforms[i, 0]` gives `[samples]`.

**preview.py -- Update generate_reconstruction_preview:**

4. Add same parameters `complex_spectrogram=None` and `normalization_stats: dict | None = None`.

5. Replace the TODO stub (lines 170-175) with:
   ```python
   # Convert 2-channel spectrograms to waveforms via ISTFT
   orig_waveforms = complex_spectrogram.complex_mel_to_waveform(
       sample_batch.cpu(), stats=normalization_stats,
   )
   recon_waveforms = complex_spectrogram.complex_mel_to_waveform(
       recon.cpu(), stats=normalization_stats,
   )
   ```

6. Update the docstring `sample_batch` type from `[B, 1, n_mels, time]` to `[B, 2, n_mels, time]`.

**loop.py -- Pass ComplexSpectrogram and norm_stats to preview generation:**

7. In the `train()` function, after the preprocessing step (line ~512 where `cache_dir, norm_stats = preprocess_complex_spectrograms(...)`) create a ComplexSpectrogram instance:
   ```python
   from distill.audio.spectrogram import ComplexSpectrogram
   from distill.training.config import ComplexSpectrogramConfig
   complex_spec = ComplexSpectrogram(config.complex_spectrogram)
   complex_spec.to(device)
   ```
   Place this right after the model and AudioSpectrogram setup (around line 441), but it needs `config.complex_spectrogram` which is the `ComplexSpectrogramConfig`. This is already available via the `config` parameter.

8. In the preview generation call (line ~692), pass the new parameters:
   ```python
   preview_paths = generate_preview(
       model=model,
       spectrogram=spectrogram,
       output_dir=preview_dir,
       epoch=epoch,
       device=device,
       complex_spectrogram=complex_spec,
       normalization_stats=norm_stats,
   )
   ```

9. Remove the `exc_info=True` warning message about "Phase 16 will wire ISTFT-based preview" (line ~707-711) since we're actually wiring it now. Replace with a simpler warning:
   ```python
   except Exception:
       logger.warning(
           "Preview generation failed for epoch %d", epoch, exc_info=True,
       )
   ```

**loop.py -- Add IF coherence / spectral quality metrics to training log:**

10. Per CONTEXT.md decision: "Add IF coherence / spectral quality metrics to console log output alongside loss values. Audio-only preview files (no metrics files saved alongside)."

    After the per-epoch metrics logging (around the print statement that shows loss values), add spectral quality metrics. The simplest approach: compute metrics from the reconstructed batch in `train_epoch` or `validate_epoch`, or compute them during preview generation and log them.

    **Best approach:** During preview generation in the training loop, after generating preview audio successfully, compute basic spectral quality metrics on the generated waveform and log them:
    ```python
    # Log IF coherence / spectral quality metrics
    if preview_paths:
        try:
            import numpy as np
            import soundfile as sf
            wav_data, sr = sf.read(str(preview_paths[0]))
            # Compute spectral quality metrics
            from distill.controls.features import compute_audio_features
            features = compute_audio_features(wav_data.astype(np.float32), sr)
            logger.info(
                "Preview metrics (epoch %d): spectral_centroid=%.1f, "
                "spectral_bandwidth=%.1f, rms_energy=%.4f",
                epoch,
                features.get("spectral_centroid", 0),
                features.get("spectral_bandwidth", 0),
                features.get("rms_energy", 0),
            )
        except Exception:
            pass  # Metrics are nice-to-have, don't fail training
    ```
  </action>
  <verify>
    <automated>cd H:/dev/Distill-complex-spec && python -c "
import inspect
from distill.training.preview import generate_preview, generate_reconstruction_preview
sig_gp = inspect.signature(generate_preview)
sig_rp = inspect.signature(generate_reconstruction_preview)
print('generate_preview has complex_spectrogram:', 'complex_spectrogram' in sig_gp.parameters)
print('generate_preview has normalization_stats:', 'normalization_stats' in sig_gp.parameters)
print('generate_reconstruction_preview has complex_spectrogram:', 'complex_spectrogram' in sig_rp.parameters)
# Check no NotImplementedError remains
import distill.training.preview as mod
src = inspect.getsource(mod)
print('NotImplementedError in preview.py:', 'NotImplementedError' in src)
"</automated>
  </verify>
  <done>Training preview functions accept ComplexSpectrogram and normalization_stats, generate waveforms via ISTFT, and save WAV files. Training loop passes ComplexSpectrogram and norm_stats to preview generation. Spectral quality metrics logged to console during training after preview generation.</done>
</task>

<task type="auto">
  <name>Task 2: Wire PCA analysis feature sweep with ISTFT and update slider labels</name>
  <files>src/distill/controls/analyzer.py</files>
  <action>
**Goal:** Re-enable the feature correlation sweep in LatentSpaceAnalyzer.analyze() using ComplexSpectrogram for ISTFT waveform generation. Update slider labels to use technical + musical style (per CONTEXT.md: "Slider labels use technical + musical style: e.g. 'PC1 (Brightness)', 'Mag-Texture'").

**analyzer.py -- Update analyze() signature:**

1. Add `complex_spectrogram=None` and `normalization_stats: dict | None = None` parameters to the `analyze` method (after `device`).

2. The analyzer currently receives `spectrogram: AudioSpectrogram` (line 127). Keep this parameter for `get_mel_shape`. The new `complex_spectrogram` parameter will be used for ISTFT.

**analyzer.py -- Fix the dataloader to handle cached spectrograms:**

3. The current code (lines 171-187) does:
   ```python
   for batch in dataloader:
       waveforms = batch[0] if isinstance(batch, tuple) else batch
       mel = spectrogram.waveform_to_mel(waveforms)
       mu, logvar = model.encode(mel)
   ```
   But in v2.0, the dataloader yields cached 2-channel spectrograms `[B, 2, n_mels, time]`, NOT waveforms. The `waveform_to_mel` call will fail because the input is already a spectrogram. Fix this:
   ```python
   for batch in dataloader:
       if isinstance(batch, (list, tuple)):
           data = batch[0]
       else:
           data = batch
       data = data.to(device)

       # v2.0: dataloader yields cached 2-channel spectrograms [B, 2, n_mels, time]
       # Encode directly -- no waveform_to_mel needed
       mu, logvar = model.encode(data)
       all_mu.append(mu.cpu().numpy())
   ```
   Remove the `waveforms` variable name, `waveforms.dim()` check, `waveforms.unsqueeze(1)`, and `spectrogram.waveform_to_mel(waveforms)` lines.

**analyzer.py -- Re-enable feature sweep with ISTFT:**

4. Replace the TODO stub block (lines 311-319) with working ISTFT waveform generation:
   ```python
   # Decode to 2-channel spectrogram, convert to waveform via ISTFT
   mel_out = model.decode(z_tensor, target_shape=mel_shape)
   if complex_spectrogram is not None:
       wav = complex_spectrogram.complex_mel_to_waveform(
           mel_out.cpu(), stats=normalization_stats,
       )
       wav_np = wav.squeeze().numpy().astype(np.float32)
       features = compute_audio_features(
           wav_np, spectrogram.config.sample_rate,
       )
       for name in FEATURE_NAMES:
           sweep_features[name].append(features[name])
   ```
   Keep the graceful degradation: if `complex_spectrogram is None`, the sweep produces empty arrays and correlations default to 0.0 (the existing guard at line 325 handles this).

**analyzer.py -- Update slider labels to technical + musical style:**

5. Per CONTEXT.md decision: labels should be "PC1 (Brightness)", "Mag-Texture" style. Update the label construction (around line 336-354):

   Replace the default labels:
   ```python
   # Old: default_labels = [f"Axis {i + 1}" for i in range(n_active)]
   default_labels = [f"PC{i + 1}" for i in range(n_active)]
   ```

   Update the suggested label format. Map FEATURE_NAMES to musical descriptors:
   ```python
   _MUSICAL_LABELS = {
       "spectral_centroid": "Brightness",
       "spectral_bandwidth": "Width",
       "spectral_rolloff": "Rolloff",
       "spectral_flatness": "Noisiness",
       "rms_energy": "Loudness",
       "zero_crossing_rate": "Texture",
   }
   ```
   Define this mapping at module level (before the class).

   Update the suggested label construction:
   ```python
   if best_feature is not None:
       musical = _MUSICAL_LABELS.get(best_feature, best_feature)
       suggested_labels.append(f"PC{comp_idx + 1} ({musical})")
   else:
       suggested_labels.append(default_labels[comp_idx])
   ```

6. Update the `latent_dim` field in the AnalysisResult construction: it's currently hardcoded default 64 in the dataclass. The analyzer correctly passes `latent_dim=latent_dim` at line 373, which will now be 128 for v2.0 models. No change needed -- just verify the default in AnalysisResult is not relied upon.

**analyzer.py -- Update callers that invoke analyze():**

7. The analyzer is called from the training loop. Check loop.py for where `LatentSpaceAnalyzer` is used and pass complex_spectrogram and normalization_stats. Search for the call site in loop.py. If the analyze call exists, update it to pass the new parameters. If it's been deferred (per 13-02 summary: "Latent space analysis unconditionally skipped for 2-channel"), re-enable it.

Search loop.py for `analyze` or `LatentSpaceAnalyzer` to find and update the call site. Per 13-02 summary, analysis was "unconditionally skipped for 2-channel" -- this skip should be REMOVED since we're now wiring it.
  </action>
  <verify>
    <automated>cd H:/dev/Distill-complex-spec && python -c "
import inspect
from distill.controls.analyzer import LatentSpaceAnalyzer
sig = inspect.signature(LatentSpaceAnalyzer.analyze)
print('analyze has complex_spectrogram:', 'complex_spectrogram' in sig.parameters)
print('analyze has normalization_stats:', 'normalization_stats' in sig.parameters)

# Check the label style
from distill.controls.analyzer import _MUSICAL_LABELS
print('Musical labels defined:', bool(_MUSICAL_LABELS))
print('Brightness label:', _MUSICAL_LABELS.get('spectral_centroid'))

# Check no TODO(Phase 16) stubs remain
src = inspect.getsource(LatentSpaceAnalyzer)
print('TODO(Phase 16) in analyzer:', 'TODO(Phase 16)' in src)
"</automated>
  </verify>
  <done>PCA analysis feature sweep generates waveforms via ISTFT for feature correlation. Slider labels use "PC1 (Brightness)" format combining technical and musical descriptors. Dataloader handling updated for cached 2-channel spectrograms. Latent space analysis re-enabled for 2-channel models in training loop.</done>
</task>

</tasks>

<verification>
1. No `TODO(Phase 16)` stubs remain in src/: `grep -rn "TODO(Phase 16)" src/` returns 0 results
2. No `NotImplementedError` with "Phase 16" message remains: `grep -rn "Phase 16 will wire" src/` returns 0 results
3. `python -m pytest tests/test_istft_reconstruction.py -x` still passes
4. `python -c "from distill.training.preview import generate_preview; from distill.controls.analyzer import LatentSpaceAnalyzer"` imports without error
</verification>

<success_criteria>
- Training preview functions generate WAV files via ISTFT (no NotImplementedError)
- PCA analysis feature sweep decodes to waveforms via ISTFT for feature correlation
- Slider labels use "PC1 (Brightness)" format
- Spectral quality metrics logged to console during training
- Dataloader handling in analyzer works with cached 2-channel spectrograms
- Latent space analysis re-enabled for 2-channel models
- Zero TODO(Phase 16) stubs remain anywhere in src/
</success_criteria>

<output>
After completion, create `.planning/phases/16-full-pipeline-integration/16-02-SUMMARY.md`
</output>
