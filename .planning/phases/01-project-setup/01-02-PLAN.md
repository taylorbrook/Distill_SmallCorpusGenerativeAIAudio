---
phase: 01-project-setup
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/small_dataset_audio/hardware/device.py
  - src/small_dataset_audio/hardware/memory.py
  - src/small_dataset_audio/hardware/benchmark.py
autonomous: true

must_haves:
  truths:
    - "Device detection selects MPS on Apple Silicon, CUDA on NVIDIA, CPU as fallback"
    - "Smoke test catches 'available but broken' GPU and falls back to CPU with warning"
    - "Device info includes name, memory total, and memory available"
    - "OOM handler clears GPU memory outside except block and provides actionable guidance"
    - "Benchmark finds maximum batch size via binary search without crashing"
  artifacts:
    - path: "src/small_dataset_audio/hardware/device.py"
      provides: "Device detection, selection, smoke test, info reporting"
      exports: ["select_device", "get_device_info", "format_device_report"]
    - path: "src/small_dataset_audio/hardware/memory.py"
      provides: "Memory querying and OOM handling"
      exports: ["get_memory_info", "safe_gpu_operation", "clear_gpu_memory"]
    - path: "src/small_dataset_audio/hardware/benchmark.py"
      provides: "Binary-search GPU batch size benchmark"
      exports: ["benchmark_max_batch_size", "format_benchmark_report"]
  key_links:
    - from: "src/small_dataset_audio/hardware/benchmark.py"
      to: "src/small_dataset_audio/hardware/device.py"
      via: "uses select_device for benchmark target"
      pattern: "from.*device.*import"
    - from: "src/small_dataset_audio/hardware/benchmark.py"
      to: "src/small_dataset_audio/hardware/memory.py"
      via: "uses clear_gpu_memory between iterations"
      pattern: "from.*memory.*import.*clear_gpu_memory"
    - from: "src/small_dataset_audio/hardware/memory.py"
      to: "src/small_dataset_audio/hardware/device.py"
      via: "uses device info for memory queries"
      pattern: "from.*device.*import"
---

<objective>
Implement hardware detection, memory management, and GPU benchmarking.

Purpose: Build the device abstraction layer that all future GPU operations (training, inference) depend on. This must correctly identify MPS/CUDA/CPU, validate the device actually works (smoke test), provide memory info, handle OOM safely, and benchmark to set intelligent defaults.

Output: Three modules in the hardware/ package providing complete device lifecycle management — from detection through benchmarking.
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-project-setup/01-RESEARCH.md
@.planning/phases/01-project-setup/01-CONTEXT.md
@.planning/phases/01-project-setup/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement device detection and memory utilities</name>
  <files>
    src/small_dataset_audio/hardware/device.py
    src/small_dataset_audio/hardware/memory.py
  </files>
  <action>
    Implement device detection and memory management following the research patterns.

    **src/small_dataset_audio/hardware/device.py:**

    1. `select_device(preference: str = "auto") -> torch.device` — Select compute device with validation.
       - If preference is "auto": use `_auto_detect()`.
       - If preference is a specific device string ("mps", "cuda", "cuda:0", "cpu"): create `torch.device(preference)`.
       - For non-CPU devices, run `_smoke_test(device)`. If smoke test fails, print warning with rich console and fall back to CPU.
       - Return validated device.

    2. `_auto_detect() -> torch.device` — Auto-detect best available device.
       - Check CUDA first (`torch.cuda.is_available()`), then MPS (`torch.backends.mps.is_available()`), then CPU.
       - Do NOT use `torch.accelerator.current_accelerator()` as primary — it may return unexpected backends. Use it only as a reference. The explicit CUDA → MPS → CPU chain is more predictable for this project.

    3. `_smoke_test(device: torch.device) -> bool` — Quick validation that device works.
       - Create two 32x32 random tensors on device, multiply, sum, transfer to CPU.
       - Check result is not NaN.
       - Wrap in try/except, return False on any exception.
       - Clean up GPU memory in finally block (torch.cuda.empty_cache() or torch.mps.empty_cache()).

    4. `get_device_info(device: torch.device) -> dict` — Query detailed info.
       - For CUDA: name via `torch.cuda.get_device_name()`, memory via `torch.cuda.mem_get_info()`, compute capability.
       - For MPS: name = "Apple Silicon (MPS)", memory from psutil (unified memory), MPS allocated from `torch.mps.current_allocated_memory()`.
       - For CPU: name = CPU model string if available (via platform module), memory from psutil.
       - Return dict with keys: type, name, memory_total_gb, memory_free_gb (where available).

    5. `format_device_report(device: torch.device, info: dict, verbose: bool = False) -> str` — Format device info for terminal display.
       - Essential (default): device type and name, available memory.
       - Verbose: add PyTorch version, compute capability (CUDA), MPS allocated memory, CPU count.
       - Use rich markup for formatting (e.g., `[bold green]` for device name).

    **src/small_dataset_audio/hardware/memory.py:**

    1. `get_memory_info(device: torch.device) -> dict` — Get current memory state.
       - CUDA: allocated, reserved, free, total via torch.cuda functions.
       - MPS: current_allocated via torch.mps, system memory via psutil.
       - CPU: virtual_memory via psutil.
       - Return dict with consistent keys: allocated_gb, total_gb, free_gb.

    2. `clear_gpu_memory(device: torch.device) -> None` — Force GPU memory cleanup.
       - Call gc.collect() first.
       - Then device-specific cache clear (torch.cuda.empty_cache() or torch.mps.empty_cache()).
       - Call gc.collect() again.
       - No-op for CPU.

    3. `safe_gpu_operation(fn, *args, device: torch.device | None = None, **kwargs)` — Run GPU op with OOM protection.
       - CRITICAL: Move recovery code OUTSIDE the except block to release frame references (see research pitfall #6).
       - Set oom_occurred flag in except block.
       - After try/except exits: if oom_occurred, call clear_gpu_memory, return None.
       - If no OOM, return the function result.
       - Re-raise non-OOM RuntimeErrors.

    Add type hints and docstrings to all public functions. Import torch only inside function bodies or behind TYPE_CHECKING guard where possible (to allow the module to be imported for introspection even if torch has issues).
  </action>
  <verify>
    1. `uv run python -c "from small_dataset_audio.hardware.device import select_device; d = select_device('cpu'); print(f'Device: {d}')"` — CPU selection works
    2. `uv run python -c "from small_dataset_audio.hardware.device import select_device, get_device_info; d = select_device(); info = get_device_info(d); print(f'Device: {info[\"name\"]}, Memory: {info.get(\"memory_total_gb\", \"N/A\")} GB')"` — auto-detect works and reports info
    3. `uv run python -c "from small_dataset_audio.hardware.memory import get_memory_info, clear_gpu_memory; import torch; d = torch.device('cpu'); info = get_memory_info(d); print(f'Memory: {info}')"` — memory info works for CPU
    4. `uv run python -c "from small_dataset_audio.hardware.memory import safe_gpu_operation; result = safe_gpu_operation(lambda: 42); assert result == 42; print('safe_gpu_operation: OK')"` — safe operation wrapper works
  </verify>
  <done>
    Device detection correctly identifies available hardware (MPS/CUDA/CPU), validates with smoke test, falls back gracefully on failure. Memory utilities provide consistent memory info across all backends and OOM-safe operation wrapper with proper exception reference handling.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement hardware benchmark</name>
  <files>
    src/small_dataset_audio/hardware/benchmark.py
  </files>
  <action>
    Implement the binary-search batch size benchmark following the research pattern.

    **src/small_dataset_audio/hardware/benchmark.py:**

    1. `benchmark_max_batch_size(device: torch.device, sample_shape: tuple = (1, 48000), min_batch: int = 1, max_batch: int = 256) -> int` — Binary search for max batch size.
       - sample_shape default is (1, 48000) = 1 second of 48kHz mono audio.
       - Binary search between min_batch and max_batch.
       - For each candidate: create random tensor of shape (batch, *sample_shape) on device, run a conv1d operation (16 output channels, kernel size 1024, padding 512) to simulate realistic compute load.
       - If operation succeeds, record as working_batch and try higher.
       - If OOM: try lower. Move cleanup OUTSIDE except block (use the flag pattern from memory.py).
       - In finally: call clear_gpu_memory.
       - For CPU: skip benchmark, return a sensible default (32) — CPU batch size is limited by RAM, not GPU memory, and binary search would be slow.
       - Return the largest working batch size.

    2. `format_benchmark_report(device: torch.device, max_batch_size: int, verbose: bool = False) -> str` — Format benchmark results for display.
       - Essential: "Max batch size: {N} (on {device_name})"
       - Verbose: add sample shape, test methodology note, memory state after benchmark.
       - Use rich markup.

    3. `run_benchmark(device: torch.device, verbose: bool = False) -> dict` — High-level benchmark runner.
       - Calls benchmark_max_batch_size.
       - Calls get_memory_info for context.
       - Returns dict with: max_batch_size, device_type, device_name, memory_total_gb.
       - Prints formatted report.
       - This is the function that app.py will call during first-run.

    Import from sibling modules: `from .device import get_device_info` and `from .memory import clear_gpu_memory, get_memory_info`.

    Add type hints and docstrings.
  </action>
  <verify>
    1. `uv run python -c "from small_dataset_audio.hardware.benchmark import benchmark_max_batch_size; import torch; result = benchmark_max_batch_size(torch.device('cpu')); print(f'CPU batch size: {result}')"` — CPU benchmark returns sensible default
    2. `uv run python -c "from small_dataset_audio.hardware.benchmark import run_benchmark; import torch; result = run_benchmark(torch.device('cpu')); print(f'Benchmark result: {result}')"` — full benchmark runner works
    3. If MPS available: `uv run python -c "from small_dataset_audio.hardware.benchmark import benchmark_max_batch_size; import torch; result = benchmark_max_batch_size(torch.device('mps')); print(f'MPS batch size: {result}')"` — MPS benchmark completes without crash
  </verify>
  <done>
    Hardware benchmark finds maximum batch size via binary search on GPU (or returns sensible default on CPU). Benchmark cleans up GPU memory between iterations. Results are formatted for display and returned as dict for config storage.
  </done>
</task>

</tasks>

<verification>
1. Device detection works for CPU (always available)
2. Device detection auto-selects MPS on Apple Silicon Mac
3. Smoke test catches broken devices (can test by requesting unavailable device)
4. Memory info returns consistent dict keys across backends
5. OOM handler uses flag pattern (recovery outside except block)
6. Benchmark completes without crash on available device
7. Benchmark returns dict suitable for storing in config
</verification>

<success_criteria>
- `select_device("auto")` returns correct device for current hardware
- `select_device("cpu")` always works as explicit fallback
- `get_device_info()` returns name and memory for any device
- `safe_gpu_operation()` catches OOM and returns None
- `benchmark_max_batch_size()` returns positive integer without crash
- All three hardware modules import without error
</success_criteria>

<output>
After completion, create `.planning/phases/01-project-setup/01-02-SUMMARY.md`
</output>
