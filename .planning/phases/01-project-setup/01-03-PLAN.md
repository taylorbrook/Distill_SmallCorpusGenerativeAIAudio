---
phase: 01-project-setup
plan: 03
type: execute
wave: 3
depends_on: ["01-01", "01-02"]
files_modified:
  - src/small_dataset_audio/validation/environment.py
  - src/small_dataset_audio/validation/startup.py
  - src/small_dataset_audio/app.py
  - src/small_dataset_audio/__main__.py
autonomous: true

must_haves:
  truths:
    - "Application reports selected device (MPS/CUDA/CPU) at startup — users always know what's running"
    - "First run walks user through config (paths, device check, create directories, optional benchmark)"
    - "Every launch validates environment (deps, device, paths) and reports failures with fix instructions then exits"
    - "Application falls back gracefully to CPU when GPU unavailable with clear message"
    - "`python -m small_dataset_audio` launches the application"
    - "`--device cpu` flag overrides auto-detection"
    - "`--verbose` flag shows detailed startup information"
  artifacts:
    - path: "src/small_dataset_audio/validation/environment.py"
      provides: "Dependency checks, version verification, path validation"
      exports: ["validate_environment", "check_python_version", "check_pytorch", "check_torchaudio", "check_paths"]
    - path: "src/small_dataset_audio/validation/startup.py"
      provides: "Full startup validation sequence orchestrating all checks"
      exports: ["run_startup_validation"]
    - path: "src/small_dataset_audio/app.py"
      provides: "Application bootstrap, first-run experience, main entry point"
      exports: ["main"]
    - path: "src/small_dataset_audio/__main__.py"
      provides: "python -m small_dataset_audio support"
      contains: "from small_dataset_audio.app import main"
  key_links:
    - from: "src/small_dataset_audio/app.py"
      to: "src/small_dataset_audio/validation/startup.py"
      via: "calls run_startup_validation on every launch"
      pattern: "from.*validation.*startup.*import.*run_startup_validation"
    - from: "src/small_dataset_audio/app.py"
      to: "src/small_dataset_audio/hardware/device.py"
      via: "calls select_device for device selection and reporting"
      pattern: "from.*hardware.*device.*import.*select_device"
    - from: "src/small_dataset_audio/app.py"
      to: "src/small_dataset_audio/config/settings.py"
      via: "loads and saves config for first-run and startup"
      pattern: "from.*config.*settings.*import"
    - from: "src/small_dataset_audio/app.py"
      to: "src/small_dataset_audio/hardware/benchmark.py"
      via: "runs benchmark during first-run setup"
      pattern: "from.*hardware.*benchmark.*import.*run_benchmark"
    - from: "src/small_dataset_audio/validation/startup.py"
      to: "src/small_dataset_audio/validation/environment.py"
      via: "orchestrates all environment checks"
      pattern: "from.*environment.*import"
    - from: "src/small_dataset_audio/__main__.py"
      to: "src/small_dataset_audio/app.py"
      via: "entry point delegates to main()"
      pattern: "from.*app.*import.*main"
---

<objective>
Implement environment validation, startup sequence, first-run experience, and application entry point.

Purpose: Wire together config, hardware, and validation into the application that users actually run. This plan creates the user-facing experience: the guided first-run, the every-launch validation, the device reporting, and the CLI argument handling. After this plan, `make run` (or `python -m small_dataset_audio`) produces a working application.

Output: A launchable application with environment validation, first-run guided setup, device reporting at startup, and --device/--verbose CLI flags.
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-project-setup/01-RESEARCH.md
@.planning/phases/01-project-setup/01-CONTEXT.md
@.planning/phases/01-project-setup/01-01-SUMMARY.md
@.planning/phases/01-project-setup/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement environment validation</name>
  <files>
    src/small_dataset_audio/validation/environment.py
    src/small_dataset_audio/validation/startup.py
  </files>
  <action>
    Create the validation system that runs on every application launch.

    **src/small_dataset_audio/validation/environment.py:**

    Individual validation checks. Each returns a list of error strings (empty = OK).

    1. `check_python_version() -> list[str]` — Verify Python >= 3.11. Use `sys.version_info`. Error message: "Python 3.11+ required (found {version}). Install with: uv python install 3.11"

    2. `check_pytorch() -> list[str]` — Verify PyTorch is installed and version >= 2.10.0. Use importlib to try importing torch. Check `torch.__version__`. Error: "PyTorch 2.10.0+ required (found {version}). Run: uv sync" or "PyTorch not installed. Run: uv sync"

    3. `check_torchaudio() -> list[str]` — Verify TorchAudio is installed. Similar pattern. Error: "TorchAudio not installed. Run: uv sync"

    4. `check_paths(config: dict) -> list[str]` — Check that configured data directories exist or can be created. Use `resolve_path()` from config.settings. Return warnings (not errors) for missing dirs — they will be created on first run. Only error if parent directory doesn't exist and can't be created.

    5. `validate_environment(config: dict) -> tuple[list[str], list[str]]` — Run all checks. Returns (errors, warnings) tuple. Errors are fatal (app should exit). Warnings are informational (app can continue).
       - Python version and PyTorch/TorchAudio are errors (fatal).
       - Missing data dirs are warnings (will be auto-created).

    IMPORTANT: Do NOT import torch at module level. Import inside check_pytorch() and check_torchaudio() functions so that validation can report "PyTorch not installed" instead of crashing with ImportError.

    **src/small_dataset_audio/validation/startup.py:**

    1. `run_startup_validation(config: dict, verbose: bool = False) -> bool` — Full startup validation sequence.
       - Call validate_environment(config) to get errors and warnings.
       - Display results using rich console:
         - Errors in red with fix instructions.
         - Warnings in yellow.
         - Success in green checkmark.
       - If verbose: show all check details even on success (Python version, PyTorch version, device info, paths).
       - If errors: display all errors (don't stop at first), then return False.
       - If warnings only: display warnings, return True.
       - If all clear: show brief "Environment OK" (or verbose details), return True.
       - Per locked decision: "Report and exit on critical failures — clear message with fix instructions, no auto-fixing."
  </action>
  <verify>
    1. `uv run python -c "from small_dataset_audio.validation.environment import validate_environment; from small_dataset_audio.config.defaults import DEFAULT_CONFIG; errors, warnings = validate_environment(DEFAULT_CONFIG); print(f'Errors: {len(errors)}, Warnings: {len(warnings)}')"` — validation runs
    2. `uv run python -c "from small_dataset_audio.validation.environment import check_python_version; print(check_python_version())"` — returns empty list on valid Python
    3. `uv run python -c "from small_dataset_audio.validation.environment import check_pytorch; print(check_pytorch())"` — returns empty list when PyTorch installed
    4. `uv run python -c "from small_dataset_audio.validation.startup import run_startup_validation; from small_dataset_audio.config.defaults import DEFAULT_CONFIG; result = run_startup_validation(DEFAULT_CONFIG); print(f'Validation passed: {result}')"` — startup validation runs and passes
  </verify>
  <done>
    Environment validation checks Python version, PyTorch, TorchAudio, and data paths. Startup validation displays results with rich formatting, shows all errors with fix instructions, and returns boolean pass/fail. No auto-fixing — report and exit pattern.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement application bootstrap, first-run experience, and entry points</name>
  <files>
    src/small_dataset_audio/app.py
    src/small_dataset_audio/__main__.py
  </files>
  <action>
    Create the main application entry point that ties everything together.

    **src/small_dataset_audio/app.py:**

    1. `parse_args() -> argparse.Namespace` — Parse CLI arguments.
       - `--device [auto|mps|cuda|cpu]`: Device override (default: "auto"). Per discretion: support --device flag for debugging and testing.
       - `--verbose`: Show detailed startup info. Per discretion: essentials by default, verbose for details.
       - `--benchmark`: Run hardware benchmark and exit.
       - `--config PATH`: Override config file path.

    2. `first_run_setup(config_path: Path) -> dict` — Guided first-run experience using rich prompts.
       - Welcome message with project name.
       - Walk through data directory paths (show defaults, allow override).
       - Create data directories.
       - Run device detection and display result.
       - Offer to run hardware benchmark (if GPU available). If user accepts, run benchmark and store result in config.
       - Set first_run_complete = True.
       - Save config.
       - Show summary of what was configured.
       - Per locked decision: "Guided first-run experience: walk user through initial config (paths, device check, create directories)."

    3. `main() -> None` — Application entry point. This is what the `sda` console script calls.
       - Parse CLI args.
       - Load config (from --config path or default).
       - If config doesn't exist or first_run_complete is False: run first_run_setup().
       - Run startup validation. If validation fails: print errors and sys.exit(1). Per locked decision: "Report and exit on critical failures."
       - Select device (using --device override or config value or auto-detect).
       - Display device report. Per locked decision: "Always display selected device at startup."
       - If --benchmark flag: run benchmark, save results to config, exit.
       - Create data directories if they don't exist (non-first-run path).
       - Print "Small Dataset Audio v{version} ready." and the device name.
       - (Future phases will add actual functionality here — training, generation, UI launch.)

    Use rich Console and Prompt for all terminal output. Use rich.panel.Panel for the welcome message. Use rich.table.Table for config summary.

    **src/small_dataset_audio/__main__.py:**

    Minimal entry point:
    ```python
    """Allow running as: python -m small_dataset_audio"""
    from small_dataset_audio.app import main

    if __name__ == "__main__":
        main()
    ```

    This enables both `python -m small_dataset_audio` and the `sda` entry point (from pyproject.toml).
  </action>
  <verify>
    1. `uv run python -m small_dataset_audio --help` — shows usage with --device, --verbose, --benchmark, --config flags
    2. `uv run python -m small_dataset_audio --device cpu --verbose` — runs with CPU device, shows verbose output, displays device info (will trigger first-run if config.toml doesn't exist — that's expected)
    3. `uv run python -m small_dataset_audio --device cpu` — runs with essentials-only output, shows device selection
    4. `uv run python -m small_dataset_audio --benchmark --device cpu` — runs benchmark on CPU
    5. `uv run python -c "from small_dataset_audio.app import main; print('Import OK')"` — main function importable
    6. Verify config.toml was created after first run: `cat config.toml` or `ls config.toml`
  </verify>
  <done>
    Application launches via `python -m small_dataset_audio` or `sda` entry point. First run walks through guided setup. Every launch validates environment. Device is always displayed at startup. --device overrides auto-detection. --verbose shows detailed info. --benchmark runs and saves results. Critical failures report and exit with fix instructions.
  </done>
</task>

</tasks>

<verification>
1. `python -m small_dataset_audio --help` shows all CLI flags
2. First run triggers guided setup (paths, device, benchmark offer)
3. Subsequent runs skip first-run, show essentials only
4. `--verbose` shows detailed startup info
5. `--device cpu` forces CPU selection
6. `--device invaliddevice` reports error and exits
7. Device name and memory are displayed on every launch
8. Validation failure (if PyTorch were missing) would show fix instructions and exit
9. config.toml is created after first run with correct values
10. `make run` works (delegates to `uv run python -m small_dataset_audio`)
</verification>

<success_criteria>
- `python -m small_dataset_audio` launches and completes without error
- First-run creates config.toml and data directories
- Device is reported at startup on every launch
- `--device cpu` overrides auto-detection
- `--verbose` increases output detail
- Environment validation catches and reports issues with fix instructions
- Application exits cleanly on critical validation failures
</success_criteria>

<output>
After completion, create `.planning/phases/01-project-setup/01-03-SUMMARY.md`
</output>
