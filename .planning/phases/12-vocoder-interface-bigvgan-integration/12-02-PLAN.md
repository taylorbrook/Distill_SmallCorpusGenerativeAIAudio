---
phase: 12-vocoder-interface-bigvgan-integration
plan: 02
type: execute
wave: 2
depends_on:
  - 12-01
files_modified:
  - src/distill/vocoder/weight_manager.py
  - src/distill/vocoder/bigvgan_vocoder.py
  - src/distill/vocoder/__init__.py
autonomous: true
requirements:
  - VOC-01
  - VOC-03
  - VOC-04

must_haves:
  truths:
    - "BigVGAN model weights download automatically on first use with visible progress"
    - "After first download, vocoder loads from cache with no network access"
    - "Vocoder produces waveform output from a mel spectrogram tensor on CUDA, MPS, and CPU"
    - "BigVGAN model is loaded with use_cuda_kernel=False for cross-platform compatibility"
    - "Vendored BigVGAN imports work without modifying vendored source files"
  artifacts:
    - path: "src/distill/vocoder/weight_manager.py"
      provides: "BigVGAN weight download, caching, and offline loading"
      exports: ["ensure_bigvgan_weights"]
    - path: "src/distill/vocoder/bigvgan_vocoder.py"
      provides: "BigVGAN vocoder implementation wrapping vendored code"
      exports: ["BigVGANVocoder"]
    - path: "src/distill/vocoder/__init__.py"
      provides: "Updated get_vocoder() factory returning BigVGANVocoder"
      exports: ["VocoderBase", "BigVGANVocoder", "get_vocoder"]
  key_links:
    - from: "src/distill/vocoder/bigvgan_vocoder.py"
      to: "vendor/bigvgan/bigvgan.py"
      via: "sys.path manipulation for vendored imports"
      pattern: "sys\\.path.*vendor.*bigvgan"
    - from: "src/distill/vocoder/bigvgan_vocoder.py"
      to: "src/distill/vocoder/weight_manager.py"
      via: "ensure_bigvgan_weights() call before model load"
      pattern: "ensure_bigvgan_weights"
    - from: "src/distill/vocoder/__init__.py"
      to: "src/distill/vocoder/bigvgan_vocoder.py"
      via: "get_vocoder factory instantiation"
      pattern: "BigVGANVocoder"
    - from: "src/distill/vocoder/bigvgan_vocoder.py"
      to: "src/distill/hardware/device.py"
      via: "select_device() for auto device detection"
      pattern: "from distill\\.hardware\\.device import select_device"
---

<objective>
Implement the BigVGAN vocoder wrapper with automatic weight downloading and cross-platform device support.

Purpose: Make BigVGAN callable as a Python object -- download weights on first use, load the 122M parameter model, and produce waveforms from mel spectrograms on any device (CUDA, MPS, CPU). This is the core vocoder infrastructure that the mel adapter (Plan 03) will complete.

Output: Working `BigVGANVocoder` class that downloads, caches, loads, and runs BigVGAN inference. Updated `get_vocoder()` factory that returns a ready-to-use instance.
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-vocoder-interface-bigvgan-integration/12-RESEARCH.md
@.planning/phases/12-vocoder-interface-bigvgan-integration/12-01-SUMMARY.md
@vendor/bigvgan/bigvgan.py
@src/distill/vocoder/base.py
@src/distill/hardware/device.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement weight manager for BigVGAN model download and caching</name>
  <files>src/distill/vocoder/weight_manager.py</files>
  <action>
Create `src/distill/vocoder/weight_manager.py` that handles BigVGAN weight download, caching, and offline loading using HuggingFace Hub.

Key design decisions (from research):
- Use HuggingFace Hub's default cache directory (`~/.cache/huggingface/hub/`) -- do NOT create a custom cache location
- Use blocking download with HF Hub's built-in tqdm progress bar -- simpler than background download
- Model: `nvidia/bigvgan_v2_44khz_128band_512x` (the one best model per REQUIREMENTS.md)
- Fully offline after initial download -- use `local_files_only=True` as fallback

Implementation:

```python
"""BigVGAN weight download, caching, and loading.

Uses HuggingFace Hub for resumable download with progress indication.
After first download, weights are cached and no network access is needed.
"""
from __future__ import annotations

import logging
from pathlib import Path

logger = logging.getLogger(__name__)

BIGVGAN_REPO_ID = "nvidia/bigvgan_v2_44khz_128band_512x"

def ensure_bigvgan_weights() -> Path:
    """Ensure BigVGAN weights are available locally, downloading if needed.

    Returns the directory containing the model files (config.json,
    bigvgan_generator.pt, etc.).

    On first call: downloads ~489MB generator weights with progress bar.
    On subsequent calls: returns cached path instantly (no network).

    Returns
    -------
    Path
        Local directory path containing BigVGAN model files.

    Raises
    ------
    OSError
        If download fails and no cached version exists.
    """
    from huggingface_hub import snapshot_download, HfFileSystem
    # ... implementation
```

The function should:
1. Try `snapshot_download` with `repo_id=BIGVGAN_REPO_ID` -- HF Hub handles caching automatically
2. If network is unavailable, try with `local_files_only=True`
3. If both fail, raise a clear error explaining the user needs to download once with internet access
4. Log download progress at INFO level
5. Return the local directory Path containing the model files

Use `snapshot_download` rather than individual `hf_hub_download` calls -- BigVGAN's `from_pretrained` expects a directory with config.json + generator weights + other files together. The `snapshot_download` function downloads the entire model repository to cache.

Also add a helper function:

```python
def is_bigvgan_cached() -> bool:
    """Check if BigVGAN weights are already cached (no download needed)."""
    try:
        snapshot_download(BIGVGAN_REPO_ID, local_files_only=True)
        return True
    except Exception:
        return False
```
  </action>
  <verify>
- `python -c "from distill.vocoder.weight_manager import ensure_bigvgan_weights, is_bigvgan_cached; print('imports OK')"` succeeds
- If run with internet: `python -c "from distill.vocoder.weight_manager import ensure_bigvgan_weights; p = ensure_bigvgan_weights(); print(f'Weights at: {p}')"` downloads and returns a valid path
- After download: `python -c "from distill.vocoder.weight_manager import is_bigvgan_cached; print(is_bigvgan_cached())"` returns True
  </verify>
  <done>
Weight manager downloads BigVGAN weights on first call with progress indication, caches them via HuggingFace Hub, and returns cached path on subsequent calls with no network access.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement BigVGAN vocoder class and wire get_vocoder() factory</name>
  <files>
    src/distill/vocoder/bigvgan_vocoder.py
    src/distill/vocoder/__init__.py
  </files>
  <action>
**Part A: Create BigVGANVocoder class**

Create `src/distill/vocoder/bigvgan_vocoder.py` implementing `VocoderBase`:

```python
"""BigVGAN-v2 universal vocoder implementation.

Wraps the vendored NVIDIA BigVGAN source code, providing automatic
weight downloading, cross-platform device support, and the standard
VocoderBase interface for mel-to-waveform conversion.

IMPORTANT: This module accepts mel spectrograms that are ALREADY in
BigVGAN's expected format (log-clamp, Slaney, 44.1kHz). The MelAdapter
(Plan 03) handles conversion from VAE format. Direct callers must
provide correctly formatted mels.
"""
```

Key implementation details:

1. **Vendored import mechanism**: Use `sys.path` manipulation to import BigVGAN:
```python
import sys
from pathlib import Path

def _import_bigvgan():
    """Import vendored BigVGAN module with sys.path manipulation."""
    vendor_dir = str(Path(__file__).resolve().parents[3] / "vendor" / "bigvgan")
    if vendor_dir not in sys.path:
        sys.path.insert(0, vendor_dir)
    import bigvgan as bigvgan_module
    return bigvgan_module
```
Do NOT use a context manager that removes the path -- BigVGAN's internal imports need the path to remain available.

2. **Model loading**: Use BigVGAN's `from_pretrained` API, NOT manual weight loading:
```python
bigvgan_module = _import_bigvgan()
model = bigvgan_module.BigVGAN.from_pretrained(
    BIGVGAN_REPO_ID,
    use_cuda_kernel=False,  # MUST be False for MPS/CPU compatibility
)
model.remove_weight_norm()
model.eval()
```
Note: `from_pretrained` handles weight download internally via HuggingFace Hub (same cache as weight_manager). Call `ensure_bigvgan_weights()` first only to trigger the download with logging before the model load.

3. **Device management**: Use the project's existing `distill.hardware.device.select_device()` for auto-detection. Move model with `.to(device)`.

4. **Inference**: Use `torch.inference_mode()` for forward pass. Input shape expected: `[B, 128, T]` (no channel dim). Output shape: `[B, 1, T*512]`.

5. **The `mel_to_waveform` method** at this stage accepts raw BigVGAN-format mels (log-clamp, Slaney). The full VAE-format input path (accepting log1p HTK mels) will be completed in Plan 03 when the MelAdapter is added. Document this clearly:
```python
def mel_to_waveform(self, mel: torch.Tensor) -> torch.Tensor:
    """Convert mel spectrogram to waveform.

    NOTE: At this stage, this method expects mels in BigVGAN's native
    format: log(clamp(slaney_mel, 1e-5)), shape [B, 128, T].
    Plan 03 adds the MelAdapter that converts VAE format automatically.

    After Plan 03: Accepts VAE format [B, 1, 128, T] log1p mels.
    """
```

6. **Sample rate**: Return `44100` (BigVGAN's native rate). Resampling to 48kHz is the generation pipeline's responsibility (Phase 14), per research recommendation.

**Part B: Update get_vocoder() factory**

Update `src/distill/vocoder/__init__.py` to wire the factory:

```python
from distill.vocoder.base import VocoderBase
from distill.vocoder.bigvgan_vocoder import BigVGANVocoder

__all__ = ["VocoderBase", "BigVGANVocoder", "get_vocoder"]

def get_vocoder(vocoder_type: str = "bigvgan", device: str = "auto") -> VocoderBase:
    if vocoder_type == "bigvgan":
        return BigVGANVocoder(device=device)
    elif vocoder_type == "hifigan":
        raise NotImplementedError(
            "Per-model HiFi-GAN vocoder is planned for Phase 16."
        )
    else:
        raise ValueError(f"Unknown vocoder type: {vocoder_type!r}. Use 'bigvgan' or 'hifigan'.")
```

Use lazy import of BigVGANVocoder inside the function body to avoid loading torch at module import time (project pattern).
  </action>
  <verify>
- `python -c "from distill.vocoder import get_vocoder; v = get_vocoder('bigvgan'); print(f'Vocoder loaded, sample_rate={v.sample_rate}')"` prints `Vocoder loaded, sample_rate=44100`
- `python -c "from distill.vocoder import BigVGANVocoder; print(BigVGANVocoder)"` shows the class
- `python -c "
import torch
from distill.vocoder import get_vocoder
v = get_vocoder('bigvgan')
# Create a dummy mel in BigVGAN format [B, 128, T]
dummy_mel = torch.randn(1, 128, 10)
wav = v.mel_to_waveform(dummy_mel)
print(f'Input: {dummy_mel.shape}, Output: {wav.shape}')
"` produces waveform output without error
  </verify>
  <done>
BigVGANVocoder loads the vendored BigVGAN model with auto-downloaded weights, runs inference on the auto-detected device (CUDA/MPS/CPU), and produces waveforms from BigVGAN-format mel spectrograms. get_vocoder("bigvgan") returns a ready-to-use instance.
  </done>
</task>

</tasks>

<verification>
1. `from distill.vocoder import get_vocoder, BigVGANVocoder, VocoderBase` all import
2. `get_vocoder("bigvgan")` returns a BigVGANVocoder instance
3. BigVGAN weights are downloaded on first use (or served from cache)
4. `is_bigvgan_cached()` returns True after first download
5. A dummy mel tensor [1, 128, 10] produces a waveform tensor output
6. `use_cuda_kernel=False` is set (grep the source)
7. Vocoder sample_rate is 44100
</verification>

<success_criteria>
BigVGAN vocoder loads and runs inference on any available device. Weights are downloaded automatically with progress on first use and cached for offline operation. The get_vocoder() factory returns a working instance.
</success_criteria>

<output>
After completion, create `.planning/phases/12-vocoder-interface-bigvgan-integration/12-02-SUMMARY.md`
</output>
