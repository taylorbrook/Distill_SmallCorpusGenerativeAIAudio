---
phase: 12-rvq-vae-core-architecture
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/distill/training/config.py
  - src/distill/models/vqvae.py
autonomous: true
requirements:
  - VQVAE-01
  - VQVAE-02
  - VQVAE-03
  - VQVAE-06

must_haves:
  truths:
    - "ConvVQVAE can be instantiated with configurable RVQ levels (2-4) and codebook dimension"
    - "ConvVQVAE forward pass accepts [B,1,n_mels,time] mel and returns (recon, indices, commit_loss)"
    - "Recon shape matches input shape exactly after pad-crop round trip"
    - "Codebook size auto-scales to dataset size (64 for <=20, 128 for <=100, 256 for >100)"
    - "RVQ uses k-means init, EMA updates, and dead code reset via vector-quantize-pytorch"
  artifacts:
    - path: "src/distill/models/vqvae.py"
      provides: "ConvVQVAE model with VQEncoder, VQDecoder, QuantizerWrapper"
      exports: ["VQEncoder", "VQDecoder", "QuantizerWrapper", "ConvVQVAE"]
    - path: "src/distill/training/config.py"
      provides: "VQVAEConfig dataclass and get_adaptive_vqvae_config function"
      contains: "class VQVAEConfig"
  key_links:
    - from: "src/distill/models/vqvae.py"
      to: "vector_quantize_pytorch.ResidualVQ"
      via: "QuantizerWrapper wraps ResidualVQ"
      pattern: "from vector_quantize_pytorch import ResidualVQ"
    - from: "src/distill/models/vqvae.py"
      to: "src/distill/training/config.py"
      via: "ConvVQVAE constructor accepts VQVAEConfig fields"
      pattern: "codebook_dim.*codebook_size.*num_quantizers"
    - from: "src/distill/training/config.py"
      to: "src/distill/models/vqvae.py"
      via: "get_adaptive_vqvae_config produces config for ConvVQVAE instantiation"
      pattern: "get_adaptive_vqvae_config"
---

<objective>
Create the ConvVQVAE model architecture with residual vector quantization, dataset-adaptive codebook configuration, and codebook health monitoring.

Purpose: This is the foundational model that replaces the continuous VAE with a discrete RVQ-VAE. All subsequent phases (training, prior, generation, code editing) depend on this model existing and producing correct forward pass outputs.
Output: A working ConvVQVAE model in `vqvae.py`, VQVAEConfig dataclass with adaptive sizing in `config.py`, and `vector-quantize-pytorch` installed as a dependency.
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-rvq-vae-core-architecture/12-RESEARCH.md
@.planning/phases/12-rvq-vae-core-architecture/12-CONTEXT.md
@src/distill/models/vae.py
@src/distill/training/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install vector-quantize-pytorch and create VQVAEConfig with adaptive sizing</name>
  <files>pyproject.toml, src/distill/training/config.py</files>
  <action>
1. Install the vector-quantize-pytorch library:
   ```
   uv add "vector-quantize-pytorch>=1.27.0"
   ```

2. Add `VQVAEConfig` dataclass to `src/distill/training/config.py` (append after existing code, do NOT modify existing TrainingConfig or get_adaptive_config):

   ```python
   @dataclass
   class VQVAEConfig:
       """VQ-VAE model and training configuration."""
       # Model architecture
       codebook_dim: int = 128        # Per user decision: default 128
       codebook_size: int = 256       # Auto-scaled by dataset size
       num_quantizers: int = 3        # Default 3 (Structure/Timbre/Detail), range 2-4

       # Quantizer (vector-quantize-pytorch params)
       decay: float = 0.95            # EMA decay (lower for small datasets)
       commitment_weight: float = 0.25  # Commitment loss weight (not library default 1.0)
       threshold_ema_dead_code: int = 2  # Dead code replacement threshold
       kmeans_init: bool = True
       kmeans_iters: int = 10

       # Regularization
       dropout: float = 0.2
       gradient_clip_norm: float = 1.0

       # Training
       batch_size: int = 32
       max_epochs: int = 200
       learning_rate: float = 1e-3
       weight_decay: float = 0.01

       # Dataset
       chunk_duration_s: float = 1.0
       val_fraction: float = 0.2
       augmentation_expansion: int = 10

       # Checkpoint/preview
       checkpoint_interval: int = 10
       preview_interval: int = 20
       max_checkpoints: int = 3

       # Device
       device: str = "auto"
       num_workers: int = 0
   ```

3. Add `get_adaptive_vqvae_config(file_count: int) -> VQVAEConfig` function after the dataclass. This implements VQVAE-02 (dataset-adaptive codebook sizing):
   - file_count <= 20: codebook_size=64, decay=0.8, dropout=0.4, max_epochs=100, learning_rate=5e-4, augmentation_expansion=15
   - file_count <= 100: codebook_size=128, decay=0.9, dropout=0.2, max_epochs=200, learning_rate=1e-3, augmentation_expansion=10
   - file_count > 100 (up to 500): codebook_size=256, decay=0.95, dropout=0.1, max_epochs=300, learning_rate=1e-3, augmentation_expansion=5
   - All tiers: num_quantizers=3, codebook_dim=128, commitment_weight=0.25, threshold_ema_dead_code=2, kmeans_init=True, kmeans_iters=10
   - Also scale batch_size adaptively: min(32, max(1, file_count * 5 // 4))
   - Also scale val_fraction: 0.5 for <10 files, 0.3 for <50, 0.2 for <200, 0.1 for >=200

Follow the exact same pattern as existing `get_adaptive_config()` -- pure Python, no torch dependency, docstring with table showing tiers.
  </action>
  <verify>
Run `python -c "from distill.training.config import VQVAEConfig, get_adaptive_vqvae_config; c = get_adaptive_vqvae_config(10); print(c.codebook_size, c.decay); c2 = get_adaptive_vqvae_config(50); print(c2.codebook_size, c2.decay); c3 = get_adaptive_vqvae_config(200); print(c3.codebook_size, c3.decay)"` -- should output "64 0.8", "128 0.9", "256 0.95". Also verify `from vector_quantize_pytorch import ResidualVQ` imports successfully.
  </verify>
  <done>VQVAEConfig dataclass exists with all fields documented. get_adaptive_vqvae_config returns correct codebook_size (64/128/256) for each dataset tier. vector-quantize-pytorch is installed and importable.</done>
</task>

<task type="auto">
  <name>Task 2: Create ConvVQVAE model with VQEncoder, VQDecoder, and QuantizerWrapper</name>
  <files>src/distill/models/vqvae.py</files>
  <action>
Create `src/distill/models/vqvae.py` as a NEW file (do NOT modify vae.py). This is a fresh design per user decision -- do NOT reuse ConvEncoder/ConvDecoder classes even though the conv backbone is structurally similar.

The file must contain these classes:

**VQEncoder(nn.Module):**
- Constructor: `__init__(self, codebook_dim: int = 128, dropout: float = 0.2)`
- 4-layer conv backbone: 1->32->64->128->256, each with Conv2d(stride=2, padding=1) + BatchNorm2d + ReLU(inplace=True) + Dropout2d(dropout)
- Final 1x1 Conv2d projection: 256 -> codebook_dim (replaces fc_mu/fc_logvar from v1.0)
- Store `_padded_shape` for decoder crop
- Forward: pad input mel to multiple of 16 on both dims, run convs, run proj, return [B, codebook_dim, H, W]
- No lazy init needed -- Conv2d layers are spatially independent

**VQDecoder(nn.Module):**
- Constructor: `__init__(self, codebook_dim: int = 128, dropout: float = 0.2)`
- 1x1 Conv2d projection: codebook_dim -> 256 (replaces fc from v1.0)
- 4-layer deconv backbone: 256->128->64->32->1, each with ConvTranspose2d(stride=2, padding=1, output_padding=1) + BatchNorm2d + ReLU + Dropout2d. Final layer uses Softplus() activation (output >= 0, matches v1.0 mel range)
- Forward: accepts `(x, target_shape=None)`, projects, deconvs, crops to target_shape if provided
- No lazy init needed -- no linear layers

**QuantizerWrapper(nn.Module):**
- Constructor accepts: dim, codebook_size, num_quantizers, decay, commitment_weight, threshold_ema_dead_code, kmeans_init, kmeans_iters
- Wraps `vector_quantize_pytorch.ResidualVQ` with those params
- Forward: accepts [B, seq_len, dim], returns (quantized, indices, commit_loss)
  - IMPORTANT: ResidualVQ returns commit_loss as a tensor -- handle both scalar and per-quantizer formats
- `get_codebook_utilization(indices)` method: compute per-level utilization (unique/total), perplexity (exp of entropy), and dead_codes count. Return dict keyed by `"level_0"`, `"level_1"`, etc.
- `get_output_from_indices(indices)` method: delegate to self.rvq.get_output_from_indices(indices), return [B, seq_len, dim]

**ConvVQVAE(nn.Module):**
- Constructor: `__init__(self, codebook_dim=128, codebook_size=256, num_quantizers=3, decay=0.95, commitment_weight=0.25, threshold_ema_dead_code=2, dropout=0.2)`
- Compose: VQEncoder, QuantizerWrapper, VQDecoder
- Store config values as attributes for persistence (codebook_dim, codebook_size, num_quantizers)
- `encode(x)` -> [B, codebook_dim, H, W]
- `quantize(embeddings)` -> (quantized_spatial, indices, commit_loss)
  - Reshape [B,D,H,W] -> [B,H*W,D] for RVQ, then reshape back after
  - CRITICAL: permute(0,2,3,1).reshape(B,H*W,D) before RVQ, then reshape(B,H,W,D).permute(0,3,1,2) after
- `decode(quantized, target_shape=None)` -> [B,1,n_mels,time]
- `codes_to_embeddings(indices, spatial_shape)` -> [B, codebook_dim, H, W]
  - For decode-from-indices path (Phase 16 encode/decode)
  - spatial_shape = (H, W) needed to reshape flat sequence back to spatial
- `forward(x)` -> (recon, indices, commit_loss)
  - Store original_shape = (x.shape[2], x.shape[3]) before encode
  - Store _spatial_shape = (H, W) after encode for codes_to_embeddings
  - Pass target_shape=original_shape to decode for pad-crop round trip

Module docstring should explain this replaces ConvVAE, that forward returns (recon, indices, commit_loss) instead of (recon, mu, logvar), and that generation requires a prior (Phase 14) rather than sampling from N(0,1).

Use `from __future__ import annotations` for modern type hints. Follow existing codebase patterns: detailed docstrings on classes and public methods, type annotations everywhere.
  </action>
  <verify>
Run a forward pass test:
```python
python -c "
import torch
from distill.models.vqvae import ConvVQVAE
model = ConvVQVAE(codebook_dim=128, codebook_size=64, num_quantizers=3)
x = torch.randn(2, 1, 128, 94)  # batch of 2, 1-second mels
recon, indices, commit_loss = model(x)
print(f'Input: {x.shape}')
print(f'Recon: {recon.shape}')
print(f'Indices: {indices.shape}')
print(f'Commit loss: {commit_loss.item():.4f}')
assert recon.shape == x.shape, f'Shape mismatch: {recon.shape} vs {x.shape}'
assert indices.shape == (2, 48, 3), f'Wrong indices shape: {indices.shape}'
print('All shapes correct!')
# Test configurable levels
model2 = ConvVQVAE(num_quantizers=2)
_, idx2, _ = model2(x)
assert idx2.shape[2] == 2, 'num_quantizers=2 not working'
model4 = ConvVQVAE(num_quantizers=4)
_, idx4, _ = model4(x)
assert idx4.shape[2] == 4, 'num_quantizers=4 not working'
print('Configurable RVQ levels (2-4) verified!')
"
```
Verify: Input [2,1,128,94], Recon [2,1,128,94] (exact match), Indices [2,48,3] (48=8*6 spatial positions, 3 quantizer levels). No errors.
  </verify>
  <done>ConvVQVAE model instantiates with configurable RVQ levels (2-4) and codebook dimension. Forward pass accepts [B,1,128,94] mel and returns (recon, indices, commit_loss) with recon.shape == input.shape. RVQ uses k-means init, EMA updates, dead code reset via vector-quantize-pytorch library. No v1.0 code was reused or modified.</done>
</task>

</tasks>

<verification>
1. `from distill.models.vqvae import ConvVQVAE, VQEncoder, VQDecoder, QuantizerWrapper` imports without error
2. `from distill.training.config import VQVAEConfig, get_adaptive_vqvae_config` imports without error
3. Forward pass produces correctly shaped outputs for various mel dimensions
4. Codebook utilization metrics can be computed from indices
5. get_adaptive_vqvae_config returns correct codebook_size for each dataset tier
6. No v1.0 files were modified (vae.py, existing config.py functions unchanged)
</verification>

<success_criteria>
- ConvVQVAE forward pass works: input [B,1,128,94] -> output (recon[B,1,128,94], indices[B,48,3], commit_loss scalar)
- Configurable RVQ levels: num_quantizers=2,3,4 all produce correct index shapes
- Dataset-adaptive config: get_adaptive_vqvae_config(10)=64, (50)=128, (200)=256 codebook sizes
- vector-quantize-pytorch installed and functional
- No modifications to existing v1.0 model code
</success_criteria>

<output>
After completion, create `.planning/phases/12-rvq-vae-core-architecture/12-01-SUMMARY.md`
</output>
