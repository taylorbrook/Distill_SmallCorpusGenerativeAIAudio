---
phase: 12-rvq-vae-core-architecture
plan: 02
type: execute
wave: 2
depends_on:
  - 12-01
files_modified:
  - src/distill/models/losses.py
  - src/distill/models/__init__.py
autonomous: true
requirements:
  - VQVAE-05

must_haves:
  truths:
    - "vqvae_loss combines MSE reconstruction with multi-scale spectral loss and commitment loss"
    - "vqvae_loss uses a single commitment_weight parameter with no KL divergence, free bits, or annealing"
    - "Multi-scale spectral loss compares mel spectrograms at multiple resolutions (full, 2x downsampled, 4x downsampled)"
    - "models __init__.py exports ConvVQVAE, VQVAEConfig, vqvae_loss, and get_adaptive_vqvae_config"
  artifacts:
    - path: "src/distill/models/losses.py"
      provides: "vqvae_loss function with multi-scale mel spectral loss"
      contains: "def vqvae_loss"
    - path: "src/distill/models/__init__.py"
      provides: "Public API exports for VQ-VAE classes and functions"
      contains: "ConvVQVAE"
  key_links:
    - from: "src/distill/models/losses.py"
      to: "src/distill/models/vqvae.py"
      via: "vqvae_loss accepts outputs from ConvVQVAE.forward()"
      pattern: "recon.*target.*commit_loss"
    - from: "src/distill/models/__init__.py"
      to: "src/distill/models/vqvae.py"
      via: "re-exports ConvVQVAE and related classes"
      pattern: "from distill.models.vqvae import"
    - from: "src/distill/models/__init__.py"
      to: "src/distill/training/config.py"
      via: "re-exports VQVAEConfig and get_adaptive_vqvae_config"
      pattern: "from distill.training.config import"
---

<objective>
Create the VQ-VAE loss function with multi-scale spectral reconstruction loss and commitment loss, then wire all new VQ-VAE exports through the models __init__.py public API.

Purpose: Completes the Phase 12 model architecture by providing the loss function that will drive VQ-VAE training (Phase 13) and making all new classes accessible through the standard `distill.models` import path. The loss function explicitly replaces KL divergence with commitment loss per user decision.
Output: `vqvae_loss()` function in losses.py, updated `__init__.py` with all VQ-VAE exports.
</objective>

<execution_context>
@C:/Users/Taylor/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Taylor/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-rvq-vae-core-architecture/12-RESEARCH.md
@.planning/phases/12-rvq-vae-core-architecture/12-01-SUMMARY.md
@src/distill/models/losses.py
@src/distill/models/__init__.py
@src/distill/models/vqvae.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create vqvae_loss function with multi-scale spectral loss</name>
  <files>src/distill/models/losses.py</files>
  <action>
Append new functions to `src/distill/models/losses.py` AFTER the existing v1.0 loss functions. Do NOT modify or remove existing `vae_loss`, `get_kl_weight`, or `compute_kl_divergence` -- they are still used by v1.0 code paths until Phase 13 fully replaces them.

Add these functions:

**`multi_scale_mel_loss(recon, target) -> torch.Tensor`:**
- Computes multi-scale MSE on mel spectrograms at different resolutions per user decision ("multi-scale spectral loss for reconstruction, multiple STFT resolutions").
- Full resolution MSE: `F.mse_loss(recon, target)`
- 2x downsampled: `F.mse_loss(F.avg_pool2d(recon, 2), F.avg_pool2d(target, 2))`
- 4x downsampled: `F.mse_loss(F.avg_pool2d(recon, 4), F.avg_pool2d(target, 4))`
- Return average of all three scales: `total / 3.0`
- Docstring explaining this captures both fine-grained and structural reconstruction quality

**`vqvae_loss(recon, target, commit_loss, commitment_weight=0.25) -> tuple[Tensor, Tensor, Tensor]`:**
- Parameters:
  - recon: [B,1,n_mels,time] reconstructed mel spectrogram
  - target: [B,1,n_mels,time] original mel spectrogram
  - commit_loss: scalar tensor from QuantizerWrapper (commitment loss from RVQ)
  - commitment_weight: float, weight for commitment loss term (default 0.25)
- Reconstruction loss: call `multi_scale_mel_loss(recon, target)`
- Commitment term: `commitment_weight * commit_loss` (handle both scalar and multi-element commit_loss via `.sum()`)
- Total: `recon_loss + weighted_commit`
- Returns: `(total_loss, recon_loss, weighted_commit)` -- all scalar tensors
- CRITICAL: No KL divergence, no free bits, no annealing. This is the complete loss function.
- Docstring explaining this replaces `vae_loss` for VQ-VAE training, and that the single `commitment_weight` parameter is the only tunable (per user decision).

Follow existing code style: `from __future__ import annotations`, type annotations, detailed docstrings matching the existing `vae_loss` pattern.
  </action>
  <verify>
Run end-to-end loss computation:
```python
python -c "
import torch
from distill.models.vqvae import ConvVQVAE
from distill.models.losses import vqvae_loss, multi_scale_mel_loss
model = ConvVQVAE(codebook_size=64, num_quantizers=3)
x = torch.randn(2, 1, 128, 94)
recon, indices, commit_loss = model(x)
total, recon_l, commit_l = vqvae_loss(recon, x, commit_loss)
print(f'Total: {total.item():.4f}')
print(f'Recon: {recon_l.item():.4f}')
print(f'Commit: {commit_l.item():.4f}')
assert total.requires_grad, 'Loss must be differentiable'
total.backward()
print('Backward pass successful!')
# Verify old loss still works
from distill.models.losses import vae_loss
print('Old vae_loss still importable: OK')
"
```
Verify total_loss is a finite, differentiable scalar. Backward pass completes without error. Old `vae_loss` still importable.
  </verify>
  <done>vqvae_loss function exists in losses.py, combines multi-scale mel reconstruction loss with commitment loss, uses single commitment_weight parameter, contains no KL divergence logic. Backward pass works. Existing v1.0 loss functions remain untouched.</done>
</task>

<task type="auto">
  <name>Task 2: Update models __init__.py to export VQ-VAE public API</name>
  <files>src/distill/models/__init__.py</files>
  <action>
Update `src/distill/models/__init__.py` to add VQ-VAE exports alongside existing v1.0 exports. Do NOT remove existing exports -- they are still needed by training loop, persistence, CLI, and UI until Phase 13 replaces them.

Add these imports:

```python
from distill.models.vqvae import ConvVQVAE, QuantizerWrapper, VQDecoder, VQEncoder
from distill.models.losses import multi_scale_mel_loss, vqvae_loss
from distill.training.config import VQVAEConfig, get_adaptive_vqvae_config
```

Add to `__all__` list (append after existing entries):

```python
# vqvae.py (v1.1)
"ConvVQVAE",
"VQEncoder",
"VQDecoder",
"QuantizerWrapper",
# losses.py (v1.1)
"vqvae_loss",
"multi_scale_mel_loss",
# config (v1.1)
"VQVAEConfig",
"get_adaptive_vqvae_config",
```

Update the module docstring to mention VQ-VAE models alongside VAE models.

Organize the imports with clear v1.0 / v1.1 section comments so the Phase 13 migration knows what to remove.
  </action>
  <verify>
Run:
```python
python -c "
from distill.models import (
    ConvVQVAE, VQEncoder, VQDecoder, QuantizerWrapper,
    vqvae_loss, multi_scale_mel_loss,
    VQVAEConfig, get_adaptive_vqvae_config,
    ConvVAE, vae_loss, load_model, save_model,
)
print('All v1.0 and v1.1 exports importable from distill.models')
config = get_adaptive_vqvae_config(50)
model = ConvVQVAE(
    codebook_dim=config.codebook_dim,
    codebook_size=config.codebook_size,
    num_quantizers=config.num_quantizers,
    decay=config.decay,
    commitment_weight=config.commitment_weight,
    dropout=config.dropout,
)
import torch
x = torch.randn(2, 1, 128, 94)
recon, indices, commit_loss = model(x)
total, recon_l, commit_l = vqvae_loss(recon, x, commit_loss, config.commitment_weight)
print(f'Config -> Model -> Forward -> Loss pipeline works!')
print(f'codebook_size={config.codebook_size}, loss={total.item():.4f}')
"
```
Verify: all imports succeed, config-to-model-to-loss pipeline runs, no import errors for either v1.0 or v1.1 symbols.
  </verify>
  <done>models __init__.py exports all VQ-VAE classes (ConvVQVAE, VQEncoder, VQDecoder, QuantizerWrapper), loss functions (vqvae_loss, multi_scale_mel_loss), and config (VQVAEConfig, get_adaptive_vqvae_config). All v1.0 exports remain functional. Full config -> model -> forward -> loss pipeline works end-to-end through the public API.</done>
</task>

</tasks>

<verification>
1. `from distill.models import ConvVQVAE, vqvae_loss, VQVAEConfig, get_adaptive_vqvae_config` -- all importable
2. Full pipeline: config = get_adaptive_vqvae_config(N) -> model = ConvVQVAE(**config_fields) -> recon, indices, commit_loss = model(x) -> total, recon_l, commit_l = vqvae_loss(recon, x, commit_loss) -> total.backward() succeeds
3. No KL divergence, free bits, or annealing in vqvae_loss
4. Multi-scale spectral loss operates at 3 resolutions
5. All v1.0 exports still work (ConvVAE, vae_loss, load_model, etc.)
</verification>

<success_criteria>
- vqvae_loss returns (total, recon_loss, commit_loss) as differentiable scalars
- multi_scale_mel_loss computes at 3 resolutions (full, 2x, 4x downsampled)
- No KL divergence anywhere in vqvae_loss
- All VQ-VAE symbols importable from distill.models
- Full config -> model -> loss pipeline works end-to-end
- v1.0 code paths unbroken (all existing exports still work)
</success_criteria>

<output>
After completion, create `.planning/phases/12-rvq-vae-core-architecture/12-02-SUMMARY.md`
</output>
