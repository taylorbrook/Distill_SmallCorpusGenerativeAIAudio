---
phase: 03-core-training-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/small_dataset_audio/audio/spectrogram.py
  - src/small_dataset_audio/models/vae.py
  - src/small_dataset_audio/models/losses.py
autonomous: true

must_haves:
  truths:
    - "Waveform can be converted to normalized log-mel spectrogram and back to waveform"
    - "VAE can encode a mel spectrogram to a 64-dim latent vector and decode it back"
    - "VAE loss function produces reconstruction loss, KL loss, and total loss with annealing"
    - "KL annealing weight increases from 0 to 1 over warmup fraction of training"
    - "Free bits prevent posterior collapse by clamping per-dimension KL minimum"
  artifacts:
    - path: "src/small_dataset_audio/audio/spectrogram.py"
      provides: "SpectrogramConfig dataclass and AudioSpectrogram conversion class"
      contains: "class AudioSpectrogram"
    - path: "src/small_dataset_audio/models/vae.py"
      provides: "ConvEncoder, ConvDecoder, ConvVAE with encode/decode/reparameterize"
      contains: "class ConvVAE"
    - path: "src/small_dataset_audio/models/losses.py"
      provides: "vae_loss function with free bits + annealing, get_kl_weight schedule"
      contains: "def vae_loss"
  key_links:
    - from: "src/small_dataset_audio/audio/spectrogram.py"
      to: "torchaudio.transforms"
      via: "MelSpectrogram, InverseMelScale, GriffinLim"
      pattern: "MelSpectrogram|InverseMelScale|GriffinLim"
    - from: "src/small_dataset_audio/models/vae.py"
      to: "src/small_dataset_audio/audio/spectrogram.py"
      via: "pad time dim to multiple of 16 for stride-2 convolutions"
      pattern: "pad.*16|F\\.pad"
    - from: "src/small_dataset_audio/models/losses.py"
      to: "src/small_dataset_audio/models/vae.py"
      via: "loss consumes mu/logvar from encoder"
      pattern: "mu|logvar"
---

<objective>
Build the mel-spectrogram representation layer and convolutional VAE model with loss function.

Purpose: These are the core neural network components that everything else in Phase 3 depends on. The spectrogram module converts between waveforms and the representation space the VAE operates on. The VAE model encodes/decodes mel spectrograms through a 64-dimensional latent space. The loss function includes KL annealing and free bits to prevent posterior collapse.

Output: Three modules -- `audio/spectrogram.py` (waveform <-> mel conversion), `models/vae.py` (convolutional VAE), `models/losses.py` (VAE loss with KL annealing).
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-core-training-engine/03-RESEARCH.md
@src/small_dataset_audio/audio/__init__.py
@src/small_dataset_audio/audio/io.py
@src/small_dataset_audio/models/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Mel spectrogram representation layer</name>
  <files>src/small_dataset_audio/audio/spectrogram.py</files>
  <action>
Create `audio/spectrogram.py` with:

1. **SpectrogramConfig** dataclass:
   - `sample_rate: int = 48_000`
   - `n_fft: int = 2048`
   - `hop_length: int = 512`
   - `n_mels: int = 128`
   - `f_min: float = 0.0`
   - `f_max: float | None = None` (defaults to sample_rate / 2)
   - `power: float = 2.0`

2. **AudioSpectrogram** class:
   - `__init__(self, config: SpectrogramConfig | None = None)` -- lazy-imports torchaudio.transforms (MelSpectrogram, InverseMelScale, GriffinLim). Creates all three transforms once, storing on self. InverseMelScale uses `n_stft = n_fft // 2 + 1`. GriffinLim uses `n_iter=64`.
   - `waveform_to_mel(self, waveform: torch.Tensor) -> torch.Tensor` -- input shape `[B, 1, samples]`, squeeze channel dim, apply MelSpectrogram, apply `torch.log1p(mel)` normalization, unsqueeze back to `[B, 1, n_mels, time]`.
   - `mel_to_waveform(self, mel_log: torch.Tensor) -> torch.Tensor` -- input `[B, 1, n_mels, time]`, squeeze, apply `torch.expm1(clamp(min=0))` denormalization, run InverseMelScale on **CPU** (torch.linalg.lstsq issues on MPS), run GriffinLim, unsqueeze to `[B, 1, samples]`.
   - `get_mel_shape(self, num_samples: int) -> tuple[int, int]` -- returns `(n_mels, time_frames)` for a given number of audio samples, useful for model architecture sizing.

Follow project patterns: lazy torch/torchaudio imports inside `__init__`, docstrings matching Phase 2 style. Use `from __future__ import annotations`.

Key technical detail: 1 second at 48kHz (48000 samples) with hop_length=512 produces 128x94 mel spectrograms (94 = ceil(48000/512) + 1 for padding).
  </action>
  <verify>
Run: `cd "/Users/taylorbrook/Dev/Small DataSet Audio" && uv run python -c "
from small_dataset_audio.audio.spectrogram import SpectrogramConfig, AudioSpectrogram
import torch
spec = AudioSpectrogram()
# Round-trip test: waveform -> mel -> waveform
waveform = torch.randn(2, 1, 48000)  # batch of 2, mono, 1 second
mel = spec.waveform_to_mel(waveform)
print(f'Mel shape: {mel.shape}')  # [2, 1, 128, ~94]
assert mel.shape[0] == 2 and mel.shape[1] == 1 and mel.shape[2] == 128
waveform_back = spec.mel_to_waveform(mel)
print(f'Waveform back shape: {waveform_back.shape}')  # [2, 1, ~48000]
assert waveform_back.shape[0] == 2 and waveform_back.shape[1] == 1
print(f'Mel time frames: {spec.get_mel_shape(48000)}')
print('PASS')
"` -- should print shapes and PASS.
  </verify>
  <done>AudioSpectrogram converts waveforms to log-mel spectrograms and back. Round-trip preserves batch dimension and produces correct shapes. InverseMelScale runs on CPU.</done>
</task>

<task type="auto">
  <name>Task 2: Convolutional VAE model and loss function</name>
  <files>src/small_dataset_audio/models/vae.py, src/small_dataset_audio/models/losses.py</files>
  <action>
**models/vae.py** -- Create the convolutional VAE:

1. **ConvEncoder(nn.Module)**:
   - `__init__(self, latent_dim: int = 64, dropout: float = 0.2)` -- 4 Conv2d blocks (1->32->64->128->256), each with stride=2, padding=1, BatchNorm2d, ReLU, Dropout2d. Compute `flatten_dim` dynamically in first forward pass (store via register_buffer or lazy init) to handle variable mel shapes. Two linear heads: `fc_mu` and `fc_logvar` mapping flatten_dim -> latent_dim.
   - `forward(self, x)` -- Pad time dimension to multiple of 16 (`F.pad(x, (0, pad_size))`), run convolutions, flatten, return `(mu, logvar)`. Store `_padded_shape` for decoder to know the target shape.

2. **ConvDecoder(nn.Module)**:
   - `__init__(self, latent_dim: int = 64, dropout: float = 0.2)` -- Linear from latent_dim to flatten_dim, then 4 ConvTranspose2d blocks (256->128->64->32->1) with stride=2, padding=1, output_padding=1, BatchNorm2d, ReLU, Dropout2d. Final layer uses Sigmoid (output is log1p-normalized mel, always >= 0).
   - `forward(self, z, target_shape)` -- Linear, reshape, transpose convolutions, crop to target_shape (the original mel spectrogram shape before padding).

3. **ConvVAE(nn.Module)**:
   - `__init__(self, latent_dim: int = 64, dropout: float = 0.2)`
   - `encode(self, x) -> (mu, logvar)` -- delegates to encoder
   - `reparameterize(self, mu, logvar) -> z` -- standard reparameterization trick: `z = mu + std * eps` where `std = exp(0.5 * logvar)`, `eps = torch.randn_like(std)`
   - `decode(self, z, target_shape=None) -> recon` -- delegates to decoder. When target_shape is None (generation mode), use default shape for 1-second audio.
   - `forward(self, x) -> (recon, mu, logvar)` -- encode, reparameterize, decode. Returns reconstruction with same shape as input.
   - Property: `latent_dim`
   - Method: `sample(self, num_samples, device) -> tensor` -- generate from random latent vectors (for preview generation).

Use float32 throughout (no float16 on MPS). Include gradient clipping note in docstring. The model should have ~3.1M parameters total. Use `from __future__ import annotations` and lazy imports for torch.nn where appropriate (though since this is a model file, direct imports of torch/nn are fine).

**models/losses.py** -- Create loss functions:

1. **vae_loss(recon, target, mu, logvar, kl_weight=1.0, free_bits=0.5) -> (total, recon_loss, kl_loss)**:
   - Reconstruction: `F.mse_loss(recon, target, reduction='mean')`
   - KL per dimension: `-0.5 * (1 + logvar - mu.pow(2) - logvar.exp())`
   - Free bits: `torch.clamp(kl_per_dim, min=free_bits)` per dimension
   - KL loss: sum over latent dims, mean over batch
   - Total: `recon_loss + kl_weight * kl_loss`

2. **get_kl_weight(epoch, total_epochs, warmup_fraction=0.3) -> float**:
   - Linear annealing from 0 to 1 over first warmup_fraction of training
   - Returns `min(1.0, epoch / warmup_epochs)`

3. **compute_kl_divergence(mu, logvar) -> float**:
   - Raw KL divergence (without free bits) for monitoring posterior collapse
   - Returns scalar mean KL value
  </action>
  <verify>
Run: `cd "/Users/taylorbrook/Dev/Small DataSet Audio" && uv run python -c "
from small_dataset_audio.models.vae import ConvVAE
from small_dataset_audio.models.losses import vae_loss, get_kl_weight, compute_kl_divergence
import torch

model = ConvVAE(latent_dim=64, dropout=0.2)
total_params = sum(p.numel() for p in model.parameters())
print(f'Total params: {total_params:,}')

# Forward pass
x = torch.randn(4, 1, 128, 94)  # batch of 4 mel spectrograms
recon, mu, logvar = model(x)
print(f'Input shape: {x.shape}, Output shape: {recon.shape}')
assert recon.shape == x.shape, f'Shape mismatch: {recon.shape} vs {x.shape}'

# Loss
total, recon_loss, kl_loss = vae_loss(recon, x, mu, logvar, kl_weight=0.5)
print(f'Total: {total.item():.4f}, Recon: {recon_loss.item():.4f}, KL: {kl_loss.item():.4f}')

# KL weight schedule
assert get_kl_weight(0, 100) == 0.0
assert get_kl_weight(15, 100, warmup_fraction=0.3) == 0.5
assert get_kl_weight(50, 100) == 1.0

# Raw KL
raw_kl = compute_kl_divergence(mu, logvar)
print(f'Raw KL: {raw_kl:.4f}')

# Sample generation
samples = model.sample(2, torch.device('cpu'))
print(f'Sample shape: {samples.shape}')
print('PASS')
"` -- should show ~3M params, matching shapes, valid losses, and PASS.
  </verify>
  <done>ConvVAE encodes mel spectrograms to 64-dim latent space and decodes back with exact shape match. Loss function computes reconstruction + KL with free bits and annealing. Model has approximately 3M parameters. Sample generation works from random latent vectors.</done>
</task>

</tasks>

<verification>
- `audio/spectrogram.py` exists with SpectrogramConfig and AudioSpectrogram
- `models/vae.py` exists with ConvVAE (encode, decode, reparameterize, sample, forward)
- `models/losses.py` exists with vae_loss, get_kl_weight, compute_kl_divergence
- Waveform -> mel -> waveform round-trip works
- VAE forward pass produces same-shape output
- Loss function returns valid scalar values
- KL annealing schedule follows linear warmup
</verification>

<success_criteria>
- AudioSpectrogram converts [B, 1, 48000] waveforms to [B, 1, 128, ~94] log-mel spectrograms and back
- ConvVAE forward pass: input [B, 1, 128, T] -> output [B, 1, 128, T] with exact shape match
- vae_loss returns (total_loss, recon_loss, kl_loss) tuple of finite scalars
- get_kl_weight returns 0.0 at epoch 0 and 1.0 after warmup
- No new dependencies added
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-training-engine/03-01-SUMMARY.md`
</output>
