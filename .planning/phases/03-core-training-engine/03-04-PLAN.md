---
phase: 03-core-training-engine
plan: 04
type: execute
wave: 3
depends_on: ["03-01", "03-02", "03-03"]
files_modified:
  - src/small_dataset_audio/training/loop.py
  - src/small_dataset_audio/training/runner.py
  - src/small_dataset_audio/training/__init__.py
  - src/small_dataset_audio/models/__init__.py
  - src/small_dataset_audio/audio/__init__.py
autonomous: false

must_haves:
  truths:
    - "User can train a VAE model on a dataset of audio files and see training progress"
    - "Training emits step-level metrics (train_loss, recon_loss, kl_loss) and epoch-level metrics (val_loss, overfitting_gap, KL divergence, ETA)"
    - "Audio previews are generated every N epochs during training"
    - "Checkpoints are saved automatically at regular intervals"
    - "User can cancel training and it saves a checkpoint before stopping"
    - "User can resume training from a checkpoint"
    - "Validation loss tracks within 20% of training loss (overfitting detection warns but continues)"
    - "KL divergence remains above 0.5 (posterior collapse prevented via annealing + free bits)"
    - "NaN detection skips bad gradient updates instead of crashing"
  artifacts:
    - path: "src/small_dataset_audio/training/loop.py"
      provides: "train_epoch, validate_epoch, train (full training orchestrator)"
      contains: "def train"
    - path: "src/small_dataset_audio/training/runner.py"
      provides: "TrainingRunner with start/cancel/resume/is_running"
      contains: "class TrainingRunner"
    - path: "src/small_dataset_audio/training/__init__.py"
      provides: "Public API re-exports for all training submodules"
      exports: ["TrainingConfig", "TrainingRunner", "MetricsHistory", "AudioTrainingDataset"]
    - path: "src/small_dataset_audio/models/__init__.py"
      provides: "Public API re-exports for models"
      exports: ["ConvVAE", "vae_loss"]
    - path: "src/small_dataset_audio/audio/__init__.py"
      provides: "Updated public API with spectrogram exports"
      exports: ["AudioSpectrogram", "SpectrogramConfig"]
  key_links:
    - from: "src/small_dataset_audio/training/loop.py"
      to: "src/small_dataset_audio/models/vae.py"
      via: "model forward pass in training step"
      pattern: "model\\(|model\\.encode|model\\.decode"
    - from: "src/small_dataset_audio/training/loop.py"
      to: "src/small_dataset_audio/models/losses.py"
      via: "vae_loss computation each step"
      pattern: "vae_loss"
    - from: "src/small_dataset_audio/training/loop.py"
      to: "src/small_dataset_audio/training/checkpoint.py"
      via: "save_checkpoint at intervals and on cancel"
      pattern: "save_checkpoint"
    - from: "src/small_dataset_audio/training/loop.py"
      to: "src/small_dataset_audio/training/preview.py"
      via: "generate_preview at preview intervals"
      pattern: "generate_preview"
    - from: "src/small_dataset_audio/training/loop.py"
      to: "src/small_dataset_audio/training/metrics.py"
      via: "emits StepMetrics and EpochMetrics via callback"
      pattern: "StepMetrics|EpochMetrics|callback"
    - from: "src/small_dataset_audio/training/runner.py"
      to: "src/small_dataset_audio/training/loop.py"
      via: "runs train() in background thread"
      pattern: "threading\\.Thread|train\\("
    - from: "src/small_dataset_audio/training/loop.py"
      to: "src/small_dataset_audio/audio/spectrogram.py"
      via: "waveform_to_mel conversion each batch"
      pattern: "waveform_to_mel|AudioSpectrogram"
---

<objective>
Build the training loop, training runner, and finalize all public API exports.

Purpose: This is the capstone plan that wires everything together. The training loop orchestrates forward passes, loss computation, gradient updates, checkpointing, preview generation, and metrics emission. The runner wraps the loop in a background thread with cancellation support (threading.Event). Public API exports make all Phase 3 modules accessible. This plan implements every remaining success criterion from the phase goal.

Output: `training/loop.py` (core training logic), `training/runner.py` (thread management), updated `__init__.py` files for models, training, and audio packages.
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-core-training-engine/03-RESEARCH.md
@.planning/phases/03-core-training-engine/03-CONTEXT.md
@.planning/phases/03-core-training-engine/03-01-SUMMARY.md
@.planning/phases/03-core-training-engine/03-02-SUMMARY.md
@.planning/phases/03-core-training-engine/03-03-SUMMARY.md
@src/small_dataset_audio/audio/spectrogram.py
@src/small_dataset_audio/models/vae.py
@src/small_dataset_audio/models/losses.py
@src/small_dataset_audio/training/config.py
@src/small_dataset_audio/training/dataset.py
@src/small_dataset_audio/training/metrics.py
@src/small_dataset_audio/training/checkpoint.py
@src/small_dataset_audio/training/preview.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Core training loop with metrics, checkpoints, and previews</name>
  <files>src/small_dataset_audio/training/loop.py</files>
  <action>
Create `training/loop.py` with:

1. **train_epoch(model, train_loader, optimizer, spectrogram, kl_weight, device, gradient_clip_norm, callback=None, cancel_event=None) -> dict**:
   - Iterates batches from train_loader.
   - Each step:
     a. Move waveform batch to device.
     b. Convert to mel spectrogram via `spectrogram.waveform_to_mel(batch)`.
     c. Forward pass: `recon, mu, logvar = model(mel)`.
     d. Compute loss: `total, recon_loss, kl_loss = vae_loss(recon, mel, mu, logvar, kl_weight, free_bits=config.free_bits)`.
     e. **NaN detection**: if `total.isnan()`, log warning, skip this step (do NOT call backward/step). Per research pitfall #4 for MPS stability.
     f. `optimizer.zero_grad()`, `total.backward()`.
     g. Gradient clipping: `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_norm)`.
     h. `optimizer.step()`.
     i. Emit StepMetrics via callback (if provided).
     j. Check cancel_event -- if set, return early with partial results.
   - Returns dict with `train_loss`, `recon_loss`, `kl_loss` (averages over all steps).

2. **validate_epoch(model, val_loader, spectrogram, kl_weight, device) -> dict**:
   - Model in eval mode, `torch.no_grad()`.
   - Iterates val_loader, computes same losses.
   - Computes raw KL divergence via `compute_kl_divergence(mu, logvar)` for posterior collapse monitoring.
   - Returns dict with `val_loss`, `val_recon_loss`, `val_kl_loss`, `kl_divergence`.

3. **train(config, file_paths, output_dir, device, callback=None, cancel_event=None, resume_checkpoint=None) -> dict**:
   This is the main training orchestrator. Steps:

   a. **Setup**:
      - Create output directories: `output_dir/checkpoints/`, `output_dir/previews/`.
      - Create AudioSpectrogram with SpectrogramConfig defaults.
      - Create ConvVAE with `config.latent_dim` and `config.regularization.dropout`.
      - Move model to device.
      - Create AdamW optimizer with `config.learning_rate` and `config.regularization.weight_decay`.
      - Create CosineAnnealingLR scheduler with `T_max=config.max_epochs`.
      - Initialize MetricsHistory.
      - If `resume_checkpoint` provided: load checkpoint, restore model/optimizer/scheduler/metrics state, set start_epoch and kl_weight from checkpoint. Per discretion: show summary of resumed state (log it).

   b. **Create data loaders**:
      - Optionally create AugmentationPipeline if config.regularization.augmentation_expansion > 0.
      - Call `create_data_loaders(file_paths, config, augmentation_pipeline)`.

   c. **Training loop** (epoch = start_epoch to config.max_epochs):
      - Compute `kl_weight = get_kl_weight(epoch, config.max_epochs, config.kl_warmup_fraction)`.
      - Call `train_epoch(...)`.
      - Call `validate_epoch(...)`.
      - Step scheduler.
      - Compute `overfitting_gap = (val_loss - train_loss) / train_loss`.
      - Compute ETA via MetricsHistory.
      - Create and emit EpochMetrics via callback. Include all: epoch, total_epochs, train_loss, val_loss, val_recon_loss, val_kl_loss, kl_divergence, overfitting_gap, learning_rate, eta_seconds, elapsed_seconds.
      - **Overfitting warning**: if overfitting_gap > 0.2, log a warning but continue training (per user locked decision: warn visually but continue, user decides when to stop).
      - **Posterior collapse warning**: if kl_divergence < 0.5, log a warning.
      - **Preview generation**: if `epoch % effective_preview_interval == 0`, call `generate_preview(...)`. Emit PreviewEvent via callback.
      - **Checkpoint saving**: if `epoch % config.checkpoint_interval == 0`, call `save_checkpoint(...)` then `manage_checkpoints(...)`.
      - **Cancel check**: if `cancel_event` is set, save an immediate checkpoint (per discretion: immediate save, don't wait for epoch to finish), emit TrainingCompleteEvent, return.

   d. **Finalize**:
      - Save final checkpoint.
      - Emit TrainingCompleteEvent with final metrics.
      - Return dict with `model`, `metrics_history`, `output_dir`, `best_checkpoint_path`.

Important implementation details:
- Use `time.time()` for step and epoch timing.
- Get current learning rate from `optimizer.param_groups[0]['lr']`.
- Device memory: if device is cuda, `torch.cuda.memory_allocated()` / 1e9. If mps, `torch.mps.current_allocated_memory()` / 1e9. Include in step metrics if needed for dashboard.
- All file operations wrapped in try/except per project error isolation pattern.
  </action>
  <verify>
Run a minimal training test: `cd "/Users/taylorbrook/Dev/Small DataSet Audio" && uv run python -c "
import torch
import tempfile
import threading
from pathlib import Path
from small_dataset_audio.training.loop import train
from small_dataset_audio.training.config import TrainingConfig, RegularizationConfig
from small_dataset_audio.training.metrics import StepMetrics, EpochMetrics, PreviewEvent

# Collect metrics
events = []
def on_event(event):
    events.append(event)
    if isinstance(event, EpochMetrics):
        print(f'  Epoch {event.epoch}: train={event.train_loss:.4f} val={event.val_loss:.4f} kl={event.kl_divergence:.4f} gap={event.overfitting_gap:.4f}')

# Minimal config: 3 epochs, small model
config = TrainingConfig(
    latent_dim=16,
    batch_size=4,
    max_epochs=3,
    learning_rate=1e-3,
    checkpoint_interval=2,
    preview_interval=2,
    preview_interval_short=1,
    regularization=RegularizationConfig(dropout=0.1, weight_decay=0.01, augmentation_expansion=0, gradient_clip_norm=1.0),
)

# Create synthetic audio files (WAV)
import soundfile as sf
import numpy as np
with tempfile.TemporaryDirectory() as tmpdir:
    data_dir = Path(tmpdir) / 'data'
    data_dir.mkdir()
    for i in range(6):
        audio = np.random.randn(48000).astype(np.float32) * 0.1
        sf.write(str(data_dir / f'test_{i}.wav'), audio, 48000)

    output_dir = Path(tmpdir) / 'output'
    file_paths = list(data_dir.glob('*.wav'))

    result = train(
        config=config,
        file_paths=file_paths,
        output_dir=output_dir,
        device=torch.device('cpu'),
        callback=on_event,
    )

    step_count = sum(1 for e in events if isinstance(e, StepMetrics))
    epoch_count = sum(1 for e in events if isinstance(e, EpochMetrics))
    preview_count = sum(1 for e in events if isinstance(e, PreviewEvent))
    print(f'Steps: {step_count}, Epochs: {epoch_count}, Previews: {preview_count}')
    assert epoch_count == 3
    assert preview_count >= 1  # at least 1 preview in 3 epochs

    # Check checkpoint exists
    ckpts = list((output_dir / 'checkpoints').glob('*.pt'))
    print(f'Checkpoints: {len(ckpts)}')
    assert len(ckpts) >= 1

    # Check preview WAV exists
    previews = list((output_dir / 'previews').glob('*.wav'))
    print(f'Preview WAVs: {len(previews)}')
    assert len(previews) >= 1

    print('PASS')
"` -- should train 3 epochs, emit metrics, save checkpoints and previews, print PASS.
  </verify>
  <done>Training loop orchestrates forward pass, loss computation, gradient clipping, NaN detection, checkpoint saving, preview generation, and metrics emission. Overfitting gap and KL divergence monitored with warnings. Cancel event triggers immediate checkpoint save.</done>
</task>

<task type="auto">
  <name>Task 2: Training runner with thread management and public API exports</name>
  <files>src/small_dataset_audio/training/runner.py, src/small_dataset_audio/training/__init__.py, src/small_dataset_audio/models/__init__.py, src/small_dataset_audio/audio/__init__.py</files>
  <action>
**training/runner.py** -- Create the TrainingRunner:

1. **TrainingRunner** class:
   - `__init__(self)`:
     - `self._thread: threading.Thread | None = None`
     - `self._cancel_event = threading.Event()`
     - `self._is_running: bool = False`
     - `self._last_error: Exception | None = None`
     - `self._result: dict | None = None`

   - `start(self, config: TrainingConfig, file_paths: list[Path], output_dir: Path, device, callback=None) -> None`:
     - Validates not already running (raise RuntimeError if so).
     - Clears cancel event and error.
     - Creates daemon thread targeting `_run_training`.
     - Starts thread, sets `_is_running = True`.

   - `_run_training(self, config, file_paths, output_dir, device, callback)`:
     - Wraps `train(...)` call from training/loop.py.
     - Passes `self._cancel_event` as the cancel_event parameter.
     - On success: stores result in `self._result`.
     - On exception: stores in `self._last_error`, logs traceback.
     - Always sets `self._is_running = False`.

   - `cancel(self) -> None`:
     - Sets `self._cancel_event`. Per discretion: immediate checkpoint save, don't wait for epoch.
     - Does NOT join the thread (let it finish its save operation).

   - `resume(self, config: TrainingConfig, file_paths: list[Path], output_dir: Path, device, checkpoint_path: Path, callback=None) -> None`:
     - Same as start() but passes `resume_checkpoint=checkpoint_path` to train().
     - Per discretion: the loop logs a summary of the resumed state.

   - `is_running(self) -> bool`
   - `last_error(self) -> Exception | None`
   - `result(self) -> dict | None`

   - `wait(self, timeout: float | None = None) -> bool`:
     - Joins thread with timeout. Returns True if thread completed.

**training/__init__.py** -- Update with all public API exports:
```python
from small_dataset_audio.training.config import (
    TrainingConfig, OverfittingPreset, RegularizationConfig,
    get_adaptive_config, get_effective_preview_interval,
)
from small_dataset_audio.training.dataset import (
    AudioTrainingDataset, create_data_loaders,
)
from small_dataset_audio.training.metrics import (
    StepMetrics, EpochMetrics, PreviewEvent, TrainingCompleteEvent,
    MetricsHistory, MetricsCallback,
)
from small_dataset_audio.training.checkpoint import (
    save_checkpoint, load_checkpoint, manage_checkpoints,
    get_best_checkpoint, list_checkpoints,
)
from small_dataset_audio.training.preview import (
    generate_preview, generate_reconstruction_preview, list_previews,
)
from small_dataset_audio.training.loop import train
from small_dataset_audio.training.runner import TrainingRunner
```
Include `__all__` listing all exports.

**models/__init__.py** -- Update:
```python
from small_dataset_audio.models.vae import ConvVAE, ConvEncoder, ConvDecoder
from small_dataset_audio.models.losses import vae_loss, get_kl_weight, compute_kl_divergence
```
Include `__all__`.

**audio/__init__.py** -- Add spectrogram exports to existing exports:
```python
from small_dataset_audio.audio.spectrogram import (
    SpectrogramConfig, AudioSpectrogram,
)
```
Add to existing `__all__` list.
  </action>
  <verify>
Run: `cd "/Users/taylorbrook/Dev/Small DataSet Audio" && uv run python -c "
import torch
import tempfile
import time
from pathlib import Path

# Test imports from public API
from small_dataset_audio.models import ConvVAE, vae_loss, get_kl_weight
from small_dataset_audio.training import (
    TrainingConfig, TrainingRunner, MetricsHistory,
    AudioTrainingDataset, get_adaptive_config, StepMetrics, EpochMetrics,
)
from small_dataset_audio.audio import AudioSpectrogram, SpectrogramConfig

# Test runner with cancel
import soundfile as sf
import numpy as np

events = []
def on_event(event):
    events.append(event)

config = TrainingConfig(
    latent_dim=16, batch_size=4, max_epochs=20,
    learning_rate=1e-3, checkpoint_interval=5, preview_interval=5,
    regularization=__import__('small_dataset_audio.training.config', fromlist=['RegularizationConfig']).RegularizationConfig(
        dropout=0.1, weight_decay=0.01, augmentation_expansion=0, gradient_clip_norm=1.0
    ),
)

with tempfile.TemporaryDirectory() as tmpdir:
    data_dir = Path(tmpdir) / 'data'
    data_dir.mkdir()
    for i in range(6):
        audio = np.random.randn(48000).astype(np.float32) * 0.1
        sf.write(str(data_dir / f'test_{i}.wav'), audio, 48000)

    output_dir = Path(tmpdir) / 'output'
    file_paths = list(data_dir.glob('*.wav'))

    runner = TrainingRunner()
    assert not runner.is_running()

    runner.start(config, file_paths, output_dir, torch.device('cpu'), callback=on_event)
    assert runner.is_running()

    # Let it run a couple epochs then cancel
    time.sleep(3)
    runner.cancel()
    runner.wait(timeout=10)

    assert not runner.is_running()
    epoch_count = sum(1 for e in events if isinstance(e, EpochMetrics))
    print(f'Epochs completed before cancel: {epoch_count}')
    assert epoch_count >= 1

    # Check that a checkpoint was saved on cancel
    ckpts = list((output_dir / 'checkpoints').glob('*.pt'))
    print(f'Checkpoints saved: {len(ckpts)}')
    assert len(ckpts) >= 1

    print('Runner cancel + checkpoint save works')
    print('PASS')
"` -- should start training, cancel, verify checkpoint saved, and PASS.
  </verify>
  <done>TrainingRunner manages training in a background thread with clean cancellation via threading.Event. Cancel triggers immediate checkpoint save. All Phase 3 modules are accessible via public API through their package __init__.py files.</done>
</task>

<task type="checkpoint:human-verify">
  <name>Task 3: Verify complete training pipeline end-to-end</name>
  <files>none</files>
  <action>
Human verifies the complete Phase 3 training engine by running the full pipeline test below. This checkpoint confirms that all components (mel spectrogram, VAE model, loss function, config presets, dataset splitting, metrics, checkpoints, previews, training loop, and runner) work together correctly.

What was built:
- Mel spectrogram layer (audio/spectrogram.py) converting waveforms to/from log-mel spectrograms
- Convolutional VAE (~3M params) with 64-dim latent space (models/vae.py)
- VAE loss with KL annealing + free bits (models/losses.py)
- Layered training config with 3 overfitting presets auto-adapting to dataset size (training/config.py)
- PyTorch Dataset with 1-second chunking and file-level validation split (training/dataset.py)
- Metrics collection via callback dataclasses (training/metrics.py)
- Checkpoint save/load with 3-recent + 1-best retention (training/checkpoint.py)
- Audio preview generation from VAE decoder (training/preview.py)
- Core training loop with NaN detection, gradient clipping, overfitting warnings (training/loop.py)
- Threaded training runner with cancel/resume (training/runner.py)
- Public API exports for all new modules

How to verify:
1. Run the following command:
```
cd "/Users/taylorbrook/Dev/Small DataSet Audio"
uv run python -c "
from small_dataset_audio.training import TrainingRunner, get_adaptive_config, EpochMetrics, PreviewEvent
from small_dataset_audio.training.preview import list_previews
from small_dataset_audio.training.checkpoint import list_checkpoints
import torch, tempfile, soundfile as sf, numpy as np
from pathlib import Path

config = get_adaptive_config(10)
print(f'Config: preset={config.preset}, epochs={config.max_epochs}, dropout={config.regularization.dropout}')
config.max_epochs = 10

with tempfile.TemporaryDirectory() as tmpdir:
    data_dir = Path(tmpdir) / 'data'
    data_dir.mkdir()
    for i in range(10):
        freq = 440 + i * 50
        t = np.linspace(0, 1, 48000)
        audio = (np.sin(2 * np.pi * freq * t) * 0.5).astype(np.float32)
        sf.write(str(data_dir / f'tone_{freq}hz.wav'), audio, 48000)

    output_dir = Path(tmpdir) / 'output'
    files = list(data_dir.glob('*.wav'))

    events = []
    runner = TrainingRunner()
    runner.start(config, files, output_dir, torch.device('cpu'), callback=lambda e: events.append(e))
    runner.wait(timeout=120)

    epochs = [e for e in events if isinstance(e, EpochMetrics)]
    previews_evt = [e for e in events if isinstance(e, PreviewEvent)]
    print(f'Epochs: {len(epochs)}, Previews: {len(previews_evt)}')
    if epochs:
        last = epochs[-1]
        print(f'Final: train={last.train_loss:.4f}, val={last.val_loss:.4f}, kl={last.kl_divergence:.4f}, gap={last.overfitting_gap:.4f}')

    ckpts = list_checkpoints(output_dir / 'checkpoints')
    preview_list = list_previews(output_dir / 'previews')
    print(f'Checkpoints on disk: {len(ckpts)}')
    print(f'Previews on disk: {len(preview_list)}')
"
```
2. Confirm: training completes 10 epochs, loss values are finite, at least 1 checkpoint and 1 preview exist
3. Optionally test with real audio files
  </action>
  <verify>User confirms the training pipeline works end-to-end by running the test command and checking output.</verify>
  <done>Phase 3 training engine verified: all components integrate correctly, training produces valid metrics, checkpoints save, and audio previews generate.</done>
</task>

</tasks>

<verification>
- `training/loop.py` exists with train_epoch, validate_epoch, train
- `training/runner.py` exists with TrainingRunner (start, cancel, resume, is_running, wait)
- `training/__init__.py` re-exports all public API from all training submodules
- `models/__init__.py` re-exports ConvVAE, vae_loss, get_kl_weight
- `audio/__init__.py` re-exports AudioSpectrogram, SpectrogramConfig
- Full training pipeline works end-to-end: files -> dataset -> model -> train -> checkpoints + previews
- Cancel triggers immediate checkpoint save
- Metrics callback receives StepMetrics, EpochMetrics, PreviewEvent
- Overfitting gap and KL divergence are monitored
</verification>

<success_criteria>
- Training completes on 6+ synthetic WAV files for 3+ epochs
- StepMetrics emitted each batch, EpochMetrics emitted each epoch
- At least 1 checkpoint .pt file saved
- At least 1 preview .wav file generated
- Cancel stops training and saves checkpoint
- All public API imports work: `from small_dataset_audio.models import ConvVAE`
- No new dependencies added
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-training-engine/03-04-SUMMARY.md`
</output>
