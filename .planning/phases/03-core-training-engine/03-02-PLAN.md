---
phase: 03-core-training-engine
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/small_dataset_audio/training/config.py
  - src/small_dataset_audio/training/dataset.py
  - src/small_dataset_audio/training/metrics.py
autonomous: true

must_haves:
  truths:
    - "Training configuration adapts regularization strength based on dataset size (5-50 stronger, 200-500 lighter)"
    - "Three overfitting presets exist: Conservative, Balanced, Aggressive with distinct parameter values"
    - "PyTorch Dataset loads audio files, chunks them to fixed 1-second segments, and converts to mel spectrograms"
    - "Validation split happens at file level (not chunk level) to prevent data leakage"
    - "Metrics system collects step-level and epoch-level metrics via callback pattern"
    - "ETA computation tracks elapsed time and estimates remaining training time"
  artifacts:
    - path: "src/small_dataset_audio/training/config.py"
      provides: "TrainingConfig, OverfittingPreset enum, RegularizationConfig, get_adaptive_config"
      contains: "class TrainingConfig"
    - path: "src/small_dataset_audio/training/dataset.py"
      provides: "AudioTrainingDataset (PyTorch Dataset), create_data_loaders with file-level split"
      contains: "class AudioTrainingDataset"
    - path: "src/small_dataset_audio/training/metrics.py"
      provides: "StepMetrics, EpochMetrics, PreviewEvent, MetricsHistory, MetricsCallback type"
      contains: "class MetricsHistory"
  key_links:
    - from: "src/small_dataset_audio/training/dataset.py"
      to: "src/small_dataset_audio/audio/io.py"
      via: "load_audio for waveform loading"
      pattern: "load_audio|get_metadata"
    - from: "src/small_dataset_audio/training/config.py"
      to: "src/small_dataset_audio/training/dataset.py"
      via: "TrainingConfig drives dataset split ratios and augmentation settings"
      pattern: "TrainingConfig|val_fraction"
    - from: "src/small_dataset_audio/training/metrics.py"
      to: "training loop (Plan 04)"
      via: "callback pattern - loop emits StepMetrics/EpochMetrics"
      pattern: "MetricsCallback|StepMetrics|EpochMetrics"
---

<objective>
Build training infrastructure: configuration system with overfitting presets, PyTorch Dataset wrapper for audio training, and metrics collection system.

Purpose: These three modules provide the data and configuration backbone that the training loop (Plan 04) consumes. The config system implements the user's locked decision for layered overfitting controls (auto defaults + presets + advanced toggles). The dataset wrapper handles the critical file-level validation split and 1-second chunking. The metrics system defines the callback-based event types that decouple training from display.

Output: Three modules -- `training/config.py` (layered training config), `training/dataset.py` (PyTorch Dataset with chunking and splitting), `training/metrics.py` (typed metrics and history).
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-core-training-engine/03-RESEARCH.md
@.planning/phases/03-core-training-engine/03-CONTEXT.md
@src/small_dataset_audio/audio/io.py
@src/small_dataset_audio/audio/augmentation.py
@src/small_dataset_audio/audio/preprocessing.py
@src/small_dataset_audio/data/dataset.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Training configuration with overfitting presets</name>
  <files>src/small_dataset_audio/training/config.py</files>
  <action>
Create `training/config.py` with:

1. **OverfittingPreset** (enum):
   - `CONSERVATIVE` -- for 5-50 files: dropout=0.4, weight_decay=0.05, augmentation_expansion=15, max_epochs=100, learning_rate=5e-4, kl_warmup_fraction=0.5, gradient_clip_norm=0.5
   - `BALANCED` -- for 50-200 files: dropout=0.2, weight_decay=0.01, augmentation_expansion=10, max_epochs=200, learning_rate=1e-3, kl_warmup_fraction=0.3, gradient_clip_norm=1.0
   - `AGGRESSIVE` -- for 200-500 files: dropout=0.1, weight_decay=0.001, augmentation_expansion=5, max_epochs=500, learning_rate=2e-3, kl_warmup_fraction=0.2, gradient_clip_norm=5.0

2. **RegularizationConfig** dataclass:
   - `dropout: float = 0.2`
   - `weight_decay: float = 0.01`
   - `augmentation_expansion: int = 10`
   - `gradient_clip_norm: float = 1.0`

3. **TrainingConfig** dataclass:
   - `latent_dim: int = 64`
   - `batch_size: int = 32`
   - `max_epochs: int = 200`
   - `learning_rate: float = 1e-3`
   - `kl_warmup_fraction: float = 0.3`
   - `free_bits: float = 0.5`
   - `val_fraction: float = 0.2` (overridden by adaptive split)
   - `chunk_duration_s: float = 1.0` (1-second chunks)
   - `checkpoint_interval: int = 10` (save every N epochs)
   - `preview_interval: int = 5` (generate preview every N epochs, per discretion recommendation)
   - `preview_interval_short: int = 2` (for runs < 50 epochs)
   - `max_checkpoints: int = 3` (retain 3 most recent + 1 best = 4 total, per discretion recommendation)
   - `preset: OverfittingPreset = OverfittingPreset.BALANCED`
   - `regularization: RegularizationConfig` (field with default_factory)
   - `device: str = "auto"` (resolves via Phase 1 hardware)
   - `num_workers: int = 0` (DataLoader workers; 0 = main process, safest cross-platform)

4. **get_adaptive_config(file_count: int) -> TrainingConfig**:
   - Select preset based on dataset size: <50 -> CONSERVATIVE, <200 -> BALANCED, >=200 -> AGGRESSIVE
   - Set val_fraction adaptively: <10 files: 0.5, 10-49: 0.3, 50-199: 0.2, 200+: 0.1 (per user locked decision: automatic validation split based on dataset size)
   - Set batch_size = min(32, estimated_chunks // 4) to avoid batch noise on tiny datasets
   - Apply preset parameters to RegularizationConfig and top-level fields
   - Return fully configured TrainingConfig

5. **get_effective_preview_interval(config: TrainingConfig) -> int**:
   - Returns `preview_interval_short` if max_epochs < 50, else `preview_interval`

Follow project dataclass patterns. Use `from __future__ import annotations`. All fields documented.
  </action>
  <verify>
Run: `cd "/Users/taylorbrook/Dev/Small DataSet Audio" && uv run python -c "
from small_dataset_audio.training.config import (
    TrainingConfig, OverfittingPreset, RegularizationConfig,
    get_adaptive_config, get_effective_preview_interval,
)

# Default config
cfg = TrainingConfig()
print(f'Default: lr={cfg.learning_rate}, epochs={cfg.max_epochs}, preset={cfg.preset}')

# Adaptive for tiny dataset (10 files)
tiny = get_adaptive_config(10)
print(f'Tiny (10): preset={tiny.preset}, dropout={tiny.regularization.dropout}, val_frac={tiny.val_fraction}, epochs={tiny.max_epochs}')
assert tiny.preset == OverfittingPreset.CONSERVATIVE
assert tiny.val_fraction == 0.3
assert tiny.regularization.dropout == 0.4

# Adaptive for medium dataset (100 files)
med = get_adaptive_config(100)
print(f'Medium (100): preset={med.preset}, val_frac={med.val_fraction}')
assert med.preset == OverfittingPreset.BALANCED
assert med.val_fraction == 0.2

# Adaptive for large dataset (300 files)
large = get_adaptive_config(300)
print(f'Large (300): preset={large.preset}, val_frac={large.val_fraction}')
assert large.preset == OverfittingPreset.AGGRESSIVE
assert large.val_fraction == 0.1

# Preview interval
assert get_effective_preview_interval(tiny) == 2  # <50 epochs
assert get_effective_preview_interval(med) == 5   # >=50 epochs
print('PASS')
"` -- should print configs and PASS.
  </verify>
  <done>TrainingConfig with 3 overfitting presets auto-adapts to dataset size. Validation fraction scales with file count. Preview intervals adjust for short runs. All preset parameters match research recommendations.</done>
</task>

<task type="auto">
  <name>Task 2: PyTorch training dataset and metrics collection</name>
  <files>src/small_dataset_audio/training/dataset.py, src/small_dataset_audio/training/metrics.py</files>
  <action>
**training/dataset.py** -- Create PyTorch Dataset wrapper:

1. **AudioTrainingDataset(torch.utils.data.Dataset)**:
   - `__init__(self, file_paths: list[Path], chunk_samples: int = 48000, sample_rate: int = 48000, augmentation_pipeline=None)`:
     - Stores file_paths, chunk_samples, sample_rate, augmentation
     - Builds chunk index: pre-scans files via `get_metadata()` to determine `(file_idx, chunk_start_sample)` pairs. Each file produces `ceil(total_samples / chunk_samples)` chunks.
     - Uses lazy imports for torch and audio modules.
   - `__len__() -> int` -- returns total chunk count
   - `__getitem__(idx) -> torch.Tensor` -- loads audio file for the chunk's file_idx via `load_audio()`, extracts the chunk slice `[chunk_start:chunk_start+chunk_samples]`, zero-pads if shorter than chunk_samples, returns mono waveform tensor `[1, chunk_samples]`. If augmentation pipeline provided, applies it with 50% probability per chunk.
   - Important: do NOT convert to mel spectrogram in the dataset. Return raw waveform chunks. The training loop handles mel conversion (keeps spectrogram transform on GPU for efficiency).

2. **create_data_loaders(files: list[Path], config: TrainingConfig, augmentation_pipeline=None) -> tuple[DataLoader, DataLoader]**:
   - Split files at FILE level (not chunk level) to prevent data leakage (per user locked decision and research pitfall #7)
   - Use `config.val_fraction` for split ratio
   - Shuffle files, then split: first N for training, rest for validation
   - Create AudioTrainingDataset for train (with augmentation) and val (WITHOUT augmentation -- per research anti-pattern)
   - Return `(train_loader, val_loader)` with config.batch_size, shuffle=True for train, shuffle=False for val, num_workers=config.num_workers, pin_memory=(config.device != 'cpu')
   - Use a fixed random seed for reproducible splits

**training/metrics.py** -- Create metrics system:

1. **StepMetrics** dataclass -- emitted every training step:
   - epoch, step, total_steps, train_loss, recon_loss, kl_loss, kl_weight, learning_rate, step_time_s (all typed fields)

2. **EpochMetrics** dataclass -- emitted every epoch:
   - epoch, total_epochs, train_loss, val_loss, val_recon_loss, val_kl_loss, kl_divergence (raw KL for posterior collapse monitoring), overfitting_gap ((val-train)/train), learning_rate, eta_seconds, elapsed_seconds

3. **PreviewEvent** dataclass -- emitted when audio preview generated:
   - epoch, audio_path (str), sample_rate (int)

4. **TrainingCompleteEvent** dataclass:
   - total_epochs, total_time_s, final_train_loss, final_val_loss, best_val_loss, best_epoch

5. **MetricsCallback** type alias: `Callable[[StepMetrics | EpochMetrics | PreviewEvent | TrainingCompleteEvent], None]`

6. **MetricsHistory** class:
   - Stores lists of StepMetrics and EpochMetrics
   - `add_step(metrics: StepMetrics)` / `add_epoch(metrics: EpochMetrics)`
   - `get_loss_curves() -> dict[str, list[float]]` -- returns train_losses, val_losses, kl_losses, recon_losses keyed by name for plotting
   - `get_best_epoch() -> int` -- epoch with lowest val_loss
   - `compute_eta(current_epoch, total_epochs) -> float` -- estimate remaining seconds based on average epoch time
   - `to_dict() -> dict` / `from_dict(data) -> MetricsHistory` -- serialization for checkpoint inclusion
   - `is_overfitting(threshold: float = 0.2) -> bool` -- True if latest overfitting_gap > threshold

Use `from __future__ import annotations`. No torch dependency in metrics.py (it's pure Python dataclasses + typing).
  </action>
  <verify>
Run: `cd "/Users/taylorbrook/Dev/Small DataSet Audio" && uv run python -c "
from small_dataset_audio.training.metrics import (
    StepMetrics, EpochMetrics, PreviewEvent, TrainingCompleteEvent,
    MetricsHistory,
)

# Metrics history
history = MetricsHistory()
history.add_step(StepMetrics(epoch=0, step=0, total_steps=100, train_loss=1.0, recon_loss=0.8, kl_loss=0.2, kl_weight=0.0, learning_rate=1e-3, step_time_s=0.1))
history.add_epoch(EpochMetrics(epoch=0, total_epochs=100, train_loss=1.0, val_loss=1.1, val_recon_loss=0.9, val_kl_loss=0.2, kl_divergence=0.8, overfitting_gap=0.1, learning_rate=1e-3, eta_seconds=900.0, elapsed_seconds=10.0))

curves = history.get_loss_curves()
print(f'Loss curves keys: {sorted(curves.keys())}')

# Serialization round-trip
data = history.to_dict()
restored = MetricsHistory.from_dict(data)
assert len(restored.epoch_metrics) == 1
print(f'Overfitting: {history.is_overfitting(threshold=0.2)}')  # False (gap=0.1)
print(f'Best epoch: {history.get_best_epoch()}')

# Dataset test
import torch
from pathlib import Path
from small_dataset_audio.training.dataset import AudioTrainingDataset
print('Dataset class imported successfully')
print('PASS')
"` -- should print keys, overfitting status, and PASS.
  </verify>
  <done>TrainingConfig adapts to dataset size via presets. AudioTrainingDataset chunks audio files into fixed-length segments with file-level validation split. MetricsHistory collects training metrics with serialization, loss curve extraction, and overfitting detection.</done>
</task>

</tasks>

<verification>
- `training/config.py` exists with TrainingConfig, OverfittingPreset, RegularizationConfig, get_adaptive_config
- `training/dataset.py` exists with AudioTrainingDataset and create_data_loaders
- `training/metrics.py` exists with StepMetrics, EpochMetrics, PreviewEvent, MetricsHistory
- Adaptive config selects correct preset for different dataset sizes
- File-level split in create_data_loaders prevents data leakage
- MetricsHistory serializes/deserializes for checkpoint inclusion
- Overfitting detection threshold matches the 20% requirement
</verification>

<success_criteria>
- get_adaptive_config(10) returns CONSERVATIVE preset with val_fraction=0.3
- get_adaptive_config(100) returns BALANCED preset with val_fraction=0.2
- get_adaptive_config(300) returns AGGRESSIVE preset with val_fraction=0.1
- AudioTrainingDataset produces fixed-size waveform chunks from variable-length audio
- MetricsHistory.is_overfitting() returns True when gap > 0.2
- MetricsHistory round-trips through to_dict/from_dict
- No new dependencies added
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-training-engine/03-02-SUMMARY.md`
</output>
