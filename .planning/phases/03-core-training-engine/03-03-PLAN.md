---
phase: 03-core-training-engine
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/small_dataset_audio/training/checkpoint.py
  - src/small_dataset_audio/training/preview.py
autonomous: true

must_haves:
  truths:
    - "Checkpoint saves model, optimizer, scheduler, epoch, step, KL weight, config, and metrics history"
    - "Checkpoint loads and restores all training state for exact resume"
    - "Only 3 most recent + 1 best-val-loss checkpoints are retained (4 max)"
    - "Audio preview generates WAV files from random latent vectors via the VAE decoder"
    - "Preview audio uses InverseMelScale + GriffinLim on CPU for mel-to-waveform conversion"
  artifacts:
    - path: "src/small_dataset_audio/training/checkpoint.py"
      provides: "save_checkpoint, load_checkpoint, manage_checkpoints, get_best_checkpoint"
      contains: "def save_checkpoint"
    - path: "src/small_dataset_audio/training/preview.py"
      provides: "generate_preview function producing WAV files from VAE latent samples"
      contains: "def generate_preview"
  key_links:
    - from: "src/small_dataset_audio/training/checkpoint.py"
      to: "torch.save/torch.load"
      via: "PyTorch checkpoint serialization"
      pattern: "torch\\.save|torch\\.load"
    - from: "src/small_dataset_audio/training/preview.py"
      to: "src/small_dataset_audio/audio/spectrogram.py"
      via: "mel_to_waveform for audio reconstruction"
      pattern: "mel_to_waveform|AudioSpectrogram"
    - from: "src/small_dataset_audio/training/preview.py"
      to: "src/small_dataset_audio/models/vae.py"
      via: "model.decode(z) for latent-to-mel generation"
      pattern: "model\\.decode|model\\.sample"
---

<objective>
Build checkpoint management and audio preview generation modules.

Purpose: Checkpoints implement the user's locked decision for automatic saving at regular intervals with resume capability. The retention policy (3 recent + 1 best = 4 max) prevents disk bloat per discretion recommendation. Audio previews implement the locked decision for generating audio every N epochs that users can listen to in a scrollable list -- enabling them to hear the model improve over time.

Output: Two modules -- `training/checkpoint.py` (save/load/manage checkpoints), `training/preview.py` (generate WAV previews from VAE decoder).
</objective>

<execution_context>
@/Users/taylorbrook/.claude/get-shit-done/workflows/execute-plan.md
@/Users/taylorbrook/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-core-training-engine/03-RESEARCH.md
@.planning/phases/03-core-training-engine/03-CONTEXT.md
@src/small_dataset_audio/audio/spectrogram.py
@src/small_dataset_audio/models/vae.py
@src/small_dataset_audio/models/losses.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Checkpoint save, load, and retention management</name>
  <files>src/small_dataset_audio/training/checkpoint.py</files>
  <action>
Create `training/checkpoint.py` with:

1. **CHECKPOINT_VERSION = 1** -- version number for forward compatibility.

2. **save_checkpoint(path, model, optimizer, scheduler, epoch, step, train_loss, val_loss, kl_weight, training_config, spectrogram_config, metrics_history_dict) -> Path**:
   - Creates a checkpoint dict containing:
     - `version`: CHECKPOINT_VERSION
     - `epoch`, `step`: training progress
     - `model_state_dict`: model.state_dict()
     - `optimizer_state_dict`: optimizer.state_dict()
     - `scheduler_state_dict`: scheduler.state_dict()
     - `train_loss`, `val_loss`, `kl_weight`: current training state
     - `training_config`: config as dict (via dataclasses.asdict or manual)
     - `spectrogram_config`: spectrogram config as dict
     - `metrics_history`: metrics_history_dict (from MetricsHistory.to_dict())
   - Uses `torch.save(checkpoint, path)`.
   - Returns the path written.
   - Naming convention: `checkpoint_epoch{epoch:04d}.pt`

3. **load_checkpoint(path, model, optimizer=None, scheduler=None, device='cpu') -> dict**:
   - Loads via `torch.load(path, map_location=device, weights_only=False)`.
   - Restores `model.load_state_dict(checkpoint['model_state_dict'])`.
   - If optimizer provided, restores optimizer state.
   - If scheduler provided, restores scheduler state.
   - Returns the full checkpoint dict (caller reads epoch, step, kl_weight, metrics, config).
   - Validates checkpoint version compatibility.

4. **manage_checkpoints(checkpoint_dir: Path, max_recent: int = 3) -> list[Path]**:
   - Lists all `checkpoint_epoch*.pt` files in directory, sorted by epoch number.
   - Identifies the best checkpoint (lowest val_loss) by loading each header.
   - Keeps: the `max_recent` most recent + the 1 best-val-loss checkpoint.
   - Deletes all others.
   - Returns list of deleted paths.
   - To avoid loading full checkpoints just for val_loss: save a small JSON sidecar `checkpoint_epoch{N:04d}.meta.json` alongside each .pt with `{epoch, val_loss, train_loss}`. manage_checkpoints reads these instead.

5. **get_best_checkpoint(checkpoint_dir: Path) -> Path | None**:
   - Returns path to the checkpoint with lowest val_loss (reads .meta.json files).
   - Returns None if no checkpoints exist.

6. **list_checkpoints(checkpoint_dir: Path) -> list[dict]**:
   - Returns list of `{path, epoch, train_loss, val_loss}` dicts sorted by epoch.
   - Reads from .meta.json sidecar files.

Use lazy torch imports. Follow project error handling pattern (log warnings, don't crash). Ensure checkpoint_dir is created if it doesn't exist.
  </action>
  <verify>
Run: `cd "/Users/taylorbrook/Dev/Small DataSet Audio" && uv run python -c "
import torch
import tempfile
from pathlib import Path
from small_dataset_audio.training.checkpoint import (
    save_checkpoint, load_checkpoint, manage_checkpoints,
    get_best_checkpoint, list_checkpoints, CHECKPOINT_VERSION,
)
from small_dataset_audio.models.vae import ConvVAE

model = ConvVAE(latent_dim=64)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

with tempfile.TemporaryDirectory() as tmpdir:
    ckpt_dir = Path(tmpdir)

    # Save 5 checkpoints with varying val_loss
    for epoch in range(5):
        val_loss = 1.0 - epoch * 0.1 if epoch < 3 else 1.0 + epoch * 0.1
        path = ckpt_dir / f'checkpoint_epoch{epoch:04d}.pt'
        save_checkpoint(
            path=path, model=model, optimizer=optimizer, scheduler=scheduler,
            epoch=epoch, step=epoch*10, train_loss=0.5,
            val_loss=val_loss, kl_weight=0.5,
            training_config={'lr': 1e-3},
            spectrogram_config={'n_mels': 128},
            metrics_history_dict={'step_metrics': [], 'epoch_metrics': []},
        )

    # List checkpoints
    ckpts = list_checkpoints(ckpt_dir)
    print(f'Checkpoints: {len(ckpts)}')
    assert len(ckpts) == 5

    # Best checkpoint
    best = get_best_checkpoint(ckpt_dir)
    print(f'Best: {best.name}')
    assert 'epoch0002' in best.name  # epoch 2 has lowest val_loss (0.8)

    # Manage: keep 3 recent + 1 best
    deleted = manage_checkpoints(ckpt_dir, max_recent=3)
    remaining = list_checkpoints(ckpt_dir)
    print(f'Deleted: {len(deleted)}, Remaining: {len(remaining)}')
    assert len(remaining) == 4  # 3 recent (2,3,4) + 1 best (2, already in recent)... adjust test

    # Load checkpoint
    ckpt = load_checkpoint(best, model, optimizer, scheduler, device='cpu')
    print(f'Loaded epoch: {ckpt[\"epoch\"]}, version: {ckpt[\"version\"]}')
    assert ckpt['version'] == CHECKPOINT_VERSION

    print('PASS')
"` -- should manage checkpoints correctly and PASS.
  </verify>
  <done>Checkpoint system saves full training state including model, optimizer, scheduler, metrics, and config. Retention policy keeps 3 most recent + 1 best-val-loss (4 max). JSON sidecar files enable fast checkpoint scanning without loading full .pt files.</done>
</task>

<task type="auto">
  <name>Task 2: Audio preview generation from VAE decoder</name>
  <files>src/small_dataset_audio/training/preview.py</files>
  <action>
Create `training/preview.py` with:

1. **generate_preview(model, spectrogram, output_dir, epoch, device, num_samples=1, sample_rate=48000) -> list[Path]**:
   - Sets model to eval mode.
   - With `torch.no_grad()`:
     - Sample random latent vectors: `z = torch.randn(num_samples, model.latent_dim, device=device)`
     - Decode: `mel_recon = model.decode(z)` (or `model.sample(num_samples, device)`)
     - Convert mel to waveform via `spectrogram.mel_to_waveform(mel_recon.cpu())` -- MUST be on CPU for InverseMelScale.
     - For each waveform: peak-normalize (`audio / max(abs(audio))`), convert to numpy, save as WAV via `soundfile.write(path, audio_np, sample_rate, subtype='PCM_16')`.
     - Filename: `preview_epoch{epoch:04d}_{i:02d}.wav`
   - Restores model to train mode.
   - Creates output_dir if it doesn't exist.
   - Returns list of saved WAV paths.

2. **generate_reconstruction_preview(model, spectrogram, sample_batch, output_dir, epoch, device, sample_rate=48000) -> list[Path]**:
   - Takes a batch of real mel spectrograms, runs through VAE (encode -> reparameterize -> decode), saves both original and reconstruction as WAV for comparison.
   - Filenames: `recon_epoch{epoch:04d}_orig_{i:02d}.wav` and `recon_epoch{epoch:04d}_recon_{i:02d}.wav`
   - Useful for monitoring reconstruction quality alongside random samples.
   - Only saves the first `min(2, batch_size)` items to limit disk usage.

3. **list_previews(preview_dir: Path) -> list[dict]**:
   - Scans preview_dir for `preview_epoch*.wav` files.
   - Returns list of `{epoch: int, path: Path, filename: str}` sorted by epoch.
   - Supports the locked decision: "Keep all previews visible in a scrollable list with epoch labels".

Per-file try/except for robustness (one failed preview doesn't stop training). Use lazy imports for torch, soundfile, numpy. Follow Phase 2 batch error isolation pattern.
  </action>
  <verify>
Run: `cd "/Users/taylorbrook/Dev/Small DataSet Audio" && uv run python -c "
import torch
import tempfile
from pathlib import Path
from small_dataset_audio.training.preview import generate_preview, list_previews
from small_dataset_audio.models.vae import ConvVAE
from small_dataset_audio.audio.spectrogram import AudioSpectrogram

model = ConvVAE(latent_dim=64)
spec = AudioSpectrogram()

with tempfile.TemporaryDirectory() as tmpdir:
    preview_dir = Path(tmpdir)

    # Generate previews for 2 epochs
    paths1 = generate_preview(model, spec, preview_dir, epoch=5, device=torch.device('cpu'), num_samples=1)
    print(f'Epoch 5 previews: {len(paths1)}, path: {paths1[0].name}')
    assert paths1[0].exists()

    paths2 = generate_preview(model, spec, preview_dir, epoch=10, device=torch.device('cpu'), num_samples=2)
    print(f'Epoch 10 previews: {len(paths2)}')

    # List previews
    all_previews = list_previews(preview_dir)
    print(f'Total previews: {len(all_previews)}')
    assert len(all_previews) >= 3
    print(f'First preview: epoch={all_previews[0][\"epoch\"]}, file={all_previews[0][\"filename\"]}')

    # Verify WAV is valid
    import soundfile as sf
    info = sf.info(str(paths1[0]))
    print(f'WAV: sr={info.samplerate}, channels={info.channels}, duration={info.duration:.2f}s')
    assert info.samplerate == 48000

    print('PASS')
"` -- should generate valid WAV previews and PASS.
  </verify>
  <done>Preview generation creates WAV files from VAE decoder output with peak normalization. Previews are listed by epoch for the scrollable timeline. Reconstruction previews allow original vs reconstructed comparison. All audio uses InverseMelScale on CPU and GriffinLim for mel-to-waveform conversion.</done>
</task>

</tasks>

<verification>
- `training/checkpoint.py` exists with save/load/manage functions
- `training/preview.py` exists with generate_preview and list_previews
- Checkpoints include all required state (model, optimizer, scheduler, epoch, config, metrics)
- Checkpoint retention deletes old files, keeping 3 recent + 1 best
- JSON sidecar files enable fast checkpoint scanning
- Preview WAV files are valid 48kHz audio with correct naming
- list_previews returns epoch-sorted preview metadata
</verification>

<success_criteria>
- save_checkpoint creates .pt file + .meta.json sidecar
- load_checkpoint restores model, optimizer, and scheduler state
- manage_checkpoints limits total checkpoints to 4 (3 recent + 1 best)
- generate_preview produces valid WAV files at 48kHz from random latent vectors
- list_previews returns sorted preview list with epoch labels
- No new dependencies added
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-training-engine/03-03-SUMMARY.md`
</output>
